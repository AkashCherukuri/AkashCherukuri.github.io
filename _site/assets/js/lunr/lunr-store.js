var store = [,{
      "title": "GDP",
      "excerpt":"Three factors are important to measure a nation’s progress.      Income (GDP)   How well the work-force is absorbed by the economy-(firms)   Inflation Rate   The Circular Flow   This is another way of modeling the Income of a nation, other than GDP. This is the Circular Flow diagram discussed earlier in the course. Here, Income = Expenditure ideally. Any surplus that firms obtain would be the profit, and be tallied up in their income.   Gross Domestic Product (GDP)   The total market value (monetary value) of the FINAL goods and services produced by the factors of production located within the nation’s borders within a given period of time. This is used to account for the nation income properly. Usually measured per year and per quarter year.   A good being “final” depends on the use of the particular good. Goods which are entirely used for the production of other goods are called Intermediary Goods. Second-hand goods’ sales are excluded, to avoid counting it twice. Household productions, legal and illegal underground transactions are also not considered. However, value of goods that are stored in the inventory is also added to the GDP, and subtracted when they leave the inventory.   Value Added - The monetary value of a firm’s sales minus the value of the intermediary goods used in the production. Note that the value added is not exactly equal to the profit, as costs of labor are not considered.   Quarterly GDP is presented after “Seasonal Adjustment” has been done. This is because policy makers want to look beyond the usual seasonal changes, such as an increment in the sales during Christmas.   There are three main ways of measuring GDP:           Expenditure Approach       Compute GDP by adding the monetary value of all final goods and services. The goods are divided into Durable consumer goods (life &gt; 3yrs), non-durable consumer goods (life &lt; 3yrs) and services (salon, maid, therapy).       The expenditure is divided into consumption, investment, government spending and net exports.       Consumption (C)       Includes household expenditure on goods and services, with the exception of purchasing new housing. “Goods” include both durable and non-durable goods. “Services” include intangible items such as haircuts, health-care and (arguably) education.       Investment (I)       Spending on factors of production, or on goods (capital goods) which would be used in the future to create more goods and services.              Business Capital - Equipment, buildings for factories or offices, Intellectual properties for machines .etc       Residential Capital - Building or buying a house (not renting)       Inventories - Goods bought by a person / firm but not sold yet           Investment is not the same as capital. Investment is used to buy capital goods to be used later. A stock is a quantity measured at a point in time. A flow is a quantity measured per unit of time. GDP is a flow measure, as it is done per year.       Government Spending (G)       Government spending on goods and services, such as payments for government jobs. Keep in mind that pensions are not counted in Government Spending because there is no exchange of goods and services taking place. Such transactions are called as Transfer Payments, and they are not included in the GDP calculations. Transfer payments can be thought of as “Negative Tax”.       Net Exports (NX)       NX = (Exports) - (Imports). As a consequence, a household buying a imported product would cause no increment in the GDP because the increase in Consumption is cancelled by the increase in Imports.       The value of total output (Y) is calculated as follows: \\(Y = C+I+G+NX\\)       Real Production (Base) means that the total value is computed with the prices during the base time period.            Income Approach       Add up all components of national income, including wages, interest, rest and profits. It can be clearly seen that $\\text{Y}=\\text{National Income}$.            Output Approach       Sum of value added in each sector       We should be getting the same GDP by all the three methods above. We choose the approach which is the easiest on a case-by-case basis.   Rules for Computing GDP      To compute the total value of different goods and services, the national income accounts use Market Prices.   Used goods and resold goods are NOT included in the calculation of GDP.   Final goods are considered in GDP, and not intermediary goods.   Sold goods’ values are included in GDP. Spoilt goods (perishable goods) doesn’t change the GDP.   Goods which are accounted for in inventories, are not accounted for again when they are sold out of the inventory.   Some goods are not sold in marketplaces, and thus, do not have market prices. For example, mango trees in the forests, small vegetable farm used by a household. We must use their Imputed Value. This value is indirectly added to the GDP, as the cost of a building would go up if it has fertile soil around it, allowing for a backyard garden.   Changes in Price   Nominal GDP - Measures output according to the current year’s prices   Real GDP - Measures output according to prices at a specific year in the past. This year is called as the base year   That is, we would like to compare goods by taking into account the change in market prices that might occur due to other external factors like inflation. The rate of inflation is usually over-estimated, and as a flip side, the rate of real economic growth is under-estimated. When we say “GDP”, we usually refer to “Real GDP” as it better represents the economy.   Keeping the base year price constant for a long period of time would cause the Real GDP to be vastly different compared to the Nominal GDP, which is not desirable. We thus, shift the Base Year prices.   GDP Deflator / Implicit Price Deflator is the ratio between the Nominal GDP and the Real GDP times 100. That is, we measure the price of the output relative to its price in the base year.   GDP Deflator can also be used to calculate the Inflation Rate between two years, year1 and year2 as follows:   \\[\\text{Inflation Rate % } = \\frac{\\text{(GDP Deflator in Year2)}-\\text{(GDP Deflator in Year1)}}{\\text{(GDP Deflator in Year1)}}\\times 100\\]  Problems with GDP   GDP by itself is not a good measure as the size of a country has a very large impact on it. So, we usually measure GDP per Capita, which is the GDP per person; to ensure that the scale of comparison is similar. However, it is possible for large disparities in the wealth to be hidden by GDP, as it measures an average.   India’s GDP is under-estimated due to a large informal sector. Volunteering, preparing food for dinner, and caring for one’s children doesn’t come under GDP; but restaurants and Foster care do. Also, exchange rates used to convert GDP in local currency into US Dollars would depend on the relative prices of internationally traded goods. Developing countries tend to have unstandardized goods, which has little impact on the rate, causing a bias on the GDP.   Comparison resistant services exist, such as health-care, education, administration etc. The price of foreign exchange is also much lower than the free market price.   GNP/GNI   Similar to GDP, we define GNP which stands for Gross National Product. This includes the production by all the people of a particular nation, whose money must be traced back to the country. That is, a migrant worker sending his income back to his home would be counted under GNP. \\(\\text{Net National Product (NNP)} = \\text{GNP} - \\text{Depreciation}\\)   \\[\\text{Net Domestic Product (NDP)} = \\text{GDP} - \\text{Depreciation}\\]  Purchasing Power Parity (PPP)   The amount of money needed to buy a standardized good in different countries. A scale called the “Big Mac Index” exists, where the good is a big mac. Some other examples of standardized goods are a starbucks coffee (ew), a men’s haircut, for starters.   ","url": "http://localhost:4000/notes/hs101/mic1"
    },{
      "title": "Introduction to Philosophy",
      "excerpt":"Broadly, Philosophy can be defined as the “Log of Wisdom”. Also called as Darshana-Sastra in Indian Philosophy. It is a discipline that investigates the nature of values, and relationships integrating humanity with world and reality as a whole.   Philosophical thinking is conceptual in nature. This is necessary to make sense out of the subject matter.      What is number?       What is Justice?    Philosopher is a person who critiques existing philosophical ideas with proof (otherwise its just an opinion). They also try to “prove” whatever is acceptable.   Philosophical Thinking   Philosophical thinking is broadly classified into 5 different categories;           Speculative       Philosophy began as speculation, and is the core of Metaphysics. Highlights the importance of imagining something, and is used to draw intuitive insights beyond observable facts.              “Imagination is more important than knowledge” - Albert Einstein                 Imaginative / Creative       Utilization of poems, and being able to draw artistic interpretation of truth. FOr example, Rabindranath Tagore was an astounding poet and a philosopher.            Reflective / Critical       Very fundamental to philosophy; being able to evaluate the accuracy and relevance of ideas is of utmost importance for the betterment of humanity.            Argumentative       Being critical requires thinking to be argumentative. Arguments are logical, and are regulated by logic.            Liberal / Inclusive       Need to be open to other ideas, to be able to eliminate prejudices. Open-mindedness is very important for the betterment of philosophy.       Philosophy is  broken down into 4 major branches, given below. We will look at each of these branches at a surface level, to know what the reaches of the topic are.      Metaphysics   Epistemology   Logic   Ethics / Aesthetics       Metaphysics   Philosophy began with metaphysics, and was closely intertwined with epistemology. It studies the causes of everything that exists or that may exist.   Metaphysics is considered as the First Philosophy by Aristotle. He classified metaphysics in the following order;      Ontology - The science of being Qua Being - The science of existence   Theology - Highest kind of being (Divinity)   Universal Science - First Principle - Truth of every existing thing, basis of proof or demonstration      A thing is a thing by virtue of something that it is    Although prominent in the pre-socratic era, Metaphysics has been affected in the renaissance era due to enlightenment and groups such as Logical Positivists, and has been claimed as “philosophical nonsense”. Today, it plays the role of disciplinary integration for the better understanding of the connections between different streams.   An example of an ontological question is:      “Who am I?”        Epistemology   Epistemology is a branch of philosophy, and is concerned with the theory of knowledge. Justification and debates are used to “prove” that an idea is “true”. A justified belief is knowledge. A belief by itself is just an opinion or doxa.      “Justified True Belief is Knowledge” - Plato    A belief is proven by;           Explanation: Should be a causual explanation (ie, what the cause of a thing is)            Justification: Argue why the causual explanation is true            Understanding: That is, the conveyed idea must be understandable by the listener       Back in the day, most knowledge consisted of passed down information from generation to generation. For progressing knowledge, it is important to doubt the validity of existing knowledge. This practice of doubting is called as Skepticism or Pyrrhonism.   In general, Skepticism suggests that knowledge is impossible.   In Indian Philosophy, knowledge is given by four sources:      Pratyaksa or by perception   Anumana or by inference   Upamana or by comparision   Sabdda or by testimonies from trusted people   There are two important schools of thought regarding the primary source knowledge:      Rationalism: Reason is primary source of knowledge   Empiricism: Experience is the primary source of knowledge   Do note that Rationalism doesn’t say that reason is the only source, rather it says that its the main source. Similarly for empiricism.       Logic and Arguments   In indian philo, Logic is known as Tarka Shastra. Logic is the science that evaluates arguments regarding a particular observation/reasoning.   That is, Logic is concerned with:      Proper construction of arguments (Tarkas)   Evaluating the validity of the argument   An argument is defined as a logical sentence that shows how a premise leads to the truth value of the proposition.      An argument is valid/invalid if it follows certain laws   Arguments can be made from true premises or false premises as well   For example, Rene Descartes was following Methodological Skepticism which said that true knowledge was attainable (opposite to what skepticism said). He would have to justify his thinking, and its validity would be judged by logic.   There are two different types of arguments;           Deductive Arguments       Introduced by Aristotle, this uses a universal premise to test the truth value/validity of another proposition/argument.       That is, if premises are true; then true conclusion necessarily follows from the premises.            Inductive Arguments       Introduced by Francis Bacon, the premises are based on previous experience and intuition. The concept of generalization is introduced here, and these arguments have methods to be followed when being applied.           Ethics and Aesthetics   Ethics is conerned with morality, values and the theory of morals. An action being “good” or “right” depends on certain principles, and requires rigorous justification as to the same. This branch is important because “good” need not necessarily imply that the action is “just”.   Similarly, Aesthetics are concerned with the following question      What makes something look good?    Although not as important as other branches of philosophy, much of human brilliance can be seen here; as without an aesthetic appeal life would be dull.  ","url": "http://localhost:4000/notes/hs301/intro-to-phil"
    },{
      "title": "Cost of Living",
      "excerpt":"Consumer Price Index (CPI)   This is a statistic which measures the overall cost of all goods and services bought by a typical consumer. CPI can be used to compute inflation and compare the standards of living at two different points of time. The inflation rate calculated by CPI is more representative of the economy than the rate calculated by GDP Deflator. Thus, when we say “Inflation”, we usually refer to “Inflation computed using CPI”.   Calculating CPI           Fix Basket of a Typical Consumer       The “basket” refers to the goods and services that a typical consumer purchases. This basket is assumed to be fixed across the times of computation, and is obtained by surveying people.            Find Prices       Find the prices of all the items in the basket at different years.            Compute Basket price       Use the prices obtained to compute the cost of the basket. Again, we assume the basket to be fixed across the computation time.            Choose a Base Year and compute CPI       Set an year as the base year for comparisons, and calculate the CPI for the remaining years by taking the ratio of the basket prices. \\(\\text{CPI in Year X} = \\frac{\\text{Basket Price in Year X}}{\\text{Basket Price in Base Year}}\\times 100\\)            Compute Inflation Rate       Similar to GDP Deflator, the Inflation rate between two years is calculated as the percentage change in the CPI between those two years. \\(\\text{Inflation Rate in Year2} = \\frac{\\text{CPI in Year2} - \\text{CPI in Year1}}{\\text{CPI in Year1}}\\times 100\\)       Core CPI - A measure of CPI excluding food and energy. This is because food and energy tend to be very variable, and core CPI better reflects inflation trends.   PPI - Stands for Producer Price Index. This computes the change in prices of the producers. Because an increase in PPI would ultimately lead to higher prices, this statistic can be used to predict changes in CPI.   PMI - Stands for Purchasing Manager Index. This computes the change in production of a factory/company on a monthly basis. &gt;50 implies expansion and &lt;-50 implies contraction.   WPI - Wholesale Price Index. CPI &gt;= WPI as not all goods are considered on WPI.   Problems with CPI   CPI assumes that the basket of consumers remains the same across years, which need not be the case practically. Some of the problems caused due to this are discussed below.      Substitution Bias - Increment in the price of a good in the basket would cause consumers to buy less of that good, and more of a substitute. This change is not reflected in the calculations.   Introduction of New goods - Developments in technology can cause the introduction of new goods which might bring the CPI down. However, this would not happen unless the basket of the consumers is changed (which is a problem).   Change in Quality of Goods - An already present good might be improved by the producer, which would have to decrease the CPI. This is not fully represented by the change in price, as there is an increase in the quality as well. We can try to decrease the price accordingly by measuring the value added, but measuring “value” is vague and difficult.   GDP Deflator and CPI comparison   The inflation rates calculated by both these terms are obviously not the same. The key difference is that the GDP deflator takes into account all the domestic-produced goods whereas the CPI takes what the consumers purchase into account. Therefore, if there is an increase in the price of imported goods, the GDP deflator is unchanged but the CPI increases.   Another subtle difference is that the basket is fixed for CPI, but the goods produced by producers in a country can be variable. Thus, the GDP deflator is more flexible to changes in goods over time.   Correcting Economic Terms for effects of Inflation      Comparing money from different times   \\[\\text{Amount Today} = \\text{Amount in Year T} \\times \\frac{\\text{Price Index Today}}{\\text{Price Index at Year T}}\\]  The CPI is usually used as the price index.      Regional Price Parities   The difference in the cost of living between different areas is represented by Regional Price Parity. The difference in prices is due to services, because relocating services is difficult. Housing services are an exceptionally good example.      Indexation   Contracts which are legally bound for large periods of time where are usually indexed. That is, the monetary exchange is corrected automatically based upon a statistic such as CPI.      Nominal and Real interest rates   Nominal interest rate is what the bank offers, without accounting for inflation. Taking inflation into account, the Real interest rate is calculated as follows:   \\[\\text{Real Interest Rate} \\approx \\text{Nominal Interest Rate} - \\text{Inflation Rate}\\] ","url": "http://localhost:4000/notes/hs101/mic2"
    },{
      "title": "Ancient Philosophers",
      "excerpt":"Three influentual ancient philosophers’ ideas have been discussed in brief below. The three people discussed are:      Thales   Pythagoras   Heraclitus       Thales of Miletus   Called as the First Philosopher, and the founder of the Ionian School of Science. We shall see what led to Thales getting this title.   Back in the day, Philosophy was closely interlinked with Mysticism and religion; wherein the priests had supremacy over knowledge.   Thales pursued Principle of movement fundamental to the order.   That is, Thales was the first to try and reason out the causees for observations in nature, not related with religion. This encouraged people to think in a similar manner, and to not be deceived by appearance alone.   In particular, he was interested in the material causes behind floods and earthquakes.   Water as a first principle   Thales is famous for stating that “Everything is Water”. He beleived that everything to be made of one single substance.   It is debatable whether Thales believed this literally, or only metaphorically to be understandable to the common people. This is akin to the Pancha-maha-bhoota idea which was floating around in Indian Philosophy.   He was the first to lead that a monopoly on truth by a religion or a monarch is impossible; and it can only be arrived at by understanding nature.   Thales was the first philosopher as he focused not only on the insights of nature, he also displayed interests in geometrical demonstrations and astronomical insights.       Pythagoras      “Intellectually the most important man that ever lived” - Bertrand Russel    Just like how Thales used water to describe nature, Pythagoras utilized numbers and mathematics. He was interested in seemingly opposite ideologies, Mysticism and Mathematics. His beliefs were influenced by Indian Philosophy, during the Conquests of Alexander.   For example, Pythogoras tried to use numbers to understand music and symphonies. His mysticism was influenced by eastern philosophy, and he believed in music as the expression of the divine.   Pythogoras was the first to:           try and explain nature using an intangible entity            utlize demonstrative deductive arguments, as such is the nature of mathematics. Although Aristotle was the one to popularize it, Pythagoras was the first to utilize them       For example, the pythagoras’ theorem can be viewed as a link between arithmetic and geometry. That is, numbers can be used to explain geometry, which is why numbers for pythagoreans is the principle of things.   Numbers for pythagoreans are the principle of things, not in a physical sense, but as a formal structure. Therefore, we can see that his ideas had a metaphysical side as well.   Contributions:           Deciphering inner workings and the purification of the mind, likely influenced by the Nikruti/Prakruti ideas in Indian Philosophy            Tried to show what cannot be seen directly using mathematics            Introduced the concept of self-evidence/Axioms; that is, you don’t need proof to say that a single line can pass through two points            Deductive Mathematical Argument, explained below       Pythagoras applied self-evidence to perspectives of moral claims. He also believed in the Law of Karma from Indian Philosophy. His moral philosophy included stuff like “Don’t steal from others”.   Axioms are self-evident/fundamental propositions.   Axiomatic Method:     Step-wise deduction through application of rules   Use follow-up steps to derive new propositions   This is Deductive Mathematical Argument; the conclusion follows intuitively.       Heraclitus      Heraclitus believed the world is in accordance with Logos and is ultimately made of fire. He also believed in a unity of opposites and harmony in the world.    Thales used a tangible substance to explain nature. Pythagoras used an intangible number to quantitatively explain nature. Heraclitus was concerned with change or becoming.   That is, for example, “If x is changing to y; can y exist without x?”   Heraclitus used Fire as the Universal Flux, the only agent of change.      Fire as the first principle   Fire is ever-living   Fire as a substance is mobile   He has also said that “All things are exchange of Fire, and Fire for all things, even as wares for gold and gold for wares”.   Change is original, and nothing can be lost or originated. That is, everything has originated from Fire either directly or indirectly and it is possible for them to turn back to fire again.   Ethically, his ideas can be summed up with the statement “Good and Ill are one”, similar to the idea of Yin and Yang.   He has also introduced the notion of LOGOS - The principle of divine reason / cosmic order. That is, he believed that every change followed a universal divine reasoning. This was believed by Pythagoras as well.   He also believed in a cosmic fire-soul, from where he believed every thought to originate.  ","url": "http://localhost:4000/notes/hs301/anc-phil"
    },{
      "title": "Production and Growth",
      "excerpt":"Productivity: The amount of goods produced per labor input. A higher productivity corresponds to a higher standard of living. One of the factors of production is capital. Capital is a produced factor of production, meaning it was an output of a production previously. There are four Determinants of output:      Physical Capital: This is all the tech and machinery needed for more efficient production.   Human Capital: Skilled human capital are essential for high productivity.   Natural Resources: Having more natural resources per worker would cause an increment in productivity. Are of two kinds, renewable and non renewable. Important, but not necessary.   Technological Knowledge: Proprietary knowledge is knowledge under secret (coke), common sense is available to everyone (Ford manufacturing) and temporary proprietary knowledge is hidden for a short time (pharma). Technological knowledge is society’s understanding, whereas human capital is a worker’s quality.   Productivity Policies   Capital suffers from diminishing returns. That is, increment in production by every additional unit of capital produced decreases as capital produced increases. ($\\log$ graph) This is called as Diminishing marginal product of capital. Therefore, increasing savings increases production for a while only. Similarly, the countries which start off poor increase productivity rapidly, and this is called as the catch-up effect.   A direct investment from abroad is called as foreign direct investment, and indirect investment (by buying stocks and the such) is called as foreign portfolio investment. This causes both GNP and GDP to rise, although GDP rise is larger than GNP rise. Less developed countries do this to increase productivity, income, and to learn new technologies. Investment is encouraged by removing restrictions.   Policies which restrict trading are called inwards-oriented policies. “I am in infancy of production, how will I survive on opening myself to the competition?” However, outwards-oriented policies usually tend to dramatically increase productivity of a country.   Human Capital   Education is investment in human capital. The opportunity cost here is the wages that could’ve been earnt had the person been working. Has a positive externalities on the society. A major problem is brain drain, where the highly educated move to the developed country, decreasing the capital of the original country further.   Healthcare is also an example of human capital. Well nourished workers are more productive, and this is an important case in developing countries.   Research is also an important factor in standard of living. Once an idea comes up, it usually becomes public property. Governments can invest in research and incentivize it by awarding patents.   Political Stability   Property rights are very important as no one would work if they expected the results of their labor to be stolen. Political instability decreases confidence that property rights would be upheld, and this decreases productivity.   Population   Population stretches the present natural resources, causing poverty. However, human ingenuity has out-weighed this (there is less poverty although population has increased 6-fold)   Population rise thins out present capital, meaning each person will have less capital to work with which decreases productivity. It also is difficult to train human capital when there’s too many.   Population rise increases the number of potential scientists and engineers, meaning a country with large population is likely to do better in terms of research.  ","url": "http://localhost:4000/notes/hs101/mic3"
    },{
      "title": "Socratic Period",
      "excerpt":"The nature of philosophy in the pre-Socratic period was highly nature-centric. In the socratic period however, philosophy was much more society-centered.   That is, it dabbled with ethics, the importance of, and the directions needed for living a pious and good life. The socratic period was marked by:      Beginning of Greek Enlightenment   Critical Thinking Encouragement - Departure from speculative thought   Individualism - Freedom emphasized   This period was the rise of Skepticism and Subjectivism.   Skepticism has been defined earlier, followers believe that “true knowledge” is impossible to attain. Subjectivism says that different people can have different beliefs. Fundamentalism is a form of extreme subjectivism, where people assert that ONLY their beliefs are true.   Fundamentalism disrupts the harmony which is needed to progress knowledge.   Although naive theories full of gods and occult powers existed, a mechanical theory of atomists was around as well during this period.   This period is the beginning of Greek enlightenment, where freedom and individualism was encouraged and they displayed a critical attitude towards life.   First half of the period was full of natural philosophy of cosmos. Man’s place was determined using Metaphysical conclusions. The second half was marked with a change in the political, economic and intellectual atmosphere. This is after the Great Persian war. That is, the nature of thinking shifted from speculative to critical.   This new age was majorly due to the establishment of a democratic institution. This ushered in many great people such as Socrates and Hippocrates.   Sophists   They were the representation of a new movement in Greece. These were wise and skillful people who worked as professional teachers and travellers. As a livelihood, they used to train people in:      Dialects - Argumentative reasoning (Lawyers, for example) and Aporia (trying to lead the other person into a contradiction)   Grammar   Rhetoric - Flow of speech   Oratory   They were relevant because it was beneficial to be a good orator in democracy, as it opens up opportunities for one to better their life and influence others as well.   However, Socrates felt that they did not use their intelligence properly, and that their logical arguments were lacking. They were also highly skeptical in nature, as that was the prevailing school of thought at the time.       Socrates      “Virtue is Knowledge - An unexamined life is not worth living” (paraphrased)    Socrates urged for clear and rational thinking, and emphasized that knowledge and virtue are necessary for society. That is, he gave importance to ethics.   As mentioned earlier, philosophy before Socrates was highly nature-centric but Socrates focused more on the betterment of society. He caused a shift in philosophy to be more society-centric in nature.   At the time, most of the Sophists lacked objectivity and placed emphasis on subjective opinions. They followed Protagoras, “Man is the measure of all things.” Ethical was divorced from epistemological, which caused issues in society.   Socrates was a moderate thinker, and was infatuated with finding the true meaning of Knowledge. To better understand it, he tried to establish why knowledge is important.   He emphasized that every person is supposed to examine and reflect upon themselves to improve. This is further more important for leaders, and he expressed moral and physical courage in politics and war.   Knowledge   Socrates believed knowledge to be the highest good. He emphasized the importance of knowledge for the improvement of one’s soul.           Knowledge is Morality       Understanding the meaning of Morality, as it is important for the proper functioning of state and society. In short, Moral Knowledge is important.            Knowledge is Virtue       He says that Virtue is good in itself, just like a happy man usually is temperate, brave, wise and just. He believed knowledge to be both the necessary and sufficient condition for virtue.       A teacher can transmit knowledge, values and rules. But Socrates believed that Virtue cannot be taught. That is, virtues are to be practiced and values are to be discovered. Every individual person must reflect on ones’ self and use the knowledge gained to better themselves.       A teacher’s role is to cure social ills such as alchoholism and the such.       Athenians however, weren’t ready to accept and criticize themselves. This was because Athens was a very wealthy city-state; due to which people were ego-istic and self-centric in nature.       Socrates believed that humans are reasonable, and stated that experts are required to understand what is just and unjust. He also emphasized the importance of establishing such a institution in Athens.       Socratic Method   This method is a way of cross-examination, used to test the validity of an opinion. A dialogue is initiated, and skilful questioning is done to prove/disprove its importance.   The Socratic Method includes:           Scepticism of truth of the matter under discussion. This is provisional wheras Sophists’ scepticism was definite in nature            Conversation - Dialogue with the motive of discovering truth            Conceptual in nature            Empirical or Inductive reasoning; reffering to particular instances            Deductive reasoning; testing validity           Plato      Disciple of Socrates, Theory of Ideas    Plato felt that Socrates missed upon utilizing Ontology while discussing ethics, and took it upon himself to do so. He brought back Metaphysics into discussion. His theory of ideas tries to combine epistemology, metaphysics and ethics.   He believed that genuine knowledge is unchanging and absolute; but our senses and experiences keep changing. (Latter was inspired by Heraclitus) He emphasized that true knowledge could be comprehended through reason.   Metaphysics - Idea of a form   The concept was originally introduced by Paramenides. It refers to the “being-ness” of an object. (What makes a Banyan Tree, a banyan tree?)   Although plato did not agree with everything said by Parmenides, he did use this concept in his works.                  Paramenides       Plato                       Idea is not a thought       It may be an object of thought                 Only one idea       Multiple, indivisible ideas exist                 Not known to us, as human knowledge is not absolute       Can be grasped using reason           Plato argues that ideas are universal and eternal, and that they aren’t subject to change. Ideas correspond to an abstract concepts, and cannot be seen directly.   Ideas are:     non-temporal   existing in an independant realm   participate in particulars   Plato believed that Forms are not related to the Physical world, that they existed in their own plane of existence.   That is, the idea of horseness is independent of the original horse it might have been based on; and exists on even after the original horse has died.   Therefore, Plato’s originality lies in raising the issue of universals when talking about particulars. For example, beautiful objects are related to the universal, beauty.   Dialectical Method   This method is used for proper comprehension, and is used to try and comprehend the form of an object/concept. We first try to generalize scattered particulars into a single idea, and then we try to classify this broad general idea into specific smaller classes. That is, this method consists of:      Generalization   Classification   This allows for clear and consistent thinking. This is also called as Thinking in Concepts.   He introduced this because he felt that the Sophists’ ideas were too cluttered and this caused confusion amongst the people regarding the true nature of things.   This requires that language has general words such as “similar” and “before”, instead of just proper nouns. That is, you cannot say that a Bamboo Banyan Tree and a Oak Tree are similar without knowing what similarity is.   Metaphysics and Epsitemology   (It was then believed that soul was responsible for thinking)   Plato believed that knowledge is latent in the soul, bringing both these branches of philosophy together. He said that the soul can contemplate on pure eternal ideas.   Idea of Unity and Diversity, all horses are united under the concept of “Horseness”; wheras the presence of various horses is diversity.   He believed that the idea of good is LOGOS. That is, he believed the idea of good to be the supreme idea.   That is, plato believed that experience is not the source of concepts. Rather, conceptual knowledge is the only genuine knowledge.   He believed in a Hierarchy of Knowledge, where doxa had the lowest position. He believed genuine knowledge to be infalliable in nature, because forms are unchanging and eternal. This is different than what Heraclitus and Protagoras were saying. The knowledge attained via senses is thus falliable in nature.   Therefore, attaining genuine knowledge requires reflection upon senses by the mind and soul.   Platonic Ethics   Plato believes in the pre-existence of soul, and that the body-soul relation is akin to the relation between a chariot and a charioteer.   Tri Partite Division           Rational Faculty: Intellectual activities are handled by this faculty            Spirited Faculty: Responsible for decisions and will. Irrational in nature.            Appetitive Faculty: Desires for pleasure, wealth and the such       He also describes two types of pleasures; intellectual and sense-driven.   Psychological Relativity: Not being driven by either pleasure or pain   Moral perfection can only be attained only when control over spirited and appetitive faculty is acheived. The purpose of life is to be good, not to chase pleasures. He says that this is possible only upon self-reflection.   ","url": "http://localhost:4000/notes/hs301/socr-per"
    },{
      "title": "Finances",
      "excerpt":"Financial Markets   Institutions where a person who wants to wants to save can directly supply funds to a person who wants to borrow.   Bond Market - Debt Finance   Bond is a certificate of indebtedness, and IOU from the company to the buyer. The amount borrowed is called the principal and the time when the bond matures is called date of maturity.   Bonds have four parameters which indicate how valuable they are:      Term: time until a bond matures. Long term =&gt; more risk   Credit Risk: How likely is it that the company will not declare bankruptcy and default the bond.   Tax Laws   Inflation Protection: Are the bonds indexed?   Stock Market - Equity Finance   A stock is a partial ownership of the company. The gains and losses are shared by the stock owners proportional to the number of shares that they own in the company.   Stock Index: a measurement of overall stock prices.   Financial Intermediaries   Institutions where a borrower can indirectly take funds from a saver. There are two financial intermediaries.   Banks   ….what we all know and love….   Mutual Funds and Index Funds   Mutual Funds operate with stocks and bonds, and offer indirect purchases at a fee. A benefactor for a mutual fund accepts the returns on the selection (portfolio) of shares bought by the funds, be it a profit or a loss.   Mutual Funds do active trading wheras Index funds just buy and hold from the companies with high index values.   Mathematics of Savings   We have already seen the expenditure approach for calculating the GDP of a nation. Consider the following equations for a closed economy: \\(\\begin{align*} \t\\text{Y} &amp;= \\text{C}+\\text{I}+\\text{G} \\\\ \t\\text{Y}-\\text{C}-\\text{G} &amp;= I \\\\     \\text{Savings} &amp;= \\text{Investment} \\\\ \\end{align*}\\) That is, the savings and investment are equal in a closed economy. Note that this is true for the economy as a whole, and not for every individual household. Also, a person buying bonds and stocks is not “investing”, but rather is “saving” his money as no capital is being purchased.   Define T to be the Taxes minus the Transfer payments. For this, we can manipulate the identity as follows: \\(\\begin{align*} \\text{Savings} &amp;= \\text{Y} - \\text{C} - \\text{G} \\\\ &amp;= (\\text{Y}-\\text{T}-\\text{C})+(\\text{T}-\\text{G})\\\\ &amp;= \\text{(Private Savings) + (Public Savings)}\\\\ \\end{align*}\\)  When Public savings are negative, the economy is said to be in Budget Deficit whereas when the public savings are positive, we say that the economy is in a Budget Surplus. The accumulation of deficits is called as Government Debt. Also, when Public Savings is 0, we say that the government has a balanced budget.   Market for Loanable Funds   “Loanable Funds” are the funds which are available to fund private investment. It can be clearly seen that it would be equal to the savings of the government, the national savings. The supply-demand graph for this hypothetical market has interest rates on the y-axis and funds people are willing to deposit on the x-axis. The supply of loanable funds are the national savings, and demand is the amount people/companies wish to borrow for investment. As interest increases, more people would like to save, increasing supply.      Taxation on Investment Returns: Shifts the demand curve to the left as taxation is increased.   Taxation on Savings: Shifts the supply curve to the left as taxation is increased.   Budget Deficit: **Public savings are negative, meaning that the government sells bonds which reduces the funds available for investment. This phenomenon is called **crowding out, and would cause the supply curve to shift to the left.   ","url": "http://localhost:4000/notes/hs101/mic4"
    },{
      "title": "Page Not Found",
      "excerpt":"Whoa there, fella! What’d you do? If this was caused by me linking the pages incorrectly, please let me know in the feedback.   To be redirected to the home page, press here.  ","url": "http://localhost:4000/404.html"
    },{
      "title": "Aristotle",
      "excerpt":"Aristotle is a philosopher from the Socratic period as well. He was the disciple of Plato, and tried to be more systematic. He was responsible for:      Division of Philosophy into branches (eg Metaphysics and Physics)   Systematic casual explanations      Philosophy as love of wisdom must be consistent and scientific    Aristotle’s work was focused on the systematic reconstruction of philosophy, and the development of ethics.   He disliked the lack of scientific explanation in Plato’s philosophy, especially regarding forms/ideas. He denied the division between ideal and material.   Aristotle and Knowledge   Aristotle believed experience to be the basis of knowledge, and true knowledge could only be obtained after knowing reasons or causes for an occurence.   (That is, he was neither an rationalist nor an empiricist) *(in my opinion)     He classified science into four categories:      Logic   Theoretical Sciences - Abstract knowledge like chemistry   Practical Sciecnes - means of conduct, ethics, politics   Productive Sciences - aesthetics, poetics   Inherence of Forms   Forms are not apart from things, but are inherent in them. They are eternally together; and matter combines with form to constitute individual things.   Plato thought the phenomenal world to be a shadow of the ideal, real world. Aristotle on the other hand advocated that the phenomenal world was not a copy of the real world. Rather, it is the real world.   This take by Aristotle advocated Realism and led to the progression of Natural Science.   Hierarchy of Forms   Aristotle believed that plurality of individual substances exist in a hierarchial order, with matter at the bottom and Pure form/God at the top. He believed matter to be the primoridal stuff.   Every individual substance is a mixture of matter and form, and matter is responsible for giving an object its uniqueness and particularity.   Human reasoning can discern the form.   Explanation of Change   (Potentiality and Actuality)   Plato’s idea of forms does not tackle the concept of change as he believed form to be non-temporal. Aristotle’s ideas challenge this notion.   He believed there to be 4 reasons for a substance changing. Using clay for making a pot as an example, these reasons are:      Material Cause: Clay is the basic, primordial stuff        Efficient Cause: Agent bringing change to primordial stuff (potter)       Formal Cause: Structure/Form of the pot   Final Cause: Use of the pot   Formal and Final cause are closely interlinked with Teleology. Aristotle believed that the best way to understand why things are the way they are, is to understand the purpose that they fulfil.   Logic and Syllogism   Syllogism is demonstration in the form of deduction. It utilizes properly formed arguments to arrive at a conlclusion from premises reagrding a middle term.   Propositions being used in an argument are called as Premises. There are four kinds of propositions:      Universal Affirmative (A) - “All X is Y” (distribute subject; X)        Particular Affirmative (I) - “Some X is Y” (distribute none)       Universal Negative (E) - “No X is Y” (distributes both subj and pred; X, Y)   Particular Negative (O) - “Some X is not Y” (distribute pred only; Y)   Similarly, Major Premise is universal in nature and Minor Premise is particular in nature.   Structure of Syllogism      Terms   Propositions   Moods - Depends on the type of proposition   Figures - Depends on the position of the middle term   Rules   A proposition contains three terms: Subject, Predicate and the Copula(relates to both subject and predicate, “is/are”).   Consider the below argument.      All games are better than League of Legends       Some games are unfinished       Therefore, some unfinished games are better than League of Legends    The mood of the above argument is AII. The middle term appears as subject in both the premises, defining the Figure of the argument.   There are four relations between propositions:      Contradictory: A/O, I/E (negations)   Contrary: A/E (“hard opposites”)   Sub-Contrary: I/O (“soft” opposites)   Subaltern: A→I, E→O (Superset)   Rules      The fallacy of the undistributed middle: Middle term must be distributed atleast once   Illicit Major: Middle term not distributed in major premise   Illicit Minor: Middle term not distributed in minor premise   The fallacy of two exclusive premises: Two negative premises       Ethics and Aristotle   He believed ethics to be Anti-Hedonistic, and Intuitionsistic in nature. He justified anti-hedonism by making a distinction between the pleasure of mind and pleasure of the body.   Aristotle says that man is a combination of body and soul. The soul is an inorganic unit, and is responsible for moving the body and for perceiving information.   He stated the following parts of a soul:      Rational: Capable of rational thought (man has all three)   Sensible: Capable of sense (animal have sensible and nutritional)   Nutritional: Capable of nutrition (plants have only nutritional)   Aristotle believed mind to be higher than the soul. It is independant of substance, and is immortal. Mind is the power to think.   That is, the soul could die and decay alongwith the body but mind doesn’t do so.   He also talks about the nature of reason, and categorized reasons into different types. They are;           Active Reason       Arises in the course of soul’s development with the other psychic functions. This refers to thinking about the consequences of an action.       This can be identified with universal reason. (is the action being performed desirable?) This is the divine mind coming to the soul.            Creative Reason       Here all concepts are actualized, and thoughts and objects are one. (How is a pot shaped from clay?)       Creative reason is immortal, imperishable and is not bound to the body.       Eudaimonia   Aristotle aimed to answer Socrates’ answer of highest good. The “good-ness” of an object depends upon the realization of its specific nature. That is, a knife is good if it performs its duty of cutting stuff well.   Similarly, a human being can be good only upon the habitual exercise of the function which makes him a human being. That is, rational thinking and virtuous actions have to be exercised.   Pleasure usually accompanies virtuous activities. Pleasure can be included as the highest good, but is not the highest good itself. This indulgence with virtuous activities is known as Eudaimonia.   Virtue   Aristotle maintains that there are two types of virtue; intellectual and moral.      Intellectual Virtue is related to wisdom, knowledge and perfection   Moral Virtue is more emotional in nature, regarding doing the right thing   Phronesis is also considered, and it is the practical virtue   He stated that virtues are moderation between excess and deficiences. For example, modesty is a mean between bashfulness and shamelessness. This mean does differ from individual to individual. A virtuous man should be able to make this choice after assessing the situation.   Justice   Aristotle felt that self-realization does not include selfish individualism. He tried to be altruistic in nature, and said that a virtuous man must love goodness for its own sake.   Justice is altruistic by nature, as justice is doing good for the society. A just society is where lawfullness and fairness are emphasized. Justice should be inclusive of all virtues.   Justice is giving man his due.   He believed that the highest happiness is a scpeculative activity, an activity which takes the form of contemplation.  ","url": "http://localhost:4000/notes/hs301/aris"
    },{
      "title": "Unemployment",
      "excerpt":"The population mass is divided into three categories for studying unemployment.      Employed: This category includes those who work as paid employees, work in their own business, or work as unpaid workers in a family member’s business. Both full-time and part-time workers are counted. This category also includes those who were not working but who had jobs from which they were temporarily absent because of, for example, vacation, illness, or bad weather.   Unemployed: This category includes those who are not employed, but are available for work, and have tried to find employment during the previous four weeks. It also includes those waiting to be recalled to a job from which they have been laid off.   Not in Labor Force: Students, home-makers, retirees etc. Home-makers are NOT unemployed!   Normal Unemployment is the usual rate of unemployment present in an economy. Cynical Unemployment refers to the variations around this mean rate. This is closely associated with the short-term economic activities and events.   Note that it is difficult to distinguish between the unemployed and the people not in the labor force. It is common for people to interchange between both states, such as a student finishing his studies or an early retiree being forced to look for a job to make ends meet. Also, Discouraged Workers the people not in the labor force, but wiling to work (they’ve left the force due to continuous rejections).   Duration and Types of Unemployment      Most spells of unemployment are short, but most unemployment observed at any given time is long-term.    That is, over time, most of the unemployed people find a job within a short period of time. However, at a given point of time, the number of unemployed who have not found a job for a long time is larger.   There are a few types of unemployment defined which try to answer this question.      Frictional Unemployment: This is caused due to the time taken to match an employer to the prospective employee with the right skill set. This is thought to be the main cause of short-term unemployment.   Structural Unemployment: This occurs when the wages are higher than the equilibrium price, causing excess in the labor that is willing to do the job (look at the demand-supply graph). The wages are usually set higher because of minimum-wage-laws, unions, and efficiency wages.  ","url": "http://localhost:4000/notes/hs101/mic5"
    },{
      "title": "Monetary Systems",
      "excerpt":"Money is a set of assets that people in a community use to exchange goods and services. Money has three functions:      Medium of exchange:  an item that buyers give to sellers when they want to purchase goods and services   Unit of account:  the yardstick people use to post prices and record debts   Store of value an item that people can use to transfer purchasing power from the present to the future   Liquidity refers to how easily an asset can be converted into money, and wealth refers to all monetary and non-monetary assets.      Commodity Money has intrinsic value, and is used as the medium of exchange. Gold Standard is an economy that uses gold as its commodity money, or an economy where money can be easily converted to ownership of gold.   Fiat Money has no intrinsic value, and its success depends upon the socio-economic status of the country.   Measuring Money Stock   Money Stock is the total amount of money that is present in an economy. This includes not only the currency (money in hands of the public) but also other assets which have a reasonably high liquidity. We divide money stock into categories M1 and M2. Here, M2 is more relaxed, meaning that assets can have lower liquidity than the assets in M1. (Do understand that all assets that belong to M1 will belong to M2 as well.)   Credit Cards do not count as money or assets!      M1: Currency, Travel deposits, cheque-able deposits .etc   M2: Everything in M1, Savings Deposits, Mutual Funds’ deposits, small time deposits .etc   Banking and money supply   Banks have the following balance sheet made, in order to identify assets and liabilities; and to see how much funds are present in each sub category. (assets = liabilities as a whole by definition)                  Assets       Liabilities                       Reserves       Deposits                 Loans       Companies’ debt                 Securities (buy stocks/bonds)       Owner’s equity (capital)           Reserve Ratio (R)  is the fraction of reserves held out of the total assets. A minimum RR is mandated by the government, but banks can have excess reserves to ensure that they can pay back the deposits. Money Multiplier is the maximum multiplier that the money supply can increase to.  \\(\\text{Money Multiplier} = \\frac{1}{R}\\)   Central Bank   The central bank is an institution which oversees the banking system and regulated the amount of money in the economy aka money supply. It does so by altering monetary policies as the need arises. The central bank firstly acts as a last resort lender of money for banks in financial crisis, a bank’s bank. The interest rate offered to banks by the central bank is called as the Discount Rate.      A higher discount rate decreases the money supply   A lower discount rate increases the money supply   It also alters the money supply predominantly by Open Market Operations (OMOs).      To decrease the money supply, the central bank sells bonds. The acquired funds are out of the hands of the public.   To increase the money supply, the central bank buys bonds. The new funds are in the hands of the public, and a new dollar as currency increases the money supply by 1$ but a new dollar in a bank increases the money supply by $1$\\times 1/R$ .   The reserve ratio (R) is altered by the central bank in these ways:      Altering Reserve Requirement: Self explanatory, change the minimum required R. A larger requirement decreases the money supply.        Changing Reverse Repo Rate: The central bank can offer interest on the reserves that bank have, encouraging the banks to have larger reserves. A larger interest would reduce the money supply in the economy. Measured in Basis Points, where 1 basis point corresponds to $0.01\\%$. The Repo Rate is the rate at which banks borrow money from central bank after depositing securities (unlike discount rate).       Change Federal Funds Rate: The federal funds rate is the interest rate that a bank-to-bank loan has. The loans are for very short durations of time, such as one day. These ensure that a bank can repay its depositors if it runs out of reserves. Increasing the interest rate would decrease the money supply.  ","url": "http://localhost:4000/notes/hs101/mic6"
    },{
      "title": "Inflation",
      "excerpt":"Quantity Theory of Money   Let the price level according to CPI be $\\text{P}$ in an economy. This means that a standard basket of goods and services can be bought using an amount $\\text{P}$. From this, we can say that the value of an individual unit of money in terms of the standard basket is $1/\\text{P}$. This is the value of money in this economy.   We draw a demand-supply graph. The supply is the money supplied by the central bank, and the demand is the liquid cash that people wish to have. The demand depends significantly on the price level, as people would need more money the higher prices are. X-axis is the quantity of money supplied, and left Y-axis is the value of money in the economy.   From this, we can see that inflation occurs when the quantity of money in the economy is increased. That is, when people have larger quantities of money, they will offer more than competitors to get services first.   Classical Dichotomy   Every quantity is divided into two types, real and nominal. Real variables measure quantities and physical stuff whereas nominal variables measure made-up stuff like money. For example, price of a CD and nominal GDP are nominal variables and number of cds is a real variable.   Relative prices are real variables as well. “One copy of Titanfall is worth fifty CoD copies” is an example of relative prices. In the long run, price changes affect only nominal variables leaving real variables almost unchanged. This is called as the Monetary Neutrality of real variables.   Quantity Equation   We define the velocity of money in an economy to be the number of times it changes hands on average over a given period of time. It is calculated as the ratio of nominal GDP with money supply. Let the price index of the economy (GDP Deflator) be $\\text{P}$, real GDP be $\\text{R}$, the velocity be $\\text{V}$ and the money supply be $\\text{M}$. Then: \\(\\begin{align*} \\text{V} &amp;= \\frac{P\\times R}{M}\\\\ \\implies \\text{V}\\times\\text{M} &amp;= \\text{P}\\times\\text{R} \\end{align*}\\) The below equation is called the quantity equation. It has been observed that velocity is nearly constant when money supply changes, and it is clear that money supply cannot change the value of real GDP (real variable). Therefore, an increase in money supply is cancelled by an increase in price index, leading to inflation.   Inflation Tax:  The “tax” levied on every person with money (as its value has dropped) due to the government increasing its supply is called as the inflation tax. Seigniorage is the income that is generated by printing money.   Fischer Effect   \\[\\text{Nominal Interest} = \\text{Real Interest} + \\text{Inflation}\\]  From monetary neutrality, we can say that inflation and nominal interest are closely tied together.   Costs of Inflation      Inflation Fallacy: The idea that inflation erodes the income which people have. Real income doesn’t depend on inflation, although nominal income does.   Shoeleather Costs: During high inflation, we would want to exchange money for goods and services as quickly as possible; or store it in banks. The costs incurred for this effort come under this category.   Menu Costs: The costs for changing menus frequently during periods of high price instability.   Taxes: Taxes are based upon nominal incomes, and inflation causes more percentage of the real income to be given away than usual.   Hyper-Inflation   Inflation of over $1\\%$ per day. When the govt doesn’t have enough money to pay debts, and buyers do not have confidence in its bonds (due to socio-economic reasons); the govt has no option other than seigniorage to get the money required.  ","url": "http://localhost:4000/notes/hs101/mic7"
    },{
      "title": "Aggregate Demand and Supply",
      "excerpt":"Recession: Periods of mild economic decrease in incomes and increase in unemployment   Depression: Periods of strong economic decrease in incomes and increase in unemployment   Economic Fluctuations   1. Economic Fluctuations are irregular and unpredictable   The fluctuations in the economy are called as the Business Cycle. This is a misleading term because the fluctuations are very irregular and are also very difficult to predict.   2. Most Macro Economic variables vary together   However, the amount of change in each variable depends on the variable under consideration. That is, even if real GDP and Income decrease during a depression, they would decrease by (obviously) different amounts.   3. As output falls, the rate of unemployment rises   Obviously, when there is less to produce, companies lay off workers.   Model of Aggregate Supply and Aggregate Demand   In the short run, the assumptions of classical dichotomy and monetary neutrality do not hold. We cannot separate our analysis of real and nominal variables. We build a new model to see how they interact with each other. Similar to a demand supply curve, we draw a graph between the price level on the y-axis and the real GDP on the x-axis. The aggregate demand is the amount of goods and services that people and companies wish to consume at a given price level and the aggregate supply is how much goods and supplies can be produced at a given price level.   Do keep in mind that the curves have been drawn assuming that all other factors such as money supply are fixed.   Why is Aggregate Demand Sloping downwards?   \\[\\text{Y} = \\text{C}+\\text{I}+\\text{G}+\\text{NX}\\]  Assume that $\\text{G}$ is fixed by a government policy. We discuss the effect of decrease in Price Level $\\text{P}$ for the three cases.      Wealth Effect: $\\text{P}$ decreases, causing consumers to become more richer by which they would tend to buy more goods.   Interest Rate Effect: Decrease in $\\text{P}$ would cause a decrease in interest rates, incentivizing people to borrow money and invest.   Exchange Rate Effect: Decrease in $\\text{P}$ causes the currency to depreciate, meaning that prices have fallen on global scale for the economy which would cause other countries to buy from our economy, increasing net exports.   The aggregate demand curve can shift due to the following reasons:      Change in Consumption: When the government increases taxes, it discourages people to spend and thus the curve shifts to the left. Similarly, when people become obsessed with savings, the curve shifts to the left as well.   Change in Investment: Investment Tax Credits, promising new technology and a increase in money supply shift the curve to the right.   Change in Government Spending: Governments having new projects and stuff shifts curve to the right.   Change in Net Exports: self-explanatory   Long-run Aggregate Supply Curve   The long run aggregate supply curve is a vertical line because the theory of classical dichotomy and monetary neutrality hold in this case. The value of the supply in the long run is called the Natural Level of Output. The long run supply curve can shift due to variety of reasons. (733/734 of Mankiw)   Stagflation: A period in which both production falls (stagnation) and the prices rise (inflation). It can be caused when the aggregate supply curve shifts to the left, due a sudden increase in costs of production and the effect is self-reinforcing.  ","url": "http://localhost:4000/notes/hs101/mic8"
    },{
      "title": "Fiscal Policies on Aggregate Demand",
      "excerpt":"Monetary Policy: The policies regarding the money supply and interest rate set by the central bank.   Fiscal Policy: The policies regarding taxation and stuff, set by the president and the Vice-President.   Theory of Liquidity Preference   The contribution of Wealth Effect and Exchange rate effect is small when compared with the interest rate effect for the downward slope of the aggregate demand curve. This theory explains how fiscal policies affect the interest rate affect and the curve. This is for a short-term, and we thus assume the rate of inflation to be constant. We draw a graph between interest rates on the y-axis and the money supply on the x-axis. The Supply curve in this case would be a vertical line, equal to the value fixed by the central bank. The Demand curve would be the quantity of money demanded for holding by the people as money is the most liquid asset.   As interest rate rises, the demand falls because the cost for holding the money increases. Equilibrium is achieved when people want to hold as much money that the central bank has supplied. When the interest is above or below the eq. value, it settles back due to there being lower and higher demand respectively.   We can now explain why an increase in price level increases rate of interest. $\\text{P}$ increases, causing the demand of money to increase, shifting the curve to the right and thus causing the interest rate to rise.   When money supply increases, this causes the interest rates to fall. This causes the aggregate demand curve to shift to the right, increasing production. (Assuming that the price level remains the same) the fk?   Zero Lower Bound   The monetary policy essentially reduces the interest rates directly or indirectly (by increasing money supply) to stimulate economic activity. However, what can the central bank to when the interest rate becomes as low as it can possibly go?      Forward Guidance: Keep interest rates low for as long as possible to stimulate economic activity.   Quantitative Easing:  Instead of just buying short-term government bonds, the central bank can buy long-term government bonds and mortgage backed securities as well.   Inflate the economy, so that the value of nominal interest is larger when compared to the real interest so that the value of the lower bound decreases.   Fiscal Policies and Aggregate Demand   Fiscal policies affect the aggregate demand directly. Suppose that a government orders the development of aircraft from a company for 20 Million. The demand curve shifts to the right, but the shift value need not be equal to 20 Million.   Multiplier Effect   An increase in the sales by the company would cause the employees to receive more wages and thus, indirectly increase the consumer spending. This increment can also cause the wages of the store workers to increase in the second wave. This reinforced increment is called the investment accelerator.   The fraction of excess money spent by the households is called Marginal Propensity to Consume (MPC). Therefore, if the money spent by the government is $\\text{G}$, the max increase in the aggregate demand $\\Delta\\text{AG}$ is:   \\[\\Delta\\text{AG}_ \\text{max} = \\frac{\\text{G}}{1-\\text{MPC}}\\]  Note that the multiplier effect can work against the economy as well. Suppose the case where NX falls, the decrease in aggregate demand would by larger by the same factor. Also, the effect need not be only for G, it can be for any case where aggregate demand changes.   Crowding out effect   An increase in government spending increases the interest rate, which in turn decreases investment incentive causing a decrease in the value by which the aggregate demand increases.  ","url": "http://localhost:4000/notes/hs101/mic9"
    },{
      "title": "CS213",
      "excerpt":"  Important Notes      In if statements; if(k&gt;0) is preferred over if(k) because k&gt;0 returns a bool while k doesn’t, thus depending on the definition of if   Passing Big Vectors or Arrays as parameters of a function would cause a lot of time loss, because copying the huge data takes place; Especially during Recursion. Using Global Arrays is better!   In creating header files, don’t do using namespace std; because the person using the header may or may not want to use that namespace   \\n is faster than endl when needing to output a large number of lines. However, use endl at the end of the output to flush the output and refresh the buffer.   C++ Libraries     Vectors  - Some Functions to be known         Running Lecture Notes    Lecture on 10th Sept           Addition is not a Basic Operation! Addition requires changing all the bits in worst case scenario; making it O(logn) on average. This can be made even faster by using parallel computing by predicting the carry of the bit, while simultaneously adding.       (How did we compute runtime as O(logn) here – Number of bits is log(n) where n is the input)       Given string S find all s1 and s2 such that interweaving both gives S   Break this down to first finding the number of distinct subsequences for s1 and s2.           If there is no repetition; we can write num_dist_subseq = 2^n.            For a case with rep, define N[i] = number of distinct sub seq from A[0] to A[i]; where A is the original string. we can define a recursive relation as A[i] = 2*A[i-1] + A[j-1].        Lecture on 14th Sept   Lab Solution for previous Thursday           The Problem is reduced to: \\(F(A {\\displaystyle \\cup} \\{n-1\\}) = F(A) + F(A {\\displaystyle \\cup} \\{n-1\\})\\)            The second part of the RHS is what caused you trouble; but what you didn’t realize was that SHIFTING THE f-Array by (2^n) unites to the left! Therefore, you solve this problem in a Divide and Conquer method.            What you were trying to do was overly complicated; you were trying to go from F(0) to F(A) by using Dynamic Programming and stuff.            Part 2; suppose all the values given were F. Reverse the steps done above, meaning you would do: \\(f(A_i) = F(A_i) - F(A - A_i)\\) And Dynamic programming can be implemented here to make the code even more faster.         Lecture on 15th Sept   213 Assignment Discussion      Avoid using namespace std; because the user who includes your header file may not want that namespace. To use cin and cout; do std::cin and std::cout.   to_array() must NOT return a pointer to the internal Array. You must make a deep copy of the Array, and return a pointer to that  array. This is what you did u idiot   const1 operator*(const2 permutation perm) const3{}            const2:- perm is a constant; i.e.; in a*b, b is a constant.       const3:- in a*b, a is a constant. This won’t work for overloading = because in a=b, a must not be a constant.       const1:- The returned permutation is a constant.             Lecture on 17th Sept      We mostly use Worst-Case analysis for checking the growth rate of an Algorithm. Average-Case makes sense in some cases, like the QuickSort Algo. These algorithms usually have a Randomization incorporated, such as the Pivot in the QuickSort.   For a Randomized Algo, the Running time is a Random Variable. We therefore, represent the runtime of the algorithm for a given input size n, as the Expected Value of the Random Variable. This is usually misleading, because the real runtime might vary and be more than the expected value, but usually it performs near the Expected Runtime.   Hindsight, you didn’t check the algo in today’s lab and submitted the O(n^2) algorithm     Lecture on 21st Sept   Previous Lab Solution      endl flushed the output a single value at a time. Therefore, use \\n when needed to print output on different lines, in the for loop; but DON’T FORGET endl at the end to flush the output completely! Read abt it here.   For the second part, we’re gonna be building f_i from prev known. We can find the greatest to the left (say j) quickly, and that will be the  limit to which we’d need to search. For k, j&lt;k&lt;=i to be a starting point; it must be the smallest among that proper subset.   What we do here is, we build a vector. (Write this logic down in OneNote, its really good.)     Lecture on 22nd Sept   Interweaving of Strings      try something… Didn’t pay attention here   Previous day’s Quiz        ","url": "http://localhost:4000/notes/cs213cn/"
    },{
      "title": "CS251 - Python",
      "excerpt":"Introduction     Python is an interpreted language; meanwhile C++ is a compiled Language.   Compiling: Check Syntax -&gt; Semantic Analysis -&gt; Optimizations -&gt; Convert optimized to x86-64 program. This x86-64 program can be run as many times as needed. Semantic Analysis is Static in nature.   Interpretation: Syntax Analysis -&gt; ByteCode Generator -&gt; ByteCode Optimization. This ByteCode runs on a Virtual Machine Simulator which runs (gets Interpreted) on your laptop. The Semantic Analysis in this case is dynamic, meaning that it occurs when the ByteCode is being executed.   ByteCode Analysis     import dis is the module to be imported; and to view the ByteCode of a function, do dis.dis(&lt;function_name&gt;).   The output of the corresponding code should be in the format of 5 columns.                  Python Line       &gt;&gt;       Functions being done       ???       Variable name                       Points to the line in the original Python File to which the stack corresponds to       Present means that a pointer (of sorts) is stored to the particular line. Done when a “jump” to the line is needed to be done; usually for for, while or else cases.       All the stuff that the Virtual Machine does at the corresponding line, in sequence.       Address of the variable or smth?       The corresponding variable name in the Python File, or the value of the corresponding constant in the Python File.             To run a python script from the Python Shell; do exec(open('&lt;YourFile&gt;.py').read()). After the script has been executed; you can print any variables in the environment without adding print() statements and running it again.   This might help in debugging the code faster.     Operator Hacks      x = n // 2 does integer division of n and 2. That means this is similar to int(n/2).   Usage of Basic Functions      print(var, end = '&lt;ending char&gt;') – Print n and the character in end after it. By default; end = \\n.   input(&lt;string&gt;) – Prints &lt;string&gt; and waits for input in the same line.  ","url": "http://localhost:4000/notes/cs251py/"
    },{
      "title": "Macroeconomics",
      "excerpt":"There are only two parties in macro economics - Households and Firms.   Microeconomics and Macroeconomics   Microeconomics focuses on the how money is managed by households and firms individually; while macro focuses on how they interact with one another in the market.   Ten Principles of Economy   10 Principles of Economics, marginal changes and rational people, social costs   Principle-1 People face trade-offs   To get what we want, we need to give up on something. This is called as a trade-off. A very prominent example of this is Efficiency V Equity.   Efficiency means that society greedily gets the most that it can from the scarce resources. Equity is when the benefits of the resources are distributed among all the members of the society fairly.   Principle-2  Cost of an item is what you give up to get it   Opportunity cost of an item is what you have given up to obtain it.   Principle-3 Rational People think at the margin   Marginal Changes are small incremental changes that affect the existing plan of action. People make decisions by comparing costs and benefits at the margin.   Principle-4 People respond to Incentives   As an extension to #3, having incentives motivates people to consider a certain product over another for the same opportunity cost.   Principle-5 Trade makes everyone better off   People can gain the goods and capital required from trading. it also allows for specialization in a certain area to better help out society.   Principle-6 Markets are usually a good way to organize Economic Activity   A market economy is a decentralized economy which allocates resources via the decisions of many firms and households. Adam Smith has made an observation that firms and households act as if they are guided by an “Invisible Hand”, which in reality is just the rational parties trying to maximize their profit.   Principle-7   Governments can sometimes improve Market Outcomes   Property rights are important for functioning of free market economies. They are the ability of an individual to own and exercise control over a resource.   Markets can work only when property rights are properly enforced. When society/political situation in a country is unstable, upholding property rights is difficult. In such cases, the government can intervene. India has a mixed market, where the railway market is controlled purely by the government and other markets such as telecom are controlled by firms.   Market Failure is the inefficient distribution of goods and services by the free market. It can be caused by:      Poor property rights   Externality - Impact of a firm or a person on the well-being of a by-stander. These are not taken into account in the free market, needing the intervention of the government.   Market Power - The ability of a person or firm to unduly influence market price. That is, if a firm has a monopoly on a raw material in the country, government intervention is needed to make sure that they don’t take advantage of this in such a way that damages the country.   Economic Models   Similar to all sciences, we have specific terminologies in this field. We use a few justified assumptions to create an Economic Model, and try to reason out useful data from this model which (hopefully) reflects real-life data pretty well. We do this to bring the attention toward a problem in the society.   Model1 - Circular Flow Diagram   This is a visual economic model which shows how money flows through the market amongst households and firms.   Factors of Production      Land - Where the factory is being put up   Labor - Workers   Capital - The money required for all operations   Entrepreneurship - Group of people to lead the firm   All factors of production are controlled by the households. There are two types of markets in such a case, a market for factors of production(M1) and a market for goods and services(M2). Firms buy in M1 and sell in M2; households buy in M2 and sell in M1.      Model2 - Production Possibilities Frontier   Not including monetary considerations here, it is a physical feasibility exercise.   This is a graph which shows the maximum possible production number of various resources given the constraint of the available factors of production (other than capital). The graph is called as Production Possibilities Frontier. A country operating on the PPF is using the factors of production optimally. Being inside the curve indicates that the production isn’t efficient.      Efficiency   Trade-offs   Opportunity Cost   Economic Growth   A shift in the PPF is observed when an innovation in the production is done to reduce wastage of material or new source of raw material is obtained.   Statement Classes   Positive Statements or Descriptive Statements describe the factual statements.   Normative Statements or Prescriptive Statements describe an opinion about the world, need not be factual.   More about Markets   Perfect Competition - A market where there are many buyers and sellers selling a homogenous product so that each person has a negligible impact on the market price and no bargaining power. Buyers and sellers are price takers, not price makers. Example: Fruits market and such   Oligopoly - Few sellers are present in the market, so the sellers have considerable market power when compared to the buyers. Sellers have luxury of differentiating their product. Example: Telecom Market   Monopoly - Only one seller, where the seller is a price maker not a price taker   Monopolistic Competition - There are many sellers with slight differentiation of the product, with each of them setting a price for their product. Very different from a monopoly, but does have some of its characteristics.   Demand and Supply   Law of Demand   The quantity demanded of a good falls when the price of the good rises. Quantity demanded is the amount of a good that buyers are willing and able to purchase. Not applicable every time, for example, the trend is opposite for paintings.   Demand Schedule - A table which shows the quantity of good demanded against the price of the good.   Demand Curve - Its just the demand schedule being graphed, with price on the Y-AXIS and Quantity demanded on the X-AXIS.   Market Demand - The horizontal summation of the individual demands of all the buyers of a product in the market would be the market demand of that product.   The Law of Demand considers that the price of the product is the only variable in play. A change in these factors shifts the demand curve. The constant factors that are assumed by it are:      Income - As income increases, the demand for an Inferior good decreases and the demand for a normal good increases.   Prices of related goods - Substitutes are the goods when fall in price of one good results in a fall in price of the other good. Complements are the products whose prices are inversely related.   Tastes and preferences   Expectations of the Consumers   Number of Buyers   Law of Supply   All things equal, the quantity supplied rises when the price of the good rises. There would be an upper limit on how much the quantity can rise to. Similar to demand, we have a Supply Schedule and the Supply Curve. Also, we assume that the other parameters are constant here.   The parameters of the Law of Supply are:      Input Prices   Technology   Expectations   Number of Sellers   Law of Supply and Demand   The claim that the price of any good will finally settle at the Equilibrium Price at which the supply of the good equals the quantity demanded. The Equilibrium price would vary depending upon the current changes in the parameters that we’ve assumed to be constant initially.   Here, the Taste and Preference parameter of the customer changed according to the weather, causing a  shift in the demand curve which in turn changes the Equilibrium Price.   Elasticity is the measure of how much buyers and sellers respond to changes in market conditions. It is relative, and is represented in percentage terms.   Price Elasticity in Demand   \\[\\text{Price Elasticity of Demand} = \\frac{\\% \\text{ change in demand}}{\\% \\text{ change in price}} = \\frac{(Q_2-Q_1)/[(Q_1+Q_2)/2]}{(P_2-P_1)/[(P_1+P_2)/2]}\\]  \\[x \\text{ Elasticity of }y = \\frac{\\%\\text{ change in }y\\text{ wrt midpoint}}{\\%\\text{ change in }x\\text{ wrt midpoint}}\\]  Notice that the percentages are calculated wrt the mean of the values and not the second value. There are a few factors which affect how elastic the demand of a quantity is:      Availability of Substitutes - If substitutes are absent, then the product tends to be inelastic.   Necessities vs Luxuries - Inelastic and Elastic respectively.   Definition of Market - (Read tb)   Time Horizon - More elastic when there is a longer time horizon.   We say that the demand is elastic if the absolute value of price elasticity in demand is greater than 1. Conversely, if the absolute value of price elasticity in demand is between 0 and 1, then we say that it is inelastic. Equal to 1 == Unitary Elastic. Similarly, if it is 0 we call it as Perfectly Inelastic and when it is infinite we call it to be Perfectly Elastic.   A straight line will not have the same elasticity throughout! This is because we have additional factors in the calculations other than just $\\Delta Q/\\Delta P$.   Income Elasticity of Demand   \\[\\text{Income Elasticity of Demand} = \\frac{\\% \\text{ change in demand}}{\\% \\text{ change in income}} = \\frac{(Q_2-Q_1)/[(Q_1+Q_2)/2]}{(I_2-I_1)/[(I_1+I_2)/2]}\\]  It can be seen that an increase in the price for a product that is Inelastic in Demand would result in the money obtained increasing. However, increasing the money for a product which is Elastic in demand causes the money obtained to decrease. Ideally, we would like to be at the point where the good is Unitary Elastic.   Controls on Prices   Are usually enacted when policy makers believe the optimal functioning of the free market is being unfair to a single party.   Price Ceiling - The legal maximum on the price that the good can be sold for. Example - MRP   Price Floor - The legal minimum on the price that the good can be sold for. Example - Minimum Wage Law   The Price ceiling is binding if it is imposed to be lesser than the equilibrium price. This causes a shortage of that particular good in the market. Similarly setting the price floor above the equilibrium price causes an excess of goods to be present in the market.   Taxes   Tax Incidence - The manner in which the tax burden is borne by the two parties.   Taxes cause a shift in the equilibrium, the profit made by the buyers and the sellers is reduced. Taxing the sellers causes the supply curve to shift upwards, and taxing the buyers causes the demand curve to shift downwards by the size of the tax in both the cases.   With respect to the demand curve, levying a tax on the sellers and the buyers has the same equilibrium price. The burden of the tax is shared by both the parties, and the share of burden depends on the slope of the curves. The inelastic party usually bears most of the taxation.   Costs of Production   Modern Microeconomics is all about Supply, Demand and Market Equilibrium.   Total Revenue - The amount of money received for selling the goods   Total Cost - The total cost of production, this includes all the opportunity costs that are needed for the production of goods and services. That is, both Explicit and Implicit costs need to be considered. For example, for an entrepreneurship, the explicit costs include the money of and stuff wheras the implicit cost is the money that could’ve been obtained by having a corporate job. \\(\\begin{align*} \\text{Economic Profit} &amp;= \\text{Total Revenue} - \\text{Total Costs} \\\\ \\text{Accounting Profit} &amp;= \\text{Total Revenue} - \\text{Explicit Costs} \\\\ \\end{align*}\\)   Production Function - A relationship between the amount of inputs needed to make a good and the amount of said good produced.   Marginal Product - The increment in the amount of good produced when a particular input is increased by one unit. Diminishing Marginal Productivity refers to the tapering of the marginal product when the input increases.   The production function is represented graphically as a Total Cost Curve which plotted between the cost (y-axis) and the quantity of output (x-axis). Diminishing Marginal Product leads to a sharp increment in the slope.   We shall introduce a few terms to make talking about costs more efficient.   Terminologies      Total Costs (TC), Total Fixed Costs (TFC) and Total Variable Costs (TVC)   Average Total Costs (ATC), Average Fixed Costs (AFC), Average Variable Costs (AVC)   \\[AC = TC/(Quantity)\\]     Marginal Costs (MC) - The increment in Total Cost needed to increase the quantity of a good produced by one unit. This tends to increase at higher costs due to diminishing marginal product. ATC is rising when MC is greater than ATC. Similarly, ATC falls when MC is less than ATC. MC crosses ATC at the Efficient Scale, where the costs are minimized. Also note that MC crosses AVC, AFC at their lowest points. \\(\\text{MC} = \\frac{\\Delta \\text{TC}}{\\Delta \\text{Q}_\\text{output}}\\)   Short and Long Run   The fixed costs in short run become variable in the long run, because everything can be changed as need be in the long run. The long run average total cost has to be always lesser then or equal to the short run average total cost.   Economies of Scale   Economies of scale - The Long Run ATC of the good falls as the quantity produced is increased.   Constant returns to scale - Long Run ATC is constant with respect to the quantity of goods produced.   Diseconomies of scale - Long Run ATC increases with the quantity of good produced   Perfectly Competitive Markets   It has many buyers and sellers selling identical products so that both buyers and sellers are price takers. Firms can freely enter and exit the market. In such a condition, the Average Revenue (AR), and the Marginal Revenue (MR) are both equal to the price of the product. \\(\\begin{align} \\text{Marginal Revenue} &amp;= \\frac{\\Delta(Total Revenue)}{\\Delta(Quantity)} = \\frac{\\Delta TR}{\\Delta Q} = P \\\\ \\text{Marginal Cost} &amp;= \\frac{\\Delta (Total Cost)}{\\Delta (Quantity)} = \\frac{\\Delta TC}{\\Delta Q}  \\\\ \\end{align}\\)   \\(\\text{Profit is maximized when Marginal Revenue equals Marginal Cost (above the ATC)!}\\) That is, if the price falls below the AVC in the short term, it is better to just produce zero output (Shutdown), as a loss will be incurred. Shutdown is a short term decision to halt production for a while, wheras Exit is the long term production stoppage. In the long run however, we exit when the price falls below the ATC.   The portion of the Marginal Cost curve above AVC in short run is called the firm’s Short run supply curve. Similarly in the long run, the portion of the marginal cost curve above ATC is the firm’s long run supply curve. The supply curves of the market and the firm remain the same, albeit with a factor difference in the quantity, because we assume that all firms are rational. The factor would be the number of firms present in the market.   In the long run, price equals the minimum of the Average Total Cost, i.e. , at the Efficient Scale. This can be understood by the law of supply and demand. assume that the price is lower than the minimum of ATC, this would cause firms to exit the market and shift supply curve leftwards, which increases the demand and price. If it is higher than ATC, many firms come in to make a profit, and summarily shift supply curve towards the right, decreasing the cost to min of ATC.   In the long run, an increase in demand only increases the quantity supplied, leaving the price unaffected.   Monopoly   A good is being produced by a single firm without any close substitutes. Monopolies fundamentally arise because of a barrier to entry in the market. The examples for such barriers are:      Ownership of a key resource. This is rarely how monopolies arise, in practice.   Only legal producer of the good, approved by the government. Patenting and copyright laws are two important ways in which this is done.   Costs of production make a single producer much efficient than others in the market. That is, as the number of consumers increases, the cost decreases (Economies of Scale!). For such a setting, Monopoly is optimal to reduce the costs of production. Such monopolies are called as Natural Monopolies, for obvious reasons.   The revenue calculations for monopoly would be: \\(\\begin{align*} \t(Price) \\times (Quantity) &amp;= (Total Revenue) \\\\     (Average Revenue) &amp;= (Total Revenue)/(Quantity) = (Price) \\\\     (Marginal Revenue) &amp;= \\Delta(Total Revenue)/\\Delta (Quantity) \\end{align*}\\) Unlike perfect competition, Marginal revenue is not equal to the Price but it is variable. The maximization condition is $\\text{Marginal Revenue equals Marginal Cost.}$ In a monopoly, the $Marginal Revenue$ is always lesser than the Price, and this can be seen because the demand curve for the firm is sloping downwards. At profit maximization, the Price would be greater than the marginal cost as well, as MR=MC.   Price Discrimination   This is the practice of selling the exact same good at different prices to different customers, even though the cost of production is the same. Examples being Pawn shops and Wholesale/Retailers. This is not possible if goods are being sold in a perfect competition, as the sellers have no buying power.   Perfect Price Discrimination   This refers to the situation when the monopolist knows each consumer personally and charges everyone differently based upon their willingness to buy products. It can be seen clearly that this is just theoretical in nature, and is done to increase the profits of the firm.   Welfare Economics   The study of how allocation of resources affects economic well being. The equilibrium in a market maximizes the welfare of both the buyers and the sellers.   Surplus      Consumer Surplus = (Amount willing to pay) - (Amount actually paid)   Producer Surplus = (Amount received for goods) - (Amount willing to sell)   Graphically, consumer surplus is the area below the demand curve and the price line. Similarly, producer surplus is the area above the supply curve and the price line.   Total Surplus = ConsumerSurplus + ProducerSurplus = (Value to Buyers) - (Cost to Sellers)   Efficiency is the way of resource allocation to maximize total surplus. In the case of a free market, the equilibrium price and quantity are the most efficient combination to maximize total surplus. Therefore, the social planner can leave the market alone. This is referred to as Laissez Faire in French.   This is present only in perfectly competitive markets though; in a monopoly the distribution of price is not efficient causing there to be deadweight losses (Triangle between monopoly quantity, demand curve and MC curve). This inefficient allocation may also be a cause of externalities.   Theory of Consumer Choice   This falls under the theory of constraint optimization, because consumers have a limited access to resources when compared to firms. This is called as the Budget Constraint of the consumer. This can be represented using linear equations. Note that the consumer in this discussion is a Price Taker, that is, the price of the good is unchanged.   For example, assume that Good1 is 10 dollars and Good2 is 2 dollars. Let the income of the consumer be 100, then the linear equation would be: \\(10\\cdot G_1 +2\\cdot G_2 = 100\\) Theoretically, we should be having a $\\leq$ in the above equation, because savings might be present. However, we are assuming that the problem is static in nature. That is, we assume that this is a one-period problem where saving money is irrelevant in the given time frame.   The slope of the Budget Constraint graph gives the rate at which one good can be exchanged for another. This is referred to as the relative price of the good.   We show the consumer’s preferences via an Indifference Curve. This is a family of curves. This curve plots the quantity of two goods which give the same amount of happiness. Mathematically, it is the locus for which happiness = k. The slope of the Indifference curve is called as the Marginal Rate of Substitution for the given pair of goods.      Higher Indifference curves are preferred to lower ones   Indifference curves are downward sloping   Indifference curves do not cross   Indifference curves are bowed inward   Perfect Substitutes - The Indifference curve is a straight line. That is, the marginal rate of substitution is a constant.   Perfect Complements - Two goods with right angle indifferent curves. These are used together, so having an excess of one adds no satisfaction.   Maximizing Happiness   Maximizing the happiness of a consumer requires the slope of the indifference curve to be equal to the slope of the budget constraint curve. That is, the Marginal rate of substitution should be equal to the relative price. At this optimal point, the person’s valuation equals the market valuation of the good. We shall now see how a change in factors cause the curves to change.      An Increase in income - The budget constraint line shifts outwards. We can say if a good is normal or inferior based on how the quantity bought changes.   A Change in Price - The equation of the budget constraint curve is $P_1x + P_2y=I$, therefore we can see that the curve changes.   Substitution Effect   A product becoming cheaper causes people to buy more of it, even if they move along the same indifference curve.   Income Effect   A product becoming cheaper will cause people to buy more of it, causing people to move to a higher or lower indifference curve. If its a normal good, more of the product is bought. Inferior goods will be bought less.   Giffen Goods   These goods are inferior in nature with an upward sloping demand curve. That is, the income effect dominates over substitution effect, in violation of the law of demand. For such goods, the demand rises as the price increases. (inferior is a misnomer for Giffen goods!)  ","url": "http://localhost:4000/notes/hs101/mac/"
    },{
      "title": "Deep Learning and CNNs",
      "excerpt":"       These are the notes made by me while I was trying to implement this paper. A rigorous mathematical approach is not followed (mainly to streamline the process of note making), rather, I have noted down the concepts and the intuition behind the concepts. Mathematical analysis of the topics covered can be found here.   Use the navigation ui on the left to browse through my notes. The results of various netowrks constructed are summarized below.   Classification Networks’ Structure  Three networks based on the structure of AlexNet, VGGNet and ResNet have been constructed for the MNIST dataset. Their architecture, and graphs comparing their loss with epochs are shown below.                                                                                                                                Graphs for AlexNet, VGG-Net and ResNet respectively                                                                                              Modified AlexNet and VGG-Net Architectures       ResNet’s architecture is identical to the one described in the above linked paper.   ResNet has been implemented below. You can press the button to generate two random images and classify them in real time.   (This might take upwards of 30 seconds because heroku app would need to boot up. I only did this because I thought that executing a script in a section called “Executive Summary” was funny for some reason. That’s an entire weekend that I am never getting back.)       Classify!      Generative Adversial Networks’ Architecture   One GAN has been implemented for the MNIST dataset so far. The architectures of the generator and the discriminator are displayed below, along with the “Loss vs Epochs” graph obtained during the training. Do notice that this GAN needed to be trained for much longer than any of the above classification networks.      Again, you can click on the button below to obtain a random output of the network. Not all the generated images resemble a digit, but most of them do.       Generate!     ","url": "http://localhost:4000/notes/dl/intro"
    },{
      "title": "Foundations of Intelligent and Learning Agents",
      "excerpt":"      The notes of CS747 have been divided weekly, and they can be found below; or accessed via the sidebar.      Week1   Week2   Week3   Week4   Week5  ","url": "http://localhost:4000/notes/cs747"
    },{
      "title": "Supervised and Unsupervised Learning",
      "excerpt":"        Supervised Learning is when a goal is acheived by learning from train data. The data contains true labels as well, and examples include linear regression and classification.   Unsupervised Learning is when objects similar to each other are grouped together. Examples include clustering and dimensionality reduction. The desired output is unobserved.   There are three canonical learning settings:          Regression - Supervised   Estimate parameters, such as least square fit            Classification - Supervised   Given parameters about an object, assign a label to it.            Unsupervised Learning   Clustering, and dimensionality reduction are prominent examples.           Supervised Learning   Formally, let $\\mathcal{X}$ be the input space and $\\mathcal{Y}$ be the output space. We would like to obtain a function $f$ belonging to the function family $\\mathcal{F}$ such that $y_i \\approx f(x_i)$, where $(x_i, y_i) \\in \\mathcal{X} \\times \\mathcal{Y}$.   In linear regression, $\\mathcal{F}$ is the Linear Function Space.   It is not guaranteed that the training data is error-prone. We would like the final estimator to be robust to errors, and one way to do this is Data Cleansing (pre-processing).   Error Function   The error function $\\mathcal{E}$ takes the curve and data as input and yields a real number as the output. This is used to quantitatively judge whether a function is a “good fit” for the given data.                  Some examples of $\\mathcal{E}$ are $\\sum       f(x_i)-y_i       $ and $\\sum (f(x_i)-y_i)^2$. We would ideally want the error to always be positive (so that stuff doesn’t cancel out).           Using the error function $\\sum (f(x_i)-y_i)^2$ is known as the  Method of Least Squares. This error function is widely used.  ","url": "http://localhost:4000/notes/cs337/Lec1"
    },{
      "title": "Linear Regression",
      "excerpt":"        Regression is about learning to predict a set of output (dependent) variables as a function of input (independent) variables.   Consider the inputs to be of form $&lt;x_i, y_i&gt;$. Attributes of $x$ are (non-linear) functions $\\phi$ which operate on $x$. An equation which linear regression tries to optimize is:      $$ Y = \\sum_{i=1}^n w_i\\phi_i(x) + b = W^T\\Phi(x) + b $$   Where $\\Phi$ is a vector of all attributes, and $W$ of all weights.   Do note that $b$ can be dropped by defining $\\widetilde{w}, \\widetilde{\\Phi}$ with one additional element being $b$ and $1$ respectively.   Linear regression is linear in terms of weights and attributes, and (generally) non-linear in terms of $x$ owing to $\\Phi$.   For example, $\\phi_1(x)$ could be the date of investment, $\\phi_2$ could be value of investment and so on.   There are general classes of basis functions, such as:     Radial Basis function   Wavelet function   Fourier Basis   Formal Notation   Dataset $\\mathcal{D} = &lt;x_1, y_1&gt; \\ldots &lt;x_m, y_m&gt;$.   Attribute/basis functions $\\phi_i$, and the general class of basis $\\Phi$ is given as shown below. Do note that we have redefined the value of $\\Phi$ now, and we shall be using this definition from here on.      $$\\Phi =    \\begin{bmatrix}     \\phi_1(x_1) &amp; \\phi_2(x_1) &amp; \\ldots &amp; \\phi_p(x_1) \\\\     \\vdots      &amp;             &amp;        &amp;             \\\\     \\phi_1(x_m) &amp; \\phi_2(x_m) &amp; \\ldots &amp; \\phi_p(x_m) \\\\   \\end{bmatrix}$$   The equation with the above redefinition becomes finding an optimal $W$ such that $Y = \\Phi W$.   General regression is the following problem;      $$\\hat{f} = \\min_{f\\in\\mathcal{F}} E(f, D)$$   Parameterized Regreesion is a bit more complex, it involves the optimization of weights in the above definition for a given $f(\\phi(x), w, b)$ for minimizing error.      $$ w* = \\min_{w,b} \\left[ E(f(\\phi(x), w, b), D) \\right] $$   The error function determines the type of regression. Some examples are given below. These will be discussed later in the course.      Least Squares Regression   Ridge Regression   Logistic Regression   Least Square Solution   Formally, the solution is given by:      $$ w* = \\min_{w,b} \\sum_{j=1}^m \\left( \\left( \\sum_{i=1}^p w_i\\phi_i(x_j) + b - y_j \\right)^2 \\right) $$   If the “true” relation between $X$ and $Y$ was linear in nature, then 0 error is attainable. That is, $y = \\Phi W$ exists, or Y belongs to the column space of Phi. We can just solve linear equations to get the optimal value of $W$.   If $Y$ is not in the column space of $\\Phi$, the closed form solution for optimal weights $W*$ is given by:      $$W* = \\left(\\Phi^T\\Phi\\right)^{-1}\\Phi^TY$$   Do note that $\\Phi^T\\Phi$ is invertible iff it has full column rank. That is:     All columns are linearly independent of each other   The columns are not data driven   It can be proven that Gradient Descent converges to the same solution as well.  ","url": "http://localhost:4000/notes/cs337/Lec2"
    },{
      "title": "Probabilistic Linear Regression",
      "excerpt":"        We can model $Y$ to be a linear function as before, with an additional noise parameter $\\epsilon \\sim \\mathcal{N}(0,\\sigma^2)$. That is, the relation is      $$ Y = W^T\\Phi(x) + \\epsilon $$   Normal distribution has the maximum entropy amongst all distributions with a given variance. The $3-\\sigma$ rule also plays a vital role for picking this. (68, 95, 99.7)   The maximum likelihood estimate of $W$ is calculated in this case. (Remember that calculation is simplified by taking $\\log$ to get the Log-Likelihood)       Overfitting and Regularization   Increasing the degrees of freedom causes this issue. The model essentially brute-forces all data points in training data, which isn’t very helpful. Overfitting is caused by $\\vert\\vert W_i\\vert\\vert$ being large, and regularization tries to suppress this.   Bayesian Linear Regression   The problem of overfitting is addressed by a prior. That is, the prior models $W$ by bounding the norm to be smaller than some $\\Theta$.   To understand this better, we shall first tackle a simpler coin-tossing example.   I have a newly minted coin which I believe to be fair. I now flip it four times and get 4 heads. The MLE would be 1, but this isn’t taking the prior belief into account. The posterior would be given by the baye’s rule.   The Prior is given by $P(H)$, and the Posterior is given by $P(H\\vert D)$. Therefore, the relation is given by:      $$P(H|D) = \\frac{P(D|H)\\times P(H)}{P(D)} \\propto P(D|H)\\times P(H)$$   We ignore the denominator as it is just a normalizing factor, and is constant as the data is known.   If $P(D\\vert H)$ follows distribution $d_1$ the posterior and prior follow the same distribution $d_2$, then $d_2$ is said to be the conjugate prior of $d_1$.   Examples include:     Gaussian - Gaussian   Bernoulli &amp; Binomial - Beta   Categorical &amp; Multinomial - Dirchlet   Beta Distribution   Wikipedia page link: Beta distribution   In short, the beta distribution has two parameters $\\alpha, \\beta$. The value of the distribution always lies between 0 and 1. The pdf is given by:     $$\\begin{eqnarray}  \\text{Beta}(\\alpha, \\beta) &amp;= \\frac{x^{\\alpha-1}(1-x)^{\\beta-1}}{B(\\alpha, \\beta)}\\\\  B(\\alpha, \\beta) &amp;= \\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha+\\beta)}\\\\  \\end{eqnarray}$$      The advantage of conjugate distributions is that the posterior is always of the same family as the prior, with just the parameters changed. This makes calculations very easy.   In the above coin tossing example, we know that Bernoulli and Beta are conjuagtes. Therefore, we can model the prior of $p$ as a BETA distribution $\\text{Beta}(\\alpha, \\beta)$ for simplicity.   If there are $h$ heads occurring in $n$ tosses, then the posterior distribution turns out to be $\\text{Beta}(\\alpha+h, \\beta+n-h)$.   If we have no prior information regarding $p$, we can model it as a beta with both parameters equal to 1. (The beta is uniform in 0 to 1 when both parameters are 1, just what we intend to have)  ","url": "http://localhost:4000/notes/cs337/Lec3"
    },{
      "title": "Bayesian Linear Regression",
      "excerpt":"        As discussed earlier, we would like the norm of $W_i$ to be small. For the sake of simplicity, we limit ourselves to a univariate case.   Here, we just want the value of $W$ to be small. We can thus assume that the prior for $W$ is a gaussian distribution of 0-mean and variance $1/\\lambda$.   Recall that the product of two gaussians, $\\mathcal{N}(\\mu_1, \\sigma_1^2)$ and $\\mathcal{N}(\\mu_2, \\sigma_2^2)$ is also a gaussian distribution with the following parameters:      $$\\begin{eqnarray}     \\frac{1}{\\sigma^2} &amp;= \\frac{1}{\\sigma_1^2} + \\frac{1}{\\sigma_2^2} \\\\     \\frac{\\mu}{\\sigma^2} &amp;= \\frac{\\mu_1}{\\sigma_1^2} + \\frac{\\mu_2}{\\sigma_2^2} \\\\    \\end{eqnarray}$$   Consider the univariate case where a random variable $X$ is a gaussian $\\mathcal{N}(\\mu, \\sigma^2)$, and $\\sigma$ is known. We have prior distribution of $\\mu$ as well, $\\mathcal{N}(\\mu_o, \\sigma_o^2)$. Given $m$ datapoints of x, it can be quite easily seen that the posterior distribution of $\\mu$ would be $\\mathcal{N}(\\mu_m, \\sigma_m)$, where:      $$\\begin{eqnarray}     \\frac{1}{\\sigma_m^2} &amp;= \\frac{1}{\\sigma_o^2} + m\\frac{1}{\\sigma^2} \\\\     \\frac{\\mu_m}{\\sigma_m^2} &amp;= \\frac{\\mu_o}{\\sigma_o^2} + m\\frac{\\mu}{\\sigma^2} \\\\    \\end{eqnarray}$$   Extend to Multivariate Case   The above result can be extended to a multi-variate linear regression case as well. In the equation, $Y = W^T\\Phi + \\epsilon$, let the $\\epsilon$ distribution’s prior be given by $(\\mu_o, \\Sigma_o^{-1})$ where $\\Sigma_o$ is analogous to $\\sigma^2$. (It is the covariance matrix)   Similarly, data would be given by $(\\mu_{mle}, \\Sigma^{-1})$ (assuming that $\\Sigma$ is fixed like in univariate), and $m$ data points are available to us. The posterior distribution can be calculated to be $(\\mu_m, \\Sigma_m)$ where      $$\\begin{eqnarray}     \\Sigma_m^{-1} &amp;=  \\Sigma_o^{-1} + m\\Sigma^{-1} \\\\     \\mu_m\\Sigma_m^{-1} &amp;= \\mu_o\\Sigma_o^{-1} + m\\mu_{mle}\\Sigma^{-1} \\\\   \\end{eqnarray}$$   However, the value of $\\Sigma$ was arbitrarily introduced by us. We have been taking $\\epsilon\\sim\\mathcal{N}(0,\\sigma^2)$. We can use this and the closed form solution to the equation, $W = (\\Phi^T\\Phi)^{-1}\\Phi^TY$ to replace its value, to obtain the final solution given below.      $$\\begin{eqnarray}     \\Sigma_m^{-1} &amp;= \\Sigma_o^{-1} + (\\Phi^T\\Phi)/\\sigma^2 \\\\     \\mu_m\\Sigma_m^{-1} &amp;= \\mu_o\\Sigma_o^{-1} + (\\Phi^TY)/\\sigma^2 \\\\   \\end{eqnarray}$$   We had taken the prior on $W$ to be $\\mu_o = 0$ and $\\Sigma_o^{-1} = \\lambda I$. By substituting these values in the above equation, we get that      $$\\begin{eqnarray}     \\Sigma_m^{-1} &amp;= \\lambda I + \\Phi^T\\Phi/\\sigma^2 \\\\     \\mu_m &amp;= (\\lambda\\sigma^2 I + \\Phi^T\\Phi)^{-1}\\Phi^T Y   \\end{eqnarray}$$  ","url": "http://localhost:4000/notes/cs337/Lec4"
    },{
      "title": "MAP and Bayes Estimates",
      "excerpt":"        The posterior distribution’s mean and variance have been calculated in the previous lecture. Because the posterior is a gaussian, we can clearly understand that      $$ \\hat{w}_{MAP} = \\hat{w}_{Bayes} = \\mu_m $$   MAP is the $w$ where a global maxima is attained, and Bayes is the expected value of $w$.   Pure Bayesian   Didn’t understand anything, kek   Regularized Ridge Regression   The Bayes and MAP estimates obtained for linear regression coincide with regularized ridge regression as well.      $$w_{ridge} = \\text{arg }\\min_w \\vert\\vert \\Phi W - y \\vert\\vert^2_2 + \\lambda\\sigma^2\\vert\\vert W\\vert\\vert^2_2$$   In case of polynomial regression, increasing $\\lambda$ tends to decrease the curvature, as 2-norm is being limited.   Replacing 2-norm with 1-norm is called as Lasso Regression, and with 0-norm is called as Support Based Penalty.   The additional term is usually represented as $\\Omega(w)$, and this formulation is known as Penalized Formulation. The original problem of wanting to limit 2-norm of $w$ to be less than or equal to $\\theta$ is called Constrained Formulation.         Claim.       For any Penalized Formulation with a particular $\\lambda$, there exists a corresponding Constrained formulation with a corresponding $\\theta$.    Lasso Regression tends to yield solutions that are sparser. That is, it is more likely for elements of $w$ to be 0 using this method. (many corners)   This follows a Laplacian distribution, and has no closed form solution. Therefore, iterating with gradient descent is the only way for Lasso regression. The algorithm for lasso regression will be explained in the next lecture.  ","url": "http://localhost:4000/notes/cs337/Lec5"
    },{
      "title": "Iterative Soft Thresholding Algorithm",
      "excerpt":"        This algorithm is used for fitting the model using lasso regression.   While the relative drop in Lasso error across $t=k$ and $t=k+1$ is significant, the following two steps are done.           LS Iterate       $w^{k+1}_{LS} = w^{k+1}_{Lasso} - \\eta\\nabla E_{LS}(w^{k}_{Lasso})$            Proximal Step       If absolute value of $\\left[w^{k+1}_{LS}\\right]_i$ is less than $\\lambda\\eta$, then the $i^{th}$ element of $w^{k+1}_{lasso}$ is 0.       Else, just reduce its magnitude by $\\lambda\\eta$ while keeping the sign intact.           Evaluating Performance   Method 1: Training Error   This idea is not good enough. Error obtained via ridge regression will always be larger than the least squared error. Also, going by training error alone lets overfitting slip by.   Method 2: Test Error   Use a different dataset, seperate from the training data to evaluate loss of the model. This is good for finding if overfitting is taking place.   There tend to be three main sources of error:     Bias - Difference between true and fitted data function   Variance - Deviation of fits as sample data changes   Noise   Bias Variance Analysis   Before starting the analysis, we assume the following three points:      Noise is Additive: $y = g(x) + \\epsilon$   Noise has mean 0 and variance $\\sigma^2$, need not be gaussian   We are performing Linear fit via OLS (Ordinary Least Squares)   Let $g$ be the true function, and $f$ be the model. If $&lt;\\hat{x},\\hat{y}&gt;$ is data, then the expected value of Least Square error is:      $$\\begin{eqnarray}   \\mathcal{E}[(f-y)^2] &amp;= \\mathcal{E}[(f-\\bar{f})^2] + (\\bar{f} - g)^2 + \\mathcal{E}[(y-g)^2] \\\\   &amp;= \\text{Variance}(g) + \\text{Bias}(g)^2 + \\sigma^2    \\end{eqnarray}$$   Here $f$ is $f(\\hat{x})$, $g$ is $g(\\hat{x})$ and $\\bar{f}$ is $\\mathcal{E}[f(\\hat{x})]$. Also, remember that $\\sigma$ is the variance of noise.   We divide the dataset into three categories:     Train: to train the model   Validation: to tune the model’s hyperparameters   Test: for final testing  ","url": "http://localhost:4000/notes/cs337/Lec6"
    },{
      "title": "Classification using Perceptrons",
      "excerpt":"        We would like a classifier which maps a data point to one of the given classes. Linear regression is not viable here as the classes are NOT REAL!   A (bad) solution is to assign numbers to classes, but this imposes ordering to the classes, which is not ideal. We use perceptrons for this case, and the classes are modeled as one-hot encoded vector.   ","url": "http://localhost:4000/notes/cs337/Lec6"
    },{
      "title": "Recurrent Neural Networks",
      "excerpt":"      Recurrent Neural Networks (RNNs) are used in situations where the sequence of data is as important (if not more) than the data itself. These networks essentially have a hidden “state” which stores the data about the information seen so far.    \tRNNs see usage in Sequence generation, NLP Classification and NLP Generation.   RNNs consist of a looped network whose output is the “hidden state”. That is, every word in a sequence of $n$ words is sent one-at-a-time; and at each iteration, the current hidden state is concatenated with the next word in the sequence to form the input.   Sticking with the same example, the “hidden state” gives memory to the network. This is quite useful in the case of NLP, as the meaning of a word quite heavily depends on the words that might have come before it.     Drawbacks with RNNs    \tThere are two major drawbacks of RNNs; they have a very short-term memory due to vanishing gradient and they can \"look\" in only one direction.   Consider the sentence “I live in Germany. I speak __.” In this case, it is a very reasonable guess that the person speaks German. And it has been shown that RNNs predict the word “German” with a high degree of accuracy. However, in the sentence “I live in Germany. I am quite fond of watching movies and playing basketball. I am fluent in __.”   In this case, the word “Germany” is very far away from the blank, meaning that it is very likely for the RNN to have dismissed this information. Also, because the network has multiple iterations done, a problem of vanishing gradient arises which makes learning that “Germany” is important infeasible. This is a drawback of the technique.   Another  drawback lies in the very nature of training. The words are input sequentially, meaning that a RNNs do not have information to the data which comes AFTER the word in the sentence. This can cause issues as the meaning of a word depends on what words are present on both of its sides.    \t\"I speak _____. I am a German.\"   Also, training RNNs is slow because the words are input sequentially. GPUs’ strongest suit is parallel processing, and this is not taken advantage of because each word is sent one after another.    \tThe above problems are mitigated to some extent via utilizing LSTM networks and Bidirectional techniques such as BERT with Attention respectively.  ","url": "http://localhost:4000/notes/chemcat/rnn"
    },{
      "title": "Multi-Armed Bandits",
      "excerpt":"        Explore Exploit Dilemma   This dilemma can be stated as the question:      Do I use the experience gained so far to pick the most optimal move, or do I keep exploring and obtain more data?    Do note that it is not guaranteed for the move made by the agent (using limited data) to be optimal globally. Such a dilemma is observable in clinical trials, online advertising and packet routing in communication networks.   Formal Definitions and Notations   Stochastic Multi-armed bandit   Has $n$ arms, each of which is a Bernoulli Distribution. The $i^{th}$ arm has mean reward $p_i$, and the largest mean is $p_M$.   It is Stochastic because the distribution of each arm is known.   Algorithm   Operates on the multi-armed bandit, by deciding which arm is to be pulled; to get a reward for that action.   That is, at any time $t$, given the history ($h^t$) it chooses an arm to sample ($a^t$) to obtain a reward $r^t$.    \t$$ h^t = (a^0, r^0, a^1, \\ldots, a^{t-1}, r^{t-1}) $$   The maximum number of pulls allowed is called as the total sampling budget or horizon ($T$).   A deterministic algorithm maps the set of all histories to the set of all arms. Similarly, a randomized algorithm maps the set of all histories to the set of all probability distributions over the arms.    \t$$ \\mathbb{P}(h^t) = \\prod^{T-1}_{t=0} \\mathbb{P}(a^t|h^t)\\cdot \\mathbb{P}(r^t|a^t) $$   Using the above relation, we can see that $(2n)^T$ possible histories are possible for a deterministic algorithm acting on a stochastic multi-armed bandit, with horizon $T$.   Epsilon Greedy Algorithms   $\\epsilon$ is a parameter $\\in [0,1]$ which controls the amount of exploration. It is used in different ways, some explained below.      $\\epsilon$G1            For $t \\leq \\epsilon T$, sample an arm uniformly at random       At $t = \\lfloor \\epsilon T \\rfloor$ determine the action with the highest empirical mean, and choose this action everytime           $\\epsilon$G2            For $t \\leq \\epsilon T$, sample an arm uniformly at random       For $t &gt; \\epsilon T$, sample the arm with the highest empirical mean       That is, the value of the mean  is updated in this case after $\\epsilon T$ steps, wheras it wasn’t done previously           $\\epsilon$G3            With probability $\\epsilon$ sample an arm uniformly at random, and with $1-\\epsilon$ sample the arm with highest empirical mean           Evaluating Algorithms   This is visualized by plotting a graph between the expected reward and the time stamp of the algorithm. To gauge the performance, three horizontal lines are plotted; $p_M$, $p_{min}$, $p_{avg}$. We expect the algorithm to start out near the average and reach the max with increasing time steps.   The expected cumulative regret of the algorithm is defined as follows:    \t$$ R_T = Tp_M - \\sum_{t=0}^{T-1}\\mathbb{E}(r^t) $$   That is, the difference between the maximum possible reward and what you end up getting. Ideally, we would like the value of $(R_T/T)$ to tend to $0$ as $T$ tends to $\\infty$.   That is, we would like the algorithm’s regret to be sublinear.  ","url": "http://localhost:4000/notes/cs747/week1"
    },{
      "title": "Regret Optimization",
      "excerpt":"        Let’s analyze the performance of $\\epsilon$G1 and $\\epsilon$G2. The regret calculations are shown below:      $$\\begin{eqnarray}   R_T &amp;=&amp; Tp_M - \\sum^{T-1}_{t=0}\\mathbb{E}(r^t) \\\\   &amp;=&amp; Tp_M - \\sum^{\\epsilon T-1}_{t=0}\\mathbb{E}(r^t) - \\sum^{T-1}_{t=\\epsilon T}\\mathbb{E}(r^t) \\\\   &amp;\\geq&amp; Tp_M - (\\epsilon T)p_{avg} - T(1-\\epsilon)p_M \\\\   &amp;\\geq&amp; \\epsilon T(p_M - p_{avg}) \\\\   &amp;\\in&amp; \\Omega (T) \\\\   \\end{eqnarray}$$   That is, the regret is not sublinear, even in the best case scenario where the algorithm performs perfectly after exploration.   Now let’s do the same regret analysis of $\\epsilon$G3.      $$\\begin{eqnarray}   R_T &amp;=&amp; Tp_M - \\sum^{T-1}_{t=0}\\mathbb{E}(r^t) \\\\   &amp;\\geq&amp; Tp_M - \\sum^{T-1}_{t=0}(\\epsilon p_{avg} + (1-\\epsilon)p_M)\\\\   &amp;\\geq&amp; \\epsilon T(p_M - p_{avg}) \\\\   &amp;\\in&amp; \\Omega(T) \\\\   \\end{eqnarray}$$   That is, all three epsilon greedy algorithms discussed earlier are not sub-linear in nature.   Acheiving Sublinear Regret   There are two general heuristics which should be met for a sub-linear algorithm.           Every arm in the multi-armed bandit must be pulled infinite number of times as $T\\rightarrow \\infty$. (Infinite Exploration)            Let $exploit(T)$ be the number of pulls that are exploitative in nature. Then, for sublinear regret we need the following;            $$\\lim_{T\\to\\infty}\\frac{\\mathbb{E}(exploit(T))}{T} = 1$$     That is, nearly all of the pulls must be of exploitative behaviour. (Greedy Limit)   Now, let $\\bar{\\mathcal{I}}$ be a set of all bandit instances with reward means strictly less than 1. Then;   An algorithm L acheives sub-linear regret on all instances of $I \\in \\bar{\\mathcal{I}}$ iff the algorithm satisfies both the above mentioned conditions.   These conditions are called as GLIE in short, which stands for “Greedy Limit Infinite Exploration”.   Modifying epsilon greedy strategies   $\\epsilon$G1/2 can be modified slightly to make it “GLIE compliant”, instead of exploring for $\\epsilon T$ pulls, we explore for $\\sqrt{T}$ pulls.      C1 satisfied since each arm pulled $\\sqrt{T}/n$ times on average   C2 satisfied as $exploit(T)$ would be $T-\\sqrt{T}$   Similarly, $\\epsilon$G3 can be fixed by making epsilon a function of $t$, as $1/(t+1)$. It can be seen pretty easily that the conditions are satisfied, using the below equation.      $$\\sum^{T-1}_{t=0}\\frac{1}{n(t+1)} = \\Theta\\left(\\frac{\\log T}{n}\\right)$$   Lai and Robbins Lower Bound   This result establishes that the lower bound on the regret  attainable for a sub-polynomial algorithm is logarithmic in $T$.   It has been stated more formally below; note the little-o notation.   If $L$ be an algorithm such that for every bandit $I\\in\\bar{\\mathcal{I}}$ and for every $\\alpha&gt;0$, as $T\\rightarrow\\infty$      $$R_T(L,I) = o(T^\\alpha)$$   Then, for every bandit instance $I\\in\\bar{\\mathcal{I}}$ as $T\\rightarrow\\infty$      $$ \\frac{R_T(L,I)}{ln(T)} = \\sum_{a:p_a(I)\\neq p_M(I)}\\frac{p_M(I) - p_a(I)}{KL(p_a(I), p_M(I))} $$   Where, $KL(x,y) = xln(x/y)+(1-x)ln((1-x)/(1-y))$   (Notice that the RHS of second equation is constant for a given bandit)   Sublinear Algorithms   UCB   At every time $t$ and arm $a$, define $\\text{ucb}^t_a$ as follows:        $$\\text{ucb}^t_a = \\hat{p}^t_a + \\sqrt{\\frac{2ln(t)}{u^t_a}}$$     Where $\\hat{p}^t_a$ is the empirical mean of that arm, and $u^t_a$ is the number of times that arm has been pulled. (Pull all the arms once before calculating)   The algorithm samples the arm with the highest ucb. This acheives a regret of $O(\\log(T))$, the optimal dependance on $T$.   KL UCB   Although UCB is optimal order-wise, the constant is still different. KL-UCB fixes this by changing the definition of UCB slightly.        $$\\text{kl-ucb}^t_a = \\max\\{ q\\in[\\hat{p}^t_a,1]\\text{ where } u^t_a KL(\\hat{p}^t_a,q)\\leq ln(t)+cln(ln(t)) \\}$$         $$\\text{where } c\\geq 3$$     Notice that $KL(\\hat{p}^t_a,q)$ monotonically increases with $q$, easy to find value by binary search! This algorithm asymptotically matches the Lai and Robbins’ Lower Bound as well.   Thompson Sampling   This algorithm uses Beta Distribution, and it’s parameters are given below. Note that this distribution is always going to give values between 0 and 1.   \\[Beta(\\alpha, \\beta) \\rightarrow \\mu = \\frac{\\alpha}{\\alpha+\\beta}, \\sigma^2 = \\frac{\\alpha\\beta}{(\\alpha+\\beta)^2(\\alpha+\\beta+1)}\\]      At time $t$, let arm $a$ have $s^t_a$ successes and $f_a^t$ failures. Then, $Beta(s^t_a+1, f_a^t+1)$ represents a belief about the true mean of that arm.   For every arm $a$, draw a sample $x^t_a \\sim Beta(s^t_a+1, f_a^t+1)$ and chose the arm which gave the maximal $x^t_a$ (and update the distribution).   This acheives optimal regret, and is excellent in practice. Usually, it performs slightly better than KL-UCB as well.   All these algorithms are examples of optimism in face of uncertainity principle.  ","url": "http://localhost:4000/notes/cs747/week2"
    },{
      "title": "Hoeffding's Inequality",
      "excerpt":"        Let $X$ be a random variable in $[0,1]$ with $E[X] = \\mu$. Now, let’s say that we draw $u$ samples from this distribution, each $x_i$, and the empirical mean is $\\bar{x}$.   Then, for any fixed $\\epsilon&gt;0$ we have      $$\\begin{eqnarray}     \\mathcal{P}\\{ \\bar{X} \\geq \\mu+\\epsilon} \\leq \\exp{-2u\\epsilon^2} \\\\     \\mathcal{P}\\{ \\bar{X} \\leq \\mu-\\epsilon} \\leq \\exp{-2u\\epsilon^2} \\\\   \\end{eqnarray}$$   Intuitively, as $u$ increases, the probablity that empirical mean deviates should decrease. Here, $\\epsilon$ acts as Confidence Interval around the true mean (which is unknown).   Doubt in Slide 4, point 2; It doesn’t seem right   Hoeffding’s Inequality can be extended to a random variable bounded in $[a,b]$ by defining a new random variable $y = \\frac{X-a}{b-a}$, and applying to this. We get;      $$\\begin{eqnarray}     \\mathcal{P}\\{ \\bar{X} \\geq \\mu+\\epsilon} \\leq \\exp{\\frac{-2u\\epsilon^2}{(b-a)^2}} \\\\     \\mathcal{P}\\{ \\bar{X} \\leq \\mu-\\epsilon} \\leq \\exp{\\frac{-2u\\epsilon^2}{(b-a)^2}} \\\\   \\end{eqnarray}$$   KL Inequality   This gives a tighter bound for empirical mean. Note that $\\mu\\pm\\epsilon$ must be positive in RHS of both the inequalities.      $$\\begin{eqnarray}     \\mathcal{P}\\{ \\bar{X} \\geq \\mu+\\epsilon} \\leq \\exp{-uKL(\\mu+\\epsilon, \\mu)} \\\\     \\mathcal{P}\\{ \\bar{X} \\leq \\mu-\\epsilon} \\leq \\exp{-uKL(\\mu+\\epsilon, \\mu)} \\\\     \\\\     KL(p,q) = pln(\\frac{p}{q})+(1-p)ln(\\frac{1-p}{1-q}) \\\\   \\end{eqnarray}$$   Note that both the inequalities are examples of Chernoff bounds.       Analysis of UCB   We shall use the above inequalities to prove that the UCB algorithm is logarithmic in nature.   Notation      $\\Delta_a = p_M - p_a$ - The difference between this arm and optimal   $Z^t_a$ - An event when arm $a$ is pulled at time $t$   $z^t_a$ - Indicator for the above event (1 if event occurs, 0 otherwise)      $$\\mathcal{E}\\[z^t_a\\] = \\mathcal{P}\\[Z^t_a\\]$$      $u^t_a$ - Number of times arm $a$ has been pulled, till time $t$     $$u^t_a = \\sum_{i=0}^{t-1}z^i_a$$      $\\bar{u}^T_a$ - A constant used in the proof, sufficient number of pulls of arm $a$ for horizon $T$     $$\\bar{u}^T_a = \\ceil{ \\frac{8}{(\\Delta_a)^2}ln(T) }$$   This proof truly be a mind = blown moment :/       Thompson Sampling  ","url": "http://localhost:4000/notes/cs747/week3"
    },{
      "title": "Markov Decision Problem",
      "excerpt":"        A Markov Decision problem $M$ is given by the tuple $(S, A, T, R, \\gamma)$. Each of these elements are defined below.      $S$ is the set of states. $\\vert S\\vert = n$        $A$ is the set of actions. $\\vert A\\vert = k$            $T$ is the Transition function. $T(s, a, s’)$ gives the probability of reaching $s’$ from $s$ by taking the action $a$. Note that $T(s,a,\\cdot)$ is a probability distribution over $S$.            $R$ is the Reward function. $R(s,a,s’)$ gives the reward for reaching $s’$ from $s$ via action $a$. We assume $R(s,a,s’)\\in [ -R_{max}, R_{max} ]$.       $\\gamma$ is the Discount factor.   Let $s^t$ denote the state at the $t^{th}$ time-step. Similarly, $a^t$ is the action taken at this time-step and $r^t$ is the reward for this action.   Trajectory is the history of the agent.   A policy $\\pi$ is a function which the agent follows to give an action at any state. For simplicity, let the agent look only at the current state for its policy. That is, $\\pi: S\\to A$. This policy is:     Markovian - Looks only at the current state and not at history   Deterministic - A single action is given, not probability distribution   Stationary - The output doesn’t change with time   $\\Pi$ is the set of all policies. In this case, $\\vert \\Pi\\vert = k^n$. We would like a policy which maximizes the overall reward obtained.   Value of a State   The value of a state under a policy $\\pi$, $V^{\\pi}(s)$ tries to tell us how “good” the state is for accumulating a large reward.      $$ V^{\\pi}(s) = \\mathcal{E}_{\\pi}\\left[ r^0 + \\gamma r^1 + \\gamma^2r^2 + \\ldots \\vert s^0 = s \\right] $$   Large $\\gamma$ means a large “lookahead”. The agent looks further ahead into the future to decide upon the value of the state. Do note that $\\gamma\\in [0,1)$       MDP Planning Problem   It can be mathematically proven that every such MDP has an optimal Markovian, deterministic, stationary policy $\\pi^*$ such that      $$ \\forall\\pi\\in\\Pi, \\forall s\\in S: V^{\\pi*}(s)\\geq  V^{\\pi}(s)$$   That is, the value of every state is larger in the optimal policy. This policy is NOT unique! The MDP Planning Problem is centered around finding this optimal policy $\\pi^*$.       Alternative Formulations   Reward Function   The reward function can be defined differently in literature. For example, it could be stochastic instead of deterministic. It might be just dependant on $(s,a)$ instead of $(s,a,s’)$.   Some authors minimize cost instead of maximizing reward. The core idealogy remains the same, however.   A multi-armed bandit is an MDP with a single state and each of the actions being associated with a uniform distribution as a reward!   Episodic Tasks   Our discussion used continuing tasks, where trajectories are infinitely long. Episodic tasks have a terminal/sink state from which outgoing transitions are absent.   There should be non-zero probability of reaching the sink state from every non-terminal state, meaning that trajectories would surely be finite.   Value function   The definition used by us is called as Infinite Discounted Reward. There are other choices as well:      Total Reward - Can only be used on episodic tasks (Obv)   Finite Horizon Reward - Set a horizon $T$ and sum up rewards till here. Optimal policies for this need not be stationary.   Average Reward - Exactly what the name suggests       Policy Evaluation   The process of calculating the value of each state given the MDP is called as policy evaluation. Bellman’s Equations are used for this procedure.   These equations are used to find the values of every state for a given MDP. For every state $s\\in S$;      $$V^{\\pi}(s) = \\sum_{s' \\in S} T(s, \\pi(s), s')\\left[ R(s, \\pi(s), s') + \\gamma V^{\\pi}(s') \\right]$$   There are $n$ such linear equations, and $n$ unknowns.     unique solution guaranteed if $\\gamma &lt; 1$   for episodic, if $\\gamma=1$, solution guaranteed if value of terminal state fixed as $0$.       Action Value Functions   The naive method of finding $\\pi^*$ for a given MDP would be to use Bellman’s equations for each policy and get the minima. This would take $\\text{poly}(n,k)\\cdot k^n$ time.   We define the Action Value function $Q^{\\pi}(s,a)$ as the first step for obtaining a more efficient solution.   $Q^{\\pi}(s,a)$ is the expected long term reward for starting at state $s$, taking action $a$ at $t=0$ and then following $\\pi$ for $t&gt;0$.   Its value is given by the equation:     $$ Q^{\\pi}(s,a) = \\sum T(s,a,s')\\left[ R(s,a,s') + \\gamma V^{\\pi}(s') \\right] $$   We know that all optimal policies have same optimal value function, and by extension, all optimal policies will have the same action value function as well.  ","url": "http://localhost:4000/notes/cs747/week4"
    },{
      "title": "MDP Planning ALgorithms",
      "excerpt":"        We would like to solve the MDP Solving problem efficiently. Before stating the algorithm, we shall first go over a bunch of theoretical results.       Banach Space   A complete, normed vector space is called as a Banach Space. That is, both the vector space and norm are well defined. Additionally, every Cauchy sequence must have a limit in that vector space.   A Cauchy sequence is a convergent sequence. Example, rationals are not Banach because an irrational number can be written as a sequence of rational numbers ($\\sqrt{2}$)   Contraction Mapping   A mapping $T:X\\to X$ is called a contraction mapping with contraction factor $L$ if $\\forall u,v\\in X$,   \\[\\vert\\vert Tv-Tu\\vert\\vert \\leq L\\vert\\vert v-u\\vert\\vert\\]  $x^*$ is a fixed point of $T$ if $Tx^*=x^*$.   Banachs Fixed-Point Theorem   This theorem states that for a given contraction mapping and a contraction factor $L\\in[0,1)$:      There exists a unique fixed point   For $x\\in X, m\\geq 0; \\vert\\vert T^mx-x^*\\vert\\vert \\leq L^m\\vert\\vert x-x^*\\vert\\vert$       Bellman Optimality Operator   $B^* : \\mathcal{R}^n\\to\\mathcal{R}^n$ for an MDP $(S,A,T,R,\\gamma)$ is given as follows;    $$ (B^*(F))(s) = \\max_{a}\\sum_{s'}T(s,a,s')\\left[ R(s,a,s') + \\gamma F(s') \\right] $$   That is, we have used notation to redefine the domain of value function to be $\\mathcal{R}^n$, and $B^*$ maps value functions.   Theorem. $(\\mathcal{R}^n, \\vert\\vert\\cdot\\vert\\vert_{\\infty})$ is  a Banach space, and $B^*$ is a contraction mapping in this space with contraction factor $\\gamma$.      $$ \\vert\\vert F\\vert\\vert_{\\infty} = \\max (\\vert f_1\\vert, \\ldots \\vert f_n\\vert) $$   Bellmans Optimality Equations   The fixed point can be obtained by solving the equation:    $$ V^*(s) = \\max_{a}\\sum_{s'}T(s,a,s')\\left[ R(s,a,s') + \\gamma V^*(s') \\right] $$   There are $n$ variables and $n$ unknowns, but the equations are non-linear in nature. These equations are called as the Bellman’s Optimality Equations for the given MDP. Algorithms for solving these equations will be discussed below.   We shall also prove later that $V^*$ is an optimal value function for a given MDP.       Value Iteration   This is a very simple algorithm. We apply the optimality operator iteratively until the difference is negligible. We know this works from the Banach’s Fixed point operator.   Also, notice that $V^*, Q^*, \\pi^*$ have a cyclic relationship.      $V^*\\to Q^*$: definition of $Q^*$   $Q^*\\to \\pi^*$: $\\text{arg}\\max_aQ^*(s,a)$   $\\pi^*\\to V^*$: Solve bellman equations for $\\pi^*$       Linear Programming   This is a technique where a Linear objective function of variables under given linear constraints is maximized. We can frame the Bellman’s optimality equations as a linear programming problem.   From the Bellman optimality equations, we can say the following. Notice that there are $nk$ linear constraints and $V^*$ staisfies all these constraints.    $$ V(s) \\geq \\max_{a}\\sum_{s'}T(s,a,s')\\left[ R(s,a,s') + \\gamma V^*(s') \\right] $$      $B^*$ preserves $\\succeq$   From this, we can say that $V \\succeq V^*$, and can thus linearise the result to be $\\sum_s V(s)\\geq \\sum_s V^*(s)$.   Therefore, the objective function to be maximized is given by    $$ -\\left( \\sum_s V(s) \\right) $$   This LP problem has $n$ variables and $nk$ constraints. Do note that a dual is possible which solves for $\\pi^*$ with $nk$ variables and $n$ constraints.  ","url": "http://localhost:4000/notes/cs747/week5"
    },{
      "title": "",
      "excerpt":"Sed     Sed is a “Stream Editor”. Meaning unlike a traditional editor, examination window is limited to a single screen. Sed is called as sed &lt;options&gt; &lt;script&gt; input_files. The line which sed is currently reading is loaded into the “pattern space”. We then modify this line via the script, and then we either hold the line, or output the modified file. The modified line goes into std::out; but can be redirected.   Passing Scripts to sed   &lt;script&gt; variable is passed either in-line enclosed within quotes, or with the -f option and writing the script in a text file and doing sed -f &lt;script file&gt; input_files.   Regular Expression Syntax        /^x1/: Search for lines beginning with x1   Options      -n : Don’t print lines by default. That is, usually, sed in.txt would print all lines, and we don’t want that. So, if we do sed -n '/search/p' in.txt would only print lines with the word “search”. Similarly sed -n '1,10 p' in.txt would print the first 10 lines.   -s: Treats independent files differently. Meaning, sed -n '1,10 p' in1.txt in2.txt would only apply the script to in1.txt and not in2.txt. But if we wanted to print the first ten lines of both, then this option is used. sed -n -s '1,10 p' in1.txt in2.txt   -r: enables usage of extended regular expressions, for convenience   -e: Allows grouping of commands together, example given below.     Scripts   A script is of the form: [address] [!] command; [] represent that the argument is optional. If address is not given, command is applied for all lines. ! is the NOT operator.   Address Usage      sed -n '3 p' in.txt: prints the third line   sed -n '$ p' in.txt: Prints the last line   sed -n -e '10 s/__/  /' -e '10 p' in.txt: in s/__/  /; s stands for replace, and here we replace ‘__’ with ‘    ‘; after which that line is printed.   sed -n -r '/19D[0-9]{6}/ p' in.txt: Print lines in the form “19D123456”   Using regular expressions, we can do some powerful stuff. Suppose, I wanted to print the lines before line 15, which started with the word engi; we do this as sed -n -r '/^engi/,15 p' in.txt  and to print all statements which are between lines starting with x1 and x2; sed -n -r '/^x1/,/^x2/ p' in.txt   Nesting is done using the curly braces {}; for example, to show all statements between lines 4 and 20 beginning with x1 is done as sed -n -e '4,20{^x1 p}' in.txt   Commands      = : print the line number and p: print the line   Modify Commands           Insert i\\: Used to insert the given string before the line address. sed '10 i\\ /'This string is inserted at line 10'/' in.txt             Append i\\: Similar to insert, but the new line is AFTER the given line number. Insert and append dont work for ranged addresses.            change c\\: Change the entire matched line with given text. sed -n -r '/19D[0-9]{6}/ c\\ Dual Degree' in.txt would change entire row to just “Dual Degree”            delete d\\: Similar to Change, but deletes the entire line. Can be used for all types of addresses.            Substitute s:       s/x1/x2/[flags]; replaces the first occurrence of x1 with x2. If /g (global) is passed into flag; all occurrences are removed.       &amp;:   To find x1, and replace with x1+x2; do sed -n 's/x1/&amp;x2/' in.txt       n\\:  Suppose we wanna replace “Blue is very sus” to “Red is not sus” (Imagine we have roll no instead of colours and using -r)  sed 's/Blue( is )very( sus)/Red\\1not\\2' in.txt; \\1 holds the value in the first parentheses and so on.            Transform y Syntax: [address1],[address2]y/x1/x2/ : replace occurrences of x1 with x2; x1 and x2 must have same length       I/O Commands      n: (default) Read a line, without \\n char, execute commands, print if -n is not used   N: Read a line, add a newline, add to pattern space and execute. Used to join lines. sed -r 'N; s/\\n//' in.txt; joins lines 1,2; 3,4 and so on   p: Copies entire pattern space to output                 P: Prints only the first line of the pattern space   h: Overwrites pattern space onto hold space          H: Appends pattern space to hold space (pattern space unchanged in both)   g: Overwrites hold space onto pattern space           G: Appends hold space to pattern space(hold space unchanged in both)   d: deletes pattern space   r filename: reads file and sends to output. The file is not passed to pattern space.   w filename: writes to the file from the pattern space   b: Similar to goto statement; can be used for if-else statements ig, usage given below   q: Quit reading the file   Script Writing   #!/bin/bash #just put each indiv word in a new line sed -n -r ' /19D[0-9]{6}/ b save\t#branch to save; otherwise continue \tw others.txt \tb\t\t\t\t\t#branch to end \t \t:save \tw dd-students.txt ' in.txt    Awk     Scans every line, splits each line into field, compare fields to pattern, perform action. The difference between awk and sed is that awk has capability to remember history between files. The syntax of the script is very similar to C.   #Basic Awk Syntax awk [options] 'script' file(s)\t\t\t#Direct console awk [options] -f scriptfile file(s)\t\t#input via file   The script part is as follows: pattern [action]; if pattern is missing then action is done for all lines, and if action is missing, then matched line is printed. Either pattern or action must be given.   A line is divided into fields by awk by using a delimiter (default - whitespace); and each of these fields can be accessed by using variables, $1 refers to the first field and so on.   Pre-Defined Variables      FS – Field Separator, default: whitespace   RS – Record Separator, default: newline   NF – Number of fields in current record   NR – Number of the current record – idx   OFS – Output Field Separator, default: whitespace   FILENAME - Current Filename   ls -al | awk '{print NR, $9}'\t\t#Print all files with numbering   ##   Script structure   The scripts are divided into three parts; BEGIN, BODY, END. The statements in BEGIN are executed once(declare variables and stuff), the ones in BODY are executed for every line, and the statements in END are executed once at the end of the file (post processing).   #Structure of Body pattern {statement} pattern {statement; statement; ...} pattern { \tstatement \tstatement \t... }   Pattern Types           awk '/special/ {print}': prints the lines with the word “special”.            Instead of this, we can check if the field matches a regular expression as: awk -F, '$1~/19D[0-9]{6}/{print NR, $0}'; meaning if the student is dd, then print his name with numbering.            ~ stands for Matching; and !~ stands for not matching in this case            However, they need just be pattern matching; we can use boolean arguments as well such as $1 + $2 &gt; 5 {statement}       Expressions   #We are feeding the input from wc -c *; in which the first variable is the number of characters of the given line awk ' BEGIN{ \tlines=0; char=0; } { \tlines++; char+=$1; } END{ \tprint lines, \" lines read\"; \tprint \"Total characters: \",char; } '   Passing Variables to awk   To pass the variables from the shell script to the awk script, use the flag -v; as:- awk -v var1=\"$shell_var\" '{script}'   Associative Arrays   No need for pre-allocating, they get defined with use.   awk -F, ' \t#($9~/[A-Z][A-Z]/){ \t\tstate[$9]+=$10 \t} \tEND{ \t\tfor(i in state){ \t\t\tprint state[i], i; \t\t} \t} '   Output Statements      print: simple w/o no formatting   printf: print with formatting   sprintf: format a string   ","url": "http://localhost:4000/notes/cs251a_bash/"
    },{
      "title": "Basic Methods",
      "excerpt":"     Linear Regression   The main aim is to estimate a linear equation representing the given set of data. There are two approaches to this.      A closed form solution.  This can be directly obtained by solving the linear differential equation. Calculate the partial derivative of the error function wrt x, y and equate both of them to zero to get the values of the parameters which minimizes the error function.   An iterative approach.  This is similar to Gradient Descent. We try to obtain the minima (L1, L2 norm etc) by calculating the gradient at each point and moving in small steps along the gradient vector.  Refer to this video for more details.   Logistic Regression   Refer to the following link to see an example of logistic regression.  ","url": "http://localhost:4000/notes/dl/basic"
    },{
      "title": "Convolutional Neural Networks",
      "excerpt":"    These networks are best suited for image processing, and the number of connections in between the hidden layers is decreased by a significant factor. This decrease is possible because the “meaning” of a pixel usually depends only on its neighbouring pixels, and not the entire image. A few basic terminologies are discussed below.       Basic Terminologies          Convolution       This step involves having a kernel of fixed size move across the image (with a stride that need not be 1) and produce a feature map which makes it easier for the network to train. Many kernels can be operated on a single image, giving many feature maps.       Do note that the kernel must always be odd-sized.            Pooling       The size of a map is reduced by first dividing it into smaller parts, each with size m×m. Each of these smaller squares is replaced by a single pixel, usually by taking the max or the average of all the values in that cell.            ReLU       This introduces non-linearity in the system so as to make the network more flexible in representing a large variety of functions.            Fully Connected Layers       Once all the “pre-processing” of the image via convolution and pooling is done, the resultant values are passed into a fully connected layer for the neurons in that layer to train. The number of layers is variable, but the output layer must have the sam enumber of neurons as the number of possible classifications. (due to obvious reasons)       Types of Convolution          Dilated Convolution       Converting a 10x10 map to a smaller map of size 6x6 using a kernel of size 3 would take two consecutive convolutions. Instead of doing this twice, we can “inflate” the size of our original kernel to 5 by adding two additional rows and columns of 0s in between. This would require the convolution to be done only once, saving computational effort.       The number of rows/columns of 0s added is called as Dilation Rate.       Example:                        1 0 1 0 1    1 1 1           0 0 0 0 0    1 1 1   ⇒       1 0 1 0 1   1 1 1           0 0 0 0 0                    1 0 1 0 1                      Transposed Convolution       This is the reverse of convolution, where we increase the size of the map by padding it and applying the feature map. This is used to “estimate” what the original map might’ve been, and is used in encoder-decoder networks.           Famous CNNs          LeNet       A very simple yet efficient network. It was mainly designed for classification of MNIST data. The net consists of two succesive convolutions and pooling, followed by two dense hidden layers.            AlexNet       This is an improvement over LeNet. This is my implementation of this net using PyTorch. I have been able to obtain a classification accuracy of 95% after training the net for 15 epochs, but noticed that the training seemed to saturate after the first epoch itself.                     Classification by Modified Alexnet              VGG       This has a very similar implementation philosophy as AlexNet, we increase the number of feature maps while decreasing the size of each feature map. A small improvement here is that convolution layers are put successively, so as to save computational time.       That is, a 7x7 kernel over c sized map would need 49c² wheras having two succesive 3×3 kernels would need 27c² computations.       This is my implementation of VGGNet, and the results are shown below.                                                                                              Training and Evaluation of modified VGG Net               GoogleNet (2014 Winner of ImageNet challenge)       Convolution with kernel size larger than (or equal to) 3 can be very expensive when the number of feature maps is huge. For this, GoogleNet has convolutions using a kernel size of 1 to reduce the feature maps, followed by the actual convolution. These 1×1 feature maps are also called as Bottle Neck feature maps.       GoogleNet also utilizes Inception Modules.            ResNet (2015 Winner of ImageNet challenge)       Short for residual network, the net submitted for the challenge had 152 layers but the number of parameters are comparable to AlexNet (Which has just 4 layers). The problem of learning slowdown is tackled in a very novel way in this network.       ResNet acheives this by having “skip connections” in between the blocks of convolution. That is, let an input X be passed into a convolution block to get an output F(X). The skip connection is used to add X to this result, yielding X+F(X). The reasoning is that although the network might fail to learn from F(X); it will be able to learn from X itself directly.       My implementation of ResNet for the MNIST dataset has been shown here. I have been able to acheive an accuracy of 98%, but I do believe 99% is possible had I trained the net for longer. The feature maps and structure of the network has been modified a little to make it more easier to train, but the core idea remains the same.                                                                                              Training and Evaluation of modified ResNet       ","url": "http://localhost:4000/notes/dl/cnn"
    },{
      "title": "Deep Learning",
      "excerpt":"       This book on deep learning has been followed, this section will contain nomenclature and a few key definitions I thought were important enough.      perceptrons: older way of representing neural networks. Can give an output of either 0 or 1, corresponding to if the value of $w\\cdot x+b$ is lesser than or greater than 0.   sigmoid neurons: we usually change the values slightly in perceptrons to slowly approach the required classification function. However, because perceptrons are binary in nature, small changes can cause drastic (and unintended) changes to the output. Sigmoid neurons try to minimize this issue.   The standard sigmoid function is given as follows:    \t$$ \\sigma (w\\cdot x+b) = \\frac{1}{1+exp(-w\\cdot x-b)} $$   That is, is is a smoothened out version of the step function. We can also see that the output changes linearly with changes in inputs (using partial derivatives). $(w\\cdot x+b)$ is called as the “Weighted input” for that particular neuron, and is represented by $z$.       MLP - Multi Layer Perceptrons   These have sigmoid neurons as layers in them. The neurons taking input are called input neurons and comprise the input layer. Similarly, we have the output neurons and the output layer. Neurons (and layers) which are neither input nor output are called as Hidden Layers. We will be using a feed-forward neural network, meaning that the output of a layer always leads to an input of another layer in a linear fashion without loops. If layers have loops, they are called Recurrent Neural networks or RNNs.   For example, a neural network responsible for detecting the number in a MNIST dataset can have just three layers; Input (28×28 neurons), hidden (variable $n$) and output (10 neurons). The network is trained using a training set, and the mean squared loss function is minimized by using gradient descent.       Gradient Descent   Given a function $f(x_1, x_2)$, the minima of the function can be computed empirically by taking its partial derivative and “walking” such that the function value is reduced.    $$\\begin{eqnarray} \\Delta f &amp;=&amp; \\frac{\\partial f}{\\partial x}(\\Delta x) + \\frac{\\partial f}{\\partial y}(\\Delta y) \\nonumber \\\\ &amp;=&amp; (\\triangledown f)\\cdot(\\Delta X) \\nonumber \\\\ &amp;=&amp; -\\eta \\parallel \\triangledown f \\parallel ^2 \\nonumber \\\\ \\end{eqnarray}$$   We have substituted $\\Delta X = -\\eta \\triangledown f$ to get the above equation, which is always negative.   $\\eta$ is called as the Learning Rate, and is directly proportional to how “large” the “steps” are.   In our case, we would be applying gradient descent and changing the values of all the biases ($b_i$) and weights ($w_i$) to minimize the cost function. A drawback of this method is that calculating the cost function requires the summation of the mean squared error over all values of training data, which would be ranging in the order of $10^5$. This causes the training to be very slow.       Stochastic Gradient Descent   Instead of taking all the $n$ values in the training data set, we create a subset called the “mini set” where each element is a random subset of size $m&lt;n$. We compute the cost function over every subset in the mini set, with the assumption that the “true” cost function and the empirically calculated cost function are nearly equal. This dramatically reduces the time required for training the network.   When the mini set is exhausted, an epoch of training is said to be completed after which the process is repeated again. This is to mark the progress of training.   ** Vectorizing sigmoid function **       Back-Propagation   Assumption1: The cost function for a set of inputs is equal to the average of the cost function for each individual input. This assumption holds for the Least-Mean-Squared cost function.   Assumption2: The cost function should be a function of the outputs of the neural network.   Given the cost function C, and the weighted input z for a neuron, we define error for this neuron δ as follows;    $$\\delta = \\frac{\\partial C}{\\partial z}$$   That is, if $\\delta$ is a large value, then a change in $z$ can bring about a change in $C$. If it is zero, then it means that $C$ is optimal wrt $z$.   There are four fundamental equations to back propogation, and they have been given below. $\\delta L$ is the $\\delta$-vector for the final layer.     δL = (∂C/∂a) ⊙ σ’(z)   δⁿ = (wⁿ⁺¹)ᵀ(δⁿ⁺¹) ⊙ σ’(z)   (∂C/∂b) = δ  ⇒  Delta of a neuron is equal to the derivative of the cost function wrt to its bias   (∂C/∂wⁿⱼₖ) = aₖⁿ⁻¹ * δⱼⁿ   (Do remember that in wⱼₖ, neuron k is in the n-1’th layer and neuron j is in the n’th layer)   This is how a single iteration of training a neural network is done:     Input a set of training examples.   Feedforward the values to get the output.   Calculate the error at the outer-most layer.   Backpropogate the error till the input layer.   Perform gradient descent, as partial derivatives wrt all biases and weights is known.       Learning Slowdown and the cross entropy cost function   We’ve been using the quadratic cost function so far. It does have a few issues, notably its derivative is very small when the value of $\\sigma(z)$ is close to 0 or 1. In gradient descent, the change in biases and weights is directly proportional to the derivative of the cost function, meaning it is possible for this function to learn very slowly when it is giving wrong results. We turn to the cross entropy cost function as a solution.    $$C = -\\frac{1}{n}\\sum_x[y\\log(\\sigma(z)) + (1-y)\\log(1-\\sigma(z))]$$   It can be checked mathematically that the derivative of this cost function wrt $b$ and $x$ is independant of $\\sigma ‘(z)$, meaning no learning slowdown occurs. Moreover, the derivative is proportional to error meaning that learning occurs faster when the model is more wrong, as we would like it.   The cross entropy cost function can be defined for an entire layer as well;-    $$C = -\\frac{1}{n}\\sum_x\\sum_y[y\\log(\\sigma(z_j^L)) + (1-y)\\log(1-\\sigma(z_j^L))]$$   Here zⱼᴸ is the j’th neuron in the final layer ‘L’.   Do note that a sigmoid function coupled with the cross entropy cost function is quite similar in terms of learning slowdown to the softmax function coupled with the log-likelihood cost function. (the derivatives wrt b and x have the same behaviour)       Avoiding overfitting           Increase the size of training data       Regularization            In L2 regularization, a new term is added to the cost function as shown below. The second summation is over all the weights in the network. $\\lambda$ is called the regularization parameter.                 $$C = -\\frac{1}{n}\\sum_x\\sum_y[y\\log(\\sigma(z_j^L)) + (1-y)\\log(1-\\sigma(z_j^L))] + \\frac{\\lambda}{2n}\\sum_w w^2$$               Similarly, L1 regularization is given below:                 $$C = -\\frac{1}{n}\\sum_x\\sum_y[y\\log(\\sigma(z_j^L)) + (1-y)\\log(1-\\sigma(z_j^L))] + \\frac{\\lambda}{2n}\\sum_w |w|$$               Dropout regularization is a technique wherina random half of the hidden neurons are ommited from the network for a single training iteration. The idea here is that “different networks can have different overfitting heuristics, and training them seperately can cause the averaging out of their errors.”           Artificially inflate the size of training data  In the case of MNIST, just rotate/blur the images by a small degree to get new training data!       Initializing the weights and biases   We have so far been initializing all weights and biases from a gaussian distribution of mean 0 and standard deviation 1. This isn’t optimal, as the standard deviation of $z=\\sum_i(w_ix_w)+b$ would be very large, proportional to the square of the umber of inputs the neuron has. This can cause the output of the sigmoid function to be nearly 0 or 1, causing stagnation as discussed earlier.   To solve this problem, we initialize $b$ as earlier but $w$ is initialized with mean 0 and standard deviation of $1/\\sqrt{n}$ where n is the number of inputs.       Universality of Neural Networks   This is a very important mathematical analysis (which I shall not write here for the sake of brevity) that neural networks (even simple ones with a single hidden layer) can compute any function with relative precision given enough time to train.   The approximation is better when there are more neurons used in the hidden layer. Also, we get an approximated continuous function as a result of estimating a discontinuous function by this method.  ","url": "http://localhost:4000/notes/dl/dl"
    },{
      "title": "Feedback",
      "excerpt":"I’d appreciate all the feedback given by you! The feedback can be about anything; such as the website’s style as an example. The form is linked here.  ","url": "http://localhost:4000/feedback/"
    },{
      "title": "Generative Adversial Networks",
      "excerpt":"     GAN stands for Generative Adversial Networks, and they belong to the set of Generative models. These models are called “generative” because of their ability to generate new data, such as a picture of a human face.   The mathematical analysis and intuition behind these networks is given below.       Inverse Transform Method and its implications   This method is used to simulate drawing values from a complicated probability distribution, using a uniform distribution. Let the CDF of the complicated distribution be F. Assume that it is invertible, and let its inverse be given by F⁻¹. Let a draw from the uniform distribution be u. It can quite easily be proven that F⁻¹(u) has the same distribution as F itself.   F⁻¹ is called as the Transform Function, as it is used to transform the uniform distribution to the target distribution.   This has a very important implication in the generation of new data. Suppose that there is a probability distribution for a vector of size n² (an image of size nxn) called the “Human Distribution” H, which tells how likely it is that the image represents a human face. We can now simply generate a random vector from uniform distribution, pass it through H⁻¹ to generate a human face!   There are very obvious problems here:     The “human distribution” is a very complex distribution over a very large space   It may or may not exist       Generative Models   In reality, we cannot explicitly say what H is. Therefore, a generative model aims at estimating the value of the transform function (H⁻¹ in this case). Training such a model has two ways, direct and adversial. Both these methods have been explained below.       Direct training (Generative Matching Networks)   We have one neural network which aims to estimate the transform function by comparing the acheived distribution to the “true” distribution. However, we do not know the true distribution (otherwise we wouldn’t need to do all this!) We thus take samples of human faces from available data and generate human faces via the neural network. Then, the Maximum Mean Discrepency between the two estimated distributions is taken as the error for back-propogation.   The neural net would strive to reduce MMD, meaning that it would learn the transform function upon training for sufficient time. This way of training for generation is used in Generative Matching Networks.       Adversial training (GANs)   We have two neural networks here, the generator and the discriminator. The generator has the same role as the previous model, wherein it tries to estimate the transform function by generating images of a human face. The discriminator is handed images from both generator and the already available data. It is tasked with classifying the images into two groups, the ones from the generator and the ones from the true data set. The generator is hence tasked with fooling the discriminator to the best possible extent.   Both the networks are trained simultaneously, and it can be clearly seen that they both are competing with each other. The discriminator tries to increase the error between the generated and the true data set whereas the generator tries to decrease this value at the same time. This competition is why the architecture is referred to as an “Adversial” network. Both the nets improve in what they’re trying to acheive by competing against each other.  ","url": "http://localhost:4000/notes/dl/gan"
    },{
      "title": "Gryffin: Bayesian Optimization of Categorical Variables",
      "excerpt":"        We formulate a chemical reaction as an optimization algorithm. The parameters associated can be either continuous or categorical. There exist methods such as PHOENICS for the optimization of continuous variables, but its pretty tough for applying the same for categorical variables.   This is where GRYFFIN steps in, it tries to extrapolate the methods used for continuous variables to categorical variables.   Thus, to understand GRYFFIN we would have to first understand PHOENICS better.   PHOENICS - Continuous Variable Optimization   Let the true function be represented by $f$, and our estimate of the function (surrogate function) so far be $\\hat{f}$. A three layered bayesian neural network is used for this purpose.   This is done by sampling values, and the acquisition function $\\alpha (z)$ tells us which value to sample at.   This is an iterative procedure, the following steps are done:     $\\alpha (z)$ proposes the optimal value at which to sample, $x’$   Sample at $x’$, and rebuild $\\hat{f}$ using the bayesian network   We stop after a set number of iterations are done, or when convergence is reached     Extending to Categorical   For example, let there be three types of Ligands, A B and C. We can represent each of these ligands as a one-hot encoded vector, such as $(1,0,0)$ for Ligand A.   We now assume the following:   The Ligands A B and C are sufficiently independant and descriptive enough of the “Ligand Space”. {. :notice–success}   We then try to represent the rest of the ligands as a vector. That is, another ligand D might be represented by the vector $(0.2, 0.3, 0.7)$, using the Gumbel-SoftMax Distribution.      This is called as Soft One Hot Encoding    This method is not fool-proof though, it is plausible that our original assumption is not valid. That is, if A and B are similar in nature, there is a redundancy in the Ligand Space!   The issue of similarity is addressed by introducing a descriptor space.   Descriptor Space   Incorporate domain knowledge to measure similarity! That is, we can plot the ligands on a 2d plot where the axes correspond to the HOMO-LUMO densities, and use the “distance” of a ligand D from each of these ligands to get the soft-one hot vector.   We would like the set of descriptors to have:      High correlation with the objective function   Number of descriptors should be small   Low pair-wise correlation amongst each other   It is quite difficult to manually select a large number of descriptors which folow the above pair of rules.   What the paper did was just send the descriptor vector into a neural network with a single hidden layer and softsign activation to get a descriptor output vector, of smaller size.   ","url": "http://localhost:4000/notes/surp/gryff"
    },{
      "title": "Hello.",
      "excerpt":"                                                                                       Feel free to go through the content on my website through the tabs located at the top. You can also search for anything specific.                                                                 ","url": "http://localhost:4000/"
    },{
      "title": "Artificial Intelligence and Machine Learning",
      "excerpt":"      The notes of CS337 have been divided lecture-wise, and they can be found below; or accessed via the sidebar.      Lecture1   Lecture2   Lecture3   Lecture4   Lecture5   Lecture6  ","url": "http://localhost:4000/notes/cs337"
    },{
      "title": "Introduction to Philosophy",
      "excerpt":"  The notes of HS301 have been divided topic-wise, and they can be found below; or accessed via the sidebar.      Branches of Philosophy   Classical Greek Philosophy   Socratic Period            Aristotle               Thank you Sudhansh for pointing out typos!  ","url": "http://localhost:4000/notes/hs301/"
    },{
      "title": "CS218",
      "excerpt":"The notes shall discuss the various categories of algorithms, and different ways of coming up with them. They have been segregated based on the idea discussed.  - [Divide and Conquer]({{ site.baseurl }}/pdfs/CS218_DAQ.pdf) - [Dynamic Programming]({{ site.baseurl }}/pdfs/CS218_DP.pdf) - [Flow Networks]({{ site.baseurl }}/pdfs/CS218_FN.pdf) - [NP-Hardness]({{ site.baseurl }}/pdfs/CS218_NP.pdf)","url": "http://localhost:4000/notes/cs218/"
    },{
      "title": "CS224",
      "excerpt":"I'll be uploading the notes here, segregated by the layers in the 5 Layer model of Networking. - [Data Link Layer - Wifi and L2 switching]({{ site.baseurl }}/pdfs/CS224_DLL.pdf) - [Networking Layer]({{ site.baseurl }}/pdfs/CS224_NL.pdf)","url": "http://localhost:4000/notes/cs224/"
    },{
      "title": "HS101",
      "excerpt":"The notes for this course have been linked below.  [Macroeconomics](/notes/hs101/mac/)  ### Microeconomics - [GDP](/notes/hs101/mic1) - [Cost of Living](/notes/hs101/mic2) - [Production and Growth](/notes/hs101/mic3) - [Finances](/notes/hs101/mic4) - [Unemployment](/notes/hs101/mic5) - [Monetary System](/notes/hs101/mic6) - [Inflation](/notes/hs101/mic7) - [Aggregate Demand and Supply](/notes/hs101/mic8) - [Fiscal Policies on Aggregate Demand](/notes/hs101/mic9)","url": "http://localhost:4000/notes/hs101/"
    },{
      "title": "PH107",
      "excerpt":"I'll be adding the material I've made for PH107 in here. Do let me know if there's any errors via the feedback form.  ### Reading Material  - [A short take on the significance of Quantum Tunneling in Quantum Computing]({{ site.baseurl }}/pdfs/PH107_TunnelingEx.pdf)  ### Tutorial Solutions  - [Solutions for Tut9]({{ site.baseurl }}/pdfs/PH107_Tut9.pdf) - [Solutions for Tut10]({{ site.baseurl }}/pdfs/PH107_Tut10.pdf)  - [Mini Quiz Solutions]({{ site.baseurl }}/pdfs/Ph107_Mini.pdf)","url": "http://localhost:4000/notes/ph107/"
    },{
      "title": "SoS-2020",
      "excerpt":"The notes for Data Structures and Algorithms that I've written during Summer of Science, 2020. I've mainly reffered to the courses offered by _Coursera_, and some tutorials by _TutorialsPoint_.  - [Link to the repo](https://github.com/AkashCherukuri/Data-Structures-and-Algorithms) - [Link to the pdf]({{ site.baseurl }}/pdfs/SoS_Report.pdf)","url": "http://localhost:4000/notes/sos2020/"
    },{
      "title": "Notes for Chemical Catalysis",
      "excerpt":"# Table of Contents 1. [Random Forest Algorithm](#random-forest-algorithm) \t- [Working](#working) \t- [Hyperparameters](#important-hyperparameters) 2. [SMILES](#smiles-notation)  -1. [References](#references)  ***  # Random Forest Algorithm This is a very versatile algorithm, as it can act as both a classifier and an regressor. It is used in various fields, such as the stock market and banking for starters.  ### Working  This algorithm consists of building multiple decision trees to act as an ensemble. Each of the decision trees have access to a random set of features. Let the number of trees in the ensemble be `N`. Each individual tree produces a result when data is provided for classification, and the majority is given as the result of the entire classifier.  This is considered to be efficient as it is very likely that one tree might have trouble with a certain type of data, but the majority of the trees can classify the data just fine. However, for this to happen, it is necessary that all trees have next to no correlation with each other.  This is acheived in the following ways:  - **Bagging:**  \tLet one of the data set during training be \\[1,2,3,4\\]. This data is modified a little randomly so that all trees in the classifier get different variations of the same data. For example, one tree might get \\[1,1,2,4\\] and another can get \\[1,2,2,3\\] and so on.  - **Random Features:**  \tDiscussed already, each tree has access to a random subset of features. This further decreases the chances of two trees having similarities.  ### Important Hyperparameters  1. `n_estimators`: Number of trees in the ensemble. Large values imply better learning but slower training and classification times. 2. `max_features`: Maximum number of features that a node can consider before splitting. Large features would improve each individual tree but correlation is increased as well. 3. `max_depth`: Maximum depth of a tree in the ensemble. Large values can cause overfitting. 4. `min_samples_split`: The minimum samples that must be present in a node before split. 5. `min_samples_leaf`: The minimum samples that a leaf can have.    ***  # SMILES Notation  SMILES stands for **S**implified **M**olecular **I**nput **L**ine **E**ntry **S**ystem. This is somewhat akin to the IUPAC nomenclature, but is designed to be compact and use ASCII characters. Five basic syntax rules are to be followed, and they have been listed below.  1. **Atom and Bond Nomenclature**  \tAtoms are represented using their atomic symbols, and hydrogen is usually exempted in the string. That is, `C` refers to methane and `CC` refers to Ethane.  \tCapital letters denote normal atoms and small letters denote aromatic atoms. That is, `CCCCCC` is Cyclo-Hexane, wheras `cccccc` is Benzene. For atoms with multi character atomic symbols, it is usually better to represent them in \\[\\]. For example, Scandium is represented as `[Sc]` and not `Sc` as the latter refers to Sulphur being attached to an aromatic carbon.  \tThe symbols used for bonds are given below. Single bonds are usually not represented manually.  \t| Symbol |  Character  | \t|:------:|:-----------:| \t|   =    | Double Bond | \t|   #\t | Triple Bond | \t|   *    | Aromatic Bond | \t|   .    | Disconnected Structures |   2. Chains  \tAs explained earlier, hydrogens need not be written down for the structure to be generated. That is, `CC#C` refers to propyne. However, if hydrogens are represented anywhere in the string, it is assumed that ALL hydrogens have been mentioned explicitly. For example, `HC(H)=C(H)(H)` is ethene.  3. Branches  \tBranches in molecules are represented using parantheses. The bond by which the branch is attached to the \"main\" chain is given after the opening paranthesis. For example, `CC(=C)C` and `CC(C)=C` both represent 2-Methyl Prop-1-ene.  4. Rings  \tRings are represented using SMILES by marking the \"start\" and \"end\" carbons of a ring using a number. That is, Cyclohexane is represented as `C1CCCCC1`, and Benzene is `c1ccccc1`. If the start and end of a ring are connected via a double or triple bond, it is mentioned before the number of the \"start\" atom. That is, `C=1CCCCC1` is Cyclo-Hexene.  5. Charge on Atoms  \tThe charge on an atom is represented in braces as `+1` or `-1`. That is, Enolate ion of Prop-2-one is given by `CC(O{-1})=C`.  ***  # References  1. [Hyperparameter tuning for Random Forest algorithm](https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74) 2. [Understanding Random Forest Algorithm](https://towardsdatascience.com/understanding-random-forest-58381e0602d2)  3. [SMILES Tutorial by USEPA](https://archive.epa.gov/med/med_archive_03/web/html/smiles.html)","url": "http://localhost:4000/notes/chemcat/"
    },{
      "title": "Theory of Machine Learning",
      "excerpt":"These notes contain the theoretical aspect of Machine Learning; and have been made during the winter vacation of 2020. _Understanding Machine Learning: From Theory to Algorithms (c) 2014 by Shai Shalev-Shwartz and Shai Ben-David_ has been taken as the main reference while reading up on this topic.  The notes have been divided into two parts, the first pdf is handwritten and contains the nomenclature and some basic derivations. The second pdf is typed, and is where most of the time has been spent by me.  - [Link to first pdf]({{site.baseurl}}/pdfs/ML1.pdf) - [Link to second pdf]({{site.baseurl}}/pdfs/ML2.pdf)","url": "http://localhost:4000/notes/toml/"
    },{
      "title": "Notes",
      "excerpt":"The notes which I've written so far will be linked down below. These notes have been written by me for projects, as class notes, or out of pure interest in the topic. Some of the notes are handwritten, wheras others have been done in LaTeX.  I am aware that the notes might have many typos which might've been missed by me in proofreading; although I may or may not re-visit them again. If you find any typos or any errors in my notes, feel free to let me know [here](https://docs.google.com/forms/d/e/1FAIpQLSfg5K7Who3oHfU4l4fgulXwf8h9csXvU88QPf83HDsMjE65XA/viewform?usp=sf_link)!  ## Self-Notes  - [Deep Learning and Convolutional Networks](/notes/dl/intro) - [Project Notes for Chemical Catalysis using ML (Prof. Sunoj, Ongoing)](/notes/chemcat) - [Theory of Machine Learning](/notes/toml/) - [Data Structures and Algorithms written for SoS-2020](/notes/sos2020/)  ## Class-Notes  - [HS301 - Introduction to Philosophy](/notes/hs301) - [CS747 - Foundations of Intelligent and Learning Agents](/notes/cs747) - [CS337 - Artificial Intelligence and Machine Learning](/notes/cs337)  ---  - [PH566 - Advanced Simulation Methods in Physics]({{site.baseurl}}/pdfs/PH566.pdf) - [CS224 - Networks](/notes/cs224/) - [CS218 - Design and Analysis of Algorithms](/notes/cs218) - [HS101 - Humanities and Social Sciences](/notes/hs101)  --- - [CS251 - Notes for Python](/notes/cs251py/)  - [CS251 - Notes for Awk and Sed](/notes/cs251a_bash/) - [CS213 - Classnotes](/notes/cs213cn/) - [CS215 - Data Analysis and Interpretation]({{site.baseurl}}/pdfs/CS215.pdf)    The notes start from the Gaussian distribution. - [CS207 - Discrete Structures]({{site.baseurl}}/pdfs/CS207.pdf)    The notes start from the Chinese Remainder Theorem.  ## Teaching-Assistant-Notes  - [PH107 - Quantum Physics and its Applications](/notes/ph107/)","url": "http://localhost:4000/notes/"
    },{
      "title": "Resources Used",
      "excerpt":"---   - [Neural Networks and Deep Learning by Miachel Nielson](http://neuralnetworksanddeeplearning.com/index.html) - A Machine Learning course [here](https://www.coursera.org/learn/machine-learning)   - Notes on Machine Learning [here](http://cs229.stanford.edu/summer2019/cs229-notes1.pdf) - [Understanding Generative Adversarial Networks](https://towardsdatascience.com/understanding-generative-adversarial-networks-gans-cd6e4651a29)","url": "http://localhost:4000/notes/dl/resources"
    },{
      "title": "Katana Zero Review",
      "excerpt":"     - **Rating** - 4/5 - **Studio** - Askiisoft - **Publisher** - Devolver Digital - **Genre** - 2D Action-Platformer  (Spoiler-free)  &nbsp;   Katana Zero is a retro-style action-platformer developed by Askiisoft and published by Devolver Digital. The visuals and the soundtrack are reminiscent of Hotline Miami, with the core gameplay loop feeling more polished.   &nbsp;   ## Gameplay You play as a nameless samurai, working for a dystopian government as a hitman. The game follows a one-hit-kill rule, meaning that a single hit is enough to eliminate both you and your enemies. A well-timed attack can parry bullets, and you can slow down time to make parrying and dodging easier. During a conversation, you can choose dialogue with an additional option of interrupting the speaker, which might open up other dialogue paths.   The game is highly replayable and even has a customizable speedrun mode with associated medals, which adds incentive to get faster. New game plus also includes an insanely difficult Hard-mode, which will keep players engaged for quite a while after finishing the game. The Hard-mode adds in new enemies, with the number of regular enemies increased significantly.   &nbsp;       ## Presentation Katana Zero's presentation is a cut above the rest. The visuals combine a neo-noir aesthetic and pixelized graphics to yield scenes that, at times, look stunning. The visuals are complemented by a fantastic synthwave soundtrack composed by LudoWic and Bill Kiley.  Evident care has been put into the presentation, making the game feel the best it possibly can. The most notable effects are screen-shake and hit pause, which makes landing an attack highly satisfying. It also offers a few basic options for the player to tweak some of these effects as they wish.  Here's one of the more melancholic tracks from the game;       Tunç Çakır - Blue Room (feat. LudoWic)        &nbsp;   ## A candle that burns twice as bright... Katana Zero is a relatively short game. It would take roughly 4 hours to complete for the first time. My personal best for speedrunning the game is approximately 26 minutes. Including NG+, it would take around 8 hours to see everything that the game has to offer.  It makes up for this by being effortless to play. I prefer a four-hour game worth replaying many times to a padded-out game barely worth playing once, so this isn't a negative for me (but it might be one for you). The story is where the game falls short as a stand-alone title. It starts off very strong, but not much is resolved at the end. However, a sequel is in the making, which might answer the many questions posed. (Keyword - Might; which is why the rating is subject to change)  &nbsp;    ## In Conclusion Katana Zero is one of the most exciting Indie games that I have played to date. The gameplay is exhilarating, the campaign effortless, and the story intriguing. I highly recommend this game, and if you're not willing to buy this game for full-price, you should at the very least be on the lookout for an offer; because even if you believe this game is not worth your money, I can essentially guarantee it is worth your time.","url": "http://localhost:4000/reviews/KatanaZero"
    },{
      "title": "About the Reviews",
      "excerpt":"## Ranking System I will be rating games on  a scale of 0 through 5. Each number's meaning has been explained below. I plan to be reasonably thorough with these reviews.  - 0 -> Unplayable - 1 -> Playable, but would not reccommend - 2 -> Enjoyable, but with serious flaws - 3 -> Reccommended, but with minor flaws - 4 -> Highly Reccommended, respectable - 5 -> Masterpiece   ## Interests I tend to enjoy faster, reflex based games more than strategic, slow-paced ones. Do keep in this in mind if you do intend to read the review, as I tend to be sterner towards such genres. My rating is my opinion alone, and is (obviously) not absolute.   ## Why here? Putting one's ideas into words effectively is an important skill to possess, be it an Engineer or otherwise. Similarly, being able to look at game design and analyzing how effectively it acheives (or fails to acheive) its goals requires critical thinking, which I wish to train upon. Games are just a medium I enjoy, and thus, makes the process of me honing my skills enjoyable.  I am putting it up on my website for everyone to view, and provide constructive feedback on; as it would ultimately help me in my journey no matter how small.","url": "http://localhost:4000/reviews/about/"
    },{
      "title": "Video Game Reviews",
      "excerpt":"I plan on writing the reviews for a few games I enjoy here, as practice. I might divide them up based on their genre later, once there's too many. I will try to follow a similar fashion for all the reviews, to maintain uniformity. Clicking on the hyperlinks below will take you to the respective review.  The rating guide, my biases, and why I've decided to put reviews up here has been discussed [here](/reviews/about/).  - [Katana Zero](/reviews/KatanaZero)","url": "http://localhost:4000/reviews/"
    },]