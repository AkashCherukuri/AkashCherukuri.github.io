var store = [,{
      "title": "GDP",
      "excerpt":"Three factors are important to measure a nation’s progress.      Income (GDP)   How well the work-force is absorbed by the economy-(firms)   Inflation Rate   The Circular Flow   This is another way of modeling the Income of a nation, other than GDP. This is the Circular Flow diagram discussed earlier in the course. Here, Income = Expenditure ideally. Any surplus that firms obtain would be the profit, and be tallied up in their income.   Gross Domestic Product (GDP)   The total market value (monetary value) of the FINAL goods and services produced by the factors of production located within the nation’s borders within a given period of time. This is used to account for the nation income properly. Usually measured per year and per quarter year.   A good being “final” depends on the use of the particular good. Goods which are entirely used for the production of other goods are called Intermediary Goods. Second-hand goods’ sales are excluded, to avoid counting it twice. Household productions, legal and illegal underground transactions are also not considered. However, value of goods that are stored in the inventory is also added to the GDP, and subtracted when they leave the inventory.   Value Added - The monetary value of a firm’s sales minus the value of the intermediary goods used in the production. Note that the value added is not exactly equal to the profit, as costs of labor are not considered.   Quarterly GDP is presented after “Seasonal Adjustment” has been done. This is because policy makers want to look beyond the usual seasonal changes, such as an increment in the sales during Christmas.   There are three main ways of measuring GDP:           Expenditure Approach       Compute GDP by adding the monetary value of all final goods and services. The goods are divided into Durable consumer goods (life &gt; 3yrs), non-durable consumer goods (life &lt; 3yrs) and services (salon, maid, therapy).       The expenditure is divided into consumption, investment, government spending and net exports.       Consumption (C)       Includes household expenditure on goods and services, with the exception of purchasing new housing. “Goods” include both durable and non-durable goods. “Services” include intangible items such as haircuts, health-care and (arguably) education.       Investment (I)       Spending on factors of production, or on goods (capital goods) which would be used in the future to create more goods and services.              Business Capital - Equipment, buildings for factories or offices, Intellectual properties for machines .etc       Residential Capital - Building or buying a house (not renting)       Inventories - Goods bought by a person / firm but not sold yet           Investment is not the same as capital. Investment is used to buy capital goods to be used later. A stock is a quantity measured at a point in time. A flow is a quantity measured per unit of time. GDP is a flow measure, as it is done per year.       Government Spending (G)       Government spending on goods and services, such as payments for government jobs. Keep in mind that pensions are not counted in Government Spending because there is no exchange of goods and services taking place. Such transactions are called as Transfer Payments, and they are not included in the GDP calculations. Transfer payments can be thought of as “Negative Tax”.       Net Exports (NX)       NX = (Exports) - (Imports). As a consequence, a household buying a imported product would cause no increment in the GDP because the increase in Consumption is cancelled by the increase in Imports.       The value of total output (Y) is calculated as follows: \\(Y = C+I+G+NX\\)       Real Production (Base) means that the total value is computed with the prices during the base time period.            Income Approach       Add up all components of national income, including wages, interest, rest and profits. It can be clearly seen that $\\text{Y}=\\text{National Income}$.            Output Approach       Sum of value added in each sector       We should be getting the same GDP by all the three methods above. We choose the approach which is the easiest on a case-by-case basis.   Rules for Computing GDP      To compute the total value of different goods and services, the national income accounts use Market Prices.   Used goods and resold goods are NOT included in the calculation of GDP.   Final goods are considered in GDP, and not intermediary goods.   Sold goods’ values are included in GDP. Spoilt goods (perishable goods) doesn’t change the GDP.   Goods which are accounted for in inventories, are not accounted for again when they are sold out of the inventory.   Some goods are not sold in marketplaces, and thus, do not have market prices. For example, mango trees in the forests, small vegetable farm used by a household. We must use their Imputed Value. This value is indirectly added to the GDP, as the cost of a building would go up if it has fertile soil around it, allowing for a backyard garden.   Changes in Price   Nominal GDP - Measures output according to the current year’s prices   Real GDP - Measures output according to prices at a specific year in the past. This year is called as the base year   That is, we would like to compare goods by taking into account the change in market prices that might occur due to other external factors like inflation. The rate of inflation is usually over-estimated, and as a flip side, the rate of real economic growth is under-estimated. When we say “GDP”, we usually refer to “Real GDP” as it better represents the economy.   Keeping the base year price constant for a long period of time would cause the Real GDP to be vastly different compared to the Nominal GDP, which is not desirable. We thus, shift the Base Year prices.   GDP Deflator / Implicit Price Deflator is the ratio between the Nominal GDP and the Real GDP times 100. That is, we measure the price of the output relative to its price in the base year.   GDP Deflator can also be used to calculate the Inflation Rate between two years, year1 and year2 as follows:   \\[\\text{Inflation Rate % } = \\frac{\\text{(GDP Deflator in Year2)}-\\text{(GDP Deflator in Year1)}}{\\text{(GDP Deflator in Year1)}}\\times 100\\]  Problems with GDP   GDP by itself is not a good measure as the size of a country has a very large impact on it. So, we usually measure GDP per Capita, which is the GDP per person; to ensure that the scale of comparison is similar. However, it is possible for large disparities in the wealth to be hidden by GDP, as it measures an average.   India’s GDP is under-estimated due to a large informal sector. Volunteering, preparing food for dinner, and caring for one’s children doesn’t come under GDP; but restaurants and Foster care do. Also, exchange rates used to convert GDP in local currency into US Dollars would depend on the relative prices of internationally traded goods. Developing countries tend to have unstandardized goods, which has little impact on the rate, causing a bias on the GDP.   Comparison resistant services exist, such as health-care, education, administration etc. The price of foreign exchange is also much lower than the free market price.   GNP/GNI   Similar to GDP, we define GNP which stands for Gross National Product. This includes the production by all the people of a particular nation, whose money must be traced back to the country. That is, a migrant worker sending his income back to his home would be counted under GNP. \\(\\text{Net National Product (NNP)} = \\text{GNP} - \\text{Depreciation}\\)   \\[\\text{Net Domestic Product (NDP)} = \\text{GDP} - \\text{Depreciation}\\]  Purchasing Power Parity (PPP)   The amount of money needed to buy a standardized good in different countries. A scale called the “Big Mac Index” exists, where the good is a big mac. Some other examples of standardized goods are a starbucks coffee (ew), a men’s haircut, for starters.   ","url": "http://localhost:4000/notes/hs101/mic1"
    },{
      "title": "Recovery Systems",
      "excerpt":"        Failures are classified into three types:      Transaction Failures - Can be due to erroneous logic in queries or system errors causing deadlocks   System Crashes   Disk Failures   There are two main kinds of blocks:      Physical Blocks are the blocks residing on the disk   Buffer Blocks are temporary blocks in main memory   input(B) transfers a physical block to the main memory, whereas output(B) transfers a block from the buffer to the disk. We assume each data item to fit in a single physical block.   Each transaction $T_i$ has its own copy of a data item $X$ called $X_i$. read(X) by the transaction assigns $X$ to $X_i$ and write(X) by the transaction would send $X_i$ to the buffer. The system can call output(B) to transfer this buffer block to main memory when it deems fit.   Log Based Recovery   Log is a sequence of log records which store information about the activities made by a transaction in a database. Consider the transaction $T_i$.                  Activity       Log                       Transaction start       $&lt;T_i, \\text{start}&gt;$                 write(X) executed       $&lt;T_i, X,V_{before}, V_{after}&gt;$                 Last statement of $T_i$ done       $&lt;T_i, \\text{commit}&gt;$           Immediate Modification scheme lets the updates of a transaction to be sent to the buffer / disk before the transaction commits. Note that log record must be written before execution of query! On the other hand, Deferred-modification scheme performs the updates to disk only at the time of commit.      If $T_i$ modifies an item $X$, no other transaction is allowed to modify $X$ until $T_i$ either commits or aborts    undo(T_i) restores original values of data items that were updated by $T_i$. For every write $&lt;T_i, X, V_1, V_2&gt;$ we add a special log record $&lt;T_i, X, V_1&gt;$. $&lt;T_i, \\text{abort}&gt;$ is written out once the abort it completed.   redo(T_i) on the other hand simply executes all the records sequentially.   Checkpoints   Periodically save the state of the database to streamline recovery procedure.      Output all log records currently in memory to stable memory   Output all modified buffer blocks to disk   Write a record $&lt;\\text{checkpoint}, L&gt;$ onto the stable memory where $L$ is a list of all active transactions at that time   All updates are stopped while checkpointing! (Fuzzy checkpoints try to fix this)   Recovery Algorithm   There are two main phases to the recovery algorithm; redo and undo phases.   Redo Phase      Find the last checkpoint $&lt;\\text{checkpoint},L&gt;$ and set undo_list to be $L$   Scan forward and redo all write operations. Add $T_i$ to undo_list when $&lt;T_i, \\text{start}&gt;$ is found and remove $T_i$ from undo_list when either $&lt;T_i, \\text{commit}&gt;$ or $&lt;T_i, \\text{abort}&gt;$ are found.   Undo Phase      Scan the log backwards from the end   Undo write operations $&lt;T_i,X,V_1,V_2&gt;$ by adding $&lt;T_i, X, V_1&gt;$ where $T_i$ belongs to undo_list   Remove $T_i$ from undo_list and add log line $&lt;T_i, \\text{abort}&gt;$ upon encountering $&lt;T_i, \\text{start}&gt;$   Stop when undo_list is empty   Buffering   Log records are generally buffered by storing them in memory instead of sending them to stable memory to improve I/O. They are flushed when a block is either full or log_force is called. log_force is usually called when a transaction commits.   Write-Ahead-Logging (WAL): Before a block of data in main memory is output to database, the corresponding log block must be flushed to the stable memory   Database also has a buffer, which has three behaviors based on how the blocks are pushed to database. No-force policy is when the updated blocks need not be pushed when a transaction commits, force policy is when the blocks are pushed when a transaction commits (not before) and steal policy is when blocks can be pushed even before a transaction commits.      Acquire a latch on the block   Flush logs   Output block to disk   Release latch on the block   Fuzzy Checkpoints   All updates have to be stopped during checkpointing, which is inefficient. Fuzzy checkpointing aims to solve this issue.      Temporarily stop updates   Write $&lt;\\text{checkpoint},L&gt;$ and force logs to stable storage   Note the list $M$ of modified blocks   Allow transactions’ actions to resume   Output all blocks in $M$ to disk following WAL   Store a pointer to checkpoint in a fixed memory on disk called last_checkpoint   Remote Backup Systems   “Heartbeat” messages are sent between the primary and the backup sites so they know the other’s status. To transfer control to the backup, use its copy of the database and the logs generated by the primary to recover data. The transactions that were incomplete are rolled back.   To reduce recovery time, the backup periodically processes redo logs so its database is kept up-to-date with the primary site. Hot-spare configuration lets the backup continually process log files as they come in.   Durability is of three levels;      One-safe: Commit as soon has transaction’s commit log is written in primary   Two-very-safe: Commit when transaction’s log is written in both primary and backup   Two-safe: Proceed as two-very-safe when both are up, and as one-safe if only primary is active  ","url": "http://localhost:4000/notes/cs317/rec_sys"
    },{
      "title": "Introduction to Philosophy",
      "excerpt":"Broadly, Philosophy can be defined as the “Log of Wisdom”. Also called as Darshana-Sastra in Indian Philosophy. It is a discipline that investigates the nature of values, and relationships integrating humanity with world and reality as a whole.   Philosophical thinking is conceptual in nature. This is necessary to make sense out of the subject matter.      What is number?       What is Justice?    Philosopher is a person who critiques existing philosophical ideas with proof (otherwise its just an opinion). They also try to “prove” whatever is acceptable.   Philosophical Thinking   Philosophical thinking is broadly classified into 5 different categories;           Speculative       Philosophy began as speculation, and is the core of Metaphysics. Highlights the importance of imagining something, and is used to draw intuitive insights beyond observable facts.              “Imagination is more important than knowledge” - Albert Einstein                 Imaginative / Creative       Utilization of poems, and being able to draw artistic interpretation of truth. FOr example, Rabindranath Tagore was an astounding poet and a philosopher.            Reflective / Critical       Very fundamental to philosophy; being able to evaluate the accuracy and relevance of ideas is of utmost importance for the betterment of humanity.            Argumentative       Being critical requires thinking to be argumentative. Arguments are logical, and are regulated by logic.            Liberal / Inclusive       Need to be open to other ideas, to be able to eliminate prejudices. Open-mindedness is very important for the betterment of philosophy.       Philosophy is  broken down into 4 major branches, given below. We will look at each of these branches at a surface level, to know what the reaches of the topic are.      Metaphysics   Epistemology   Logic   Ethics / Aesthetics       Metaphysics   Philosophy began with metaphysics, and was closely intertwined with epistemology. It studies the causes of everything that exists or that may exist.   Metaphysics is considered as the First Philosophy by Aristotle. He classified metaphysics in the following order;      Ontology - The science of being Qua Being - The science of existence   Theology - Highest kind of being (Divinity)   Universal Science - First Principle - Truth of every existing thing, basis of proof or demonstration      A thing is a thing by virtue of something that it is    Although prominent in the pre-socratic era, Metaphysics has been affected in the renaissance era due to enlightenment and groups such as Logical Positivists, and has been claimed as “philosophical nonsense”. Today, it plays the role of disciplinary integration for the better understanding of the connections between different streams.   An example of an ontological question is:      “Who am I?”        Epistemology   Epistemology is a branch of philosophy, and is concerned with the theory of knowledge. Justification and debates are used to “prove” that an idea is “true”. A justified belief is knowledge. A belief by itself is just an opinion or doxa.      “Justified True Belief is Knowledge” - Plato    A belief is proven by;           Explanation: Should be a causual explanation (ie, what the cause of a thing is)            Justification: Argue why the causual explanation is true            Understanding: That is, the conveyed idea must be understandable by the listener       Back in the day, most knowledge consisted of passed down information from generation to generation. For progressing knowledge, it is important to doubt the validity of existing knowledge. This practice of doubting is called as Skepticism or Pyrrhonism.   In general, Skepticism suggests that knowledge is impossible.   In Indian Philosophy, knowledge is given by four sources:      Pratyaksa or by perception   Anumana or by inference   Upamana or by comparision   Sabdda or by testimonies from trusted people   There are two important schools of thought regarding the primary source knowledge:      Rationalism: Reason is primary source of knowledge   Empiricism: Experience is the primary source of knowledge   Do note that Rationalism doesn’t say that reason is the only source, rather it says that its the main source. Similarly for empiricism.       Logic and Arguments   In indian philo, Logic is known as Tarka Shastra. Logic is the science that evaluates arguments regarding a particular observation/reasoning.   That is, Logic is concerned with:      Proper construction of arguments (Tarkas)   Evaluating the validity of the argument   An argument is defined as a logical sentence that shows how a premise leads to the truth value of the proposition.      An argument is valid/invalid if it follows certain laws   Arguments can be made from true premises or false premises as well   For example, Rene Descartes was following Methodological Skepticism which said that true knowledge was attainable (opposite to what skepticism said). He would have to justify his thinking, and its validity would be judged by logic.   There are two different types of arguments;           Deductive Arguments       Introduced by Aristotle, this uses a universal premise to test the truth value/validity of another proposition/argument.       That is, if premises are true; then true conclusion necessarily follows from the premises.            Inductive Arguments       Introduced by Francis Bacon, the premises are based on previous experience and intuition. The concept of generalization is introduced here, and these arguments have methods to be followed when being applied.           Ethics and Aesthetics   Ethics is conerned with morality, values and the theory of morals. An action being “good” or “right” depends on certain principles, and requires rigorous justification as to the same. This branch is important because “good” need not necessarily imply that the action is “just”.   Similarly, Aesthetics are concerned with the following question      What makes something look good?    Although not as important as other branches of philosophy, much of human brilliance can be seen here; as without an aesthetic appeal life would be dull.  ","url": "http://localhost:4000/notes/hs301/intro-to-phil"
    },{
      "title": "Cost of Living",
      "excerpt":"Consumer Price Index (CPI)   This is a statistic which measures the overall cost of all goods and services bought by a typical consumer. CPI can be used to compute inflation and compare the standards of living at two different points of time. The inflation rate calculated by CPI is more representative of the economy than the rate calculated by GDP Deflator. Thus, when we say “Inflation”, we usually refer to “Inflation computed using CPI”.   Calculating CPI           Fix Basket of a Typical Consumer       The “basket” refers to the goods and services that a typical consumer purchases. This basket is assumed to be fixed across the times of computation, and is obtained by surveying people.            Find Prices       Find the prices of all the items in the basket at different years.            Compute Basket price       Use the prices obtained to compute the cost of the basket. Again, we assume the basket to be fixed across the computation time.            Choose a Base Year and compute CPI       Set an year as the base year for comparisons, and calculate the CPI for the remaining years by taking the ratio of the basket prices. \\(\\text{CPI in Year X} = \\frac{\\text{Basket Price in Year X}}{\\text{Basket Price in Base Year}}\\times 100\\)            Compute Inflation Rate       Similar to GDP Deflator, the Inflation rate between two years is calculated as the percentage change in the CPI between those two years. \\(\\text{Inflation Rate in Year2} = \\frac{\\text{CPI in Year2} - \\text{CPI in Year1}}{\\text{CPI in Year1}}\\times 100\\)       Core CPI - A measure of CPI excluding food and energy. This is because food and energy tend to be very variable, and core CPI better reflects inflation trends.   PPI - Stands for Producer Price Index. This computes the change in prices of the producers. Because an increase in PPI would ultimately lead to higher prices, this statistic can be used to predict changes in CPI.   PMI - Stands for Purchasing Manager Index. This computes the change in production of a factory/company on a monthly basis. &gt;50 implies expansion and &lt;-50 implies contraction.   WPI - Wholesale Price Index. CPI &gt;= WPI as not all goods are considered on WPI.   Problems with CPI   CPI assumes that the basket of consumers remains the same across years, which need not be the case practically. Some of the problems caused due to this are discussed below.      Substitution Bias - Increment in the price of a good in the basket would cause consumers to buy less of that good, and more of a substitute. This change is not reflected in the calculations.   Introduction of New goods - Developments in technology can cause the introduction of new goods which might bring the CPI down. However, this would not happen unless the basket of the consumers is changed (which is a problem).   Change in Quality of Goods - An already present good might be improved by the producer, which would have to decrease the CPI. This is not fully represented by the change in price, as there is an increase in the quality as well. We can try to decrease the price accordingly by measuring the value added, but measuring “value” is vague and difficult.   GDP Deflator and CPI comparison   The inflation rates calculated by both these terms are obviously not the same. The key difference is that the GDP deflator takes into account all the domestic-produced goods whereas the CPI takes what the consumers purchase into account. Therefore, if there is an increase in the price of imported goods, the GDP deflator is unchanged but the CPI increases.   Another subtle difference is that the basket is fixed for CPI, but the goods produced by producers in a country can be variable. Thus, the GDP deflator is more flexible to changes in goods over time.   Correcting Economic Terms for effects of Inflation      Comparing money from different times   \\[\\text{Amount Today} = \\text{Amount in Year T} \\times \\frac{\\text{Price Index Today}}{\\text{Price Index at Year T}}\\]  The CPI is usually used as the price index.      Regional Price Parities   The difference in the cost of living between different areas is represented by Regional Price Parity. The difference in prices is due to services, because relocating services is difficult. Housing services are an exceptionally good example.      Indexation   Contracts which are legally bound for large periods of time where are usually indexed. That is, the monetary exchange is corrected automatically based upon a statistic such as CPI.      Nominal and Real interest rates   Nominal interest rate is what the bank offers, without accounting for inflation. Taking inflation into account, the Real interest rate is calculated as follows:   \\[\\text{Real Interest Rate} \\approx \\text{Nominal Interest Rate} - \\text{Inflation Rate}\\] ","url": "http://localhost:4000/notes/hs101/mic2"
    },{
      "title": "Ancient Philosophers",
      "excerpt":"Three influentual ancient philosophers’ ideas have been discussed in brief below. The three people discussed are:      Thales   Pythagoras   Heraclitus       Thales of Miletus   Called as the First Philosopher, and the founder of the Ionian School of Science. We shall see what led to Thales getting this title.   Back in the day, Philosophy was closely interlinked with Mysticism and religion; wherein the priests had supremacy over knowledge.   Thales pursued Principle of movement fundamental to the order.   That is, Thales was the first to try and reason out the causees for observations in nature, not related with religion. This encouraged people to think in a similar manner, and to not be deceived by appearance alone.   In particular, he was interested in the material causes behind floods and earthquakes.   Water as a first principle   Thales is famous for stating that “Everything is Water”. He beleived that everything to be made of one single substance.   It is debatable whether Thales believed this literally, or only metaphorically to be understandable to the common people. This is akin to the Pancha-maha-bhoota idea which was floating around in Indian Philosophy.   He was the first to lead that a monopoly on truth by a religion or a monarch is impossible; and it can only be arrived at by understanding nature.   Thales was the first philosopher as he focused not only on the insights of nature, he also displayed interests in geometrical demonstrations and astronomical insights.       Pythagoras      “Intellectually the most important man that ever lived” - Bertrand Russel    Just like how Thales used water to describe nature, Pythagoras utilized numbers and mathematics. He was interested in seemingly opposite ideologies, Mysticism and Mathematics. His beliefs were influenced by Indian Philosophy, during the Conquests of Alexander.   For example, Pythogoras tried to use numbers to understand music and symphonies. His mysticism was influenced by eastern philosophy, and he believed in music as the expression of the divine.   Pythogoras was the first to:           try and explain nature using an intangible entity            utlize demonstrative deductive arguments, as such is the nature of mathematics. Although Aristotle was the one to popularize it, Pythagoras was the first to utilize them       For example, the pythagoras’ theorem can be viewed as a link between arithmetic and geometry. That is, numbers can be used to explain geometry, which is why numbers for pythagoreans is the principle of things.   Numbers for pythagoreans are the principle of things, not in a physical sense, but as a formal structure. Therefore, we can see that his ideas had a metaphysical side as well.   Contributions:           Deciphering inner workings and the purification of the mind, likely influenced by the Nikruti/Prakruti ideas in Indian Philosophy            Tried to show what cannot be seen directly using mathematics            Introduced the concept of self-evidence/Axioms; that is, you don’t need proof to say that a single line can pass through two points            Deductive Mathematical Argument, explained below       Pythagoras applied self-evidence to perspectives of moral claims. He also believed in the Law of Karma from Indian Philosophy. His moral philosophy included stuff like “Don’t steal from others”.   Axioms are self-evident/fundamental propositions.   Axiomatic Method:     Step-wise deduction through application of rules   Use follow-up steps to derive new propositions   This is Deductive Mathematical Argument; the conclusion follows intuitively.       Heraclitus      Heraclitus believed the world is in accordance with Logos and is ultimately made of fire. He also believed in a unity of opposites and harmony in the world.    Thales used a tangible substance to explain nature. Pythagoras used an intangible number to quantitatively explain nature. Heraclitus was concerned with change or becoming.   That is, for example, “If x is changing to y; can y exist without x?”   Heraclitus used Fire as the Universal Flux, the only agent of change.      Fire as the first principle   Fire is ever-living   Fire as a substance is mobile   He has also said that “All things are exchange of Fire, and Fire for all things, even as wares for gold and gold for wares”.   Change is original, and nothing can be lost or originated. That is, everything has originated from Fire either directly or indirectly and it is possible for them to turn back to fire again.   Ethically, his ideas can be summed up with the statement “Good and Ill are one”, similar to the idea of Yin and Yang.   He has also introduced the notion of LOGOS - The principle of divine reason / cosmic order. That is, he believed that every change followed a universal divine reasoning. This was believed by Pythagoras as well.   He also believed in a cosmic fire-soul, from where he believed every thought to originate.  ","url": "http://localhost:4000/notes/hs301/anc-phil"
    },{
      "title": "Production and Growth",
      "excerpt":"Productivity: The amount of goods produced per labor input. A higher productivity corresponds to a higher standard of living. One of the factors of production is capital. Capital is a produced factor of production, meaning it was an output of a production previously. There are four Determinants of output:      Physical Capital: This is all the tech and machinery needed for more efficient production.   Human Capital: Skilled human capital are essential for high productivity.   Natural Resources: Having more natural resources per worker would cause an increment in productivity. Are of two kinds, renewable and non renewable. Important, but not necessary.   Technological Knowledge: Proprietary knowledge is knowledge under secret (coke), common sense is available to everyone (Ford manufacturing) and temporary proprietary knowledge is hidden for a short time (pharma). Technological knowledge is society’s understanding, whereas human capital is a worker’s quality.   Productivity Policies   Capital suffers from diminishing returns. That is, increment in production by every additional unit of capital produced decreases as capital produced increases. ($\\log$ graph) This is called as Diminishing marginal product of capital. Therefore, increasing savings increases production for a while only. Similarly, the countries which start off poor increase productivity rapidly, and this is called as the catch-up effect.   A direct investment from abroad is called as foreign direct investment, and indirect investment (by buying stocks and the such) is called as foreign portfolio investment. This causes both GNP and GDP to rise, although GDP rise is larger than GNP rise. Less developed countries do this to increase productivity, income, and to learn new technologies. Investment is encouraged by removing restrictions.   Policies which restrict trading are called inwards-oriented policies. “I am in infancy of production, how will I survive on opening myself to the competition?” However, outwards-oriented policies usually tend to dramatically increase productivity of a country.   Human Capital   Education is investment in human capital. The opportunity cost here is the wages that could’ve been earnt had the person been working. Has a positive externalities on the society. A major problem is brain drain, where the highly educated move to the developed country, decreasing the capital of the original country further.   Healthcare is also an example of human capital. Well nourished workers are more productive, and this is an important case in developing countries.   Research is also an important factor in standard of living. Once an idea comes up, it usually becomes public property. Governments can invest in research and incentivize it by awarding patents.   Political Stability   Property rights are very important as no one would work if they expected the results of their labor to be stolen. Political instability decreases confidence that property rights would be upheld, and this decreases productivity.   Population   Population stretches the present natural resources, causing poverty. However, human ingenuity has out-weighed this (there is less poverty although population has increased 6-fold)   Population rise thins out present capital, meaning each person will have less capital to work with which decreases productivity. It also is difficult to train human capital when there’s too many.   Population rise increases the number of potential scientists and engineers, meaning a country with large population is likely to do better in terms of research.  ","url": "http://localhost:4000/notes/hs101/mic3"
    },{
      "title": "Socratic Period",
      "excerpt":"The nature of philosophy in the pre-Socratic period was highly nature-centric. In the socratic period however, philosophy was much more society-centered.   That is, it dabbled with ethics, the importance of, and the directions needed for living a pious and good life. The socratic period was marked by:      Beginning of Greek Enlightenment   Critical Thinking Encouragement - Departure from speculative thought   Individualism - Freedom emphasized   This period was the rise of Skepticism and Subjectivism.   Skepticism has been defined earlier, followers believe that “true knowledge” is impossible to attain. Subjectivism says that different people can have different beliefs. Fundamentalism is a form of extreme subjectivism, where people assert that ONLY their beliefs are true.   Fundamentalism disrupts the harmony which is needed to progress knowledge.   Although naive theories full of gods and occult powers existed, a mechanical theory of atomists was around as well during this period.   This period is the beginning of Greek enlightenment, where freedom and individualism was encouraged and they displayed a critical attitude towards life.   First half of the period was full of natural philosophy of cosmos. Man’s place was determined using Metaphysical conclusions. The second half was marked with a change in the political, economic and intellectual atmosphere. This is after the Great Persian war. That is, the nature of thinking shifted from speculative to critical.   This new age was majorly due to the establishment of a democratic institution. This ushered in many great people such as Socrates and Hippocrates.   Sophists   They were the representation of a new movement in Greece. These were wise and skillful people who worked as professional teachers and travellers. As a livelihood, they used to train people in:      Dialects - Argumentative reasoning (Lawyers, for example) and Aporia (trying to lead the other person into a contradiction)   Grammar   Rhetoric - Flow of speech   Oratory   They were relevant because it was beneficial to be a good orator in democracy, as it opens up opportunities for one to better their life and influence others as well.   However, Socrates felt that they did not use their intelligence properly, and that their logical arguments were lacking. They were also highly skeptical in nature, as that was the prevailing school of thought at the time.       Socrates      “Virtue is Knowledge - An unexamined life is not worth living” (paraphrased)    Socrates urged for clear and rational thinking, and emphasized that knowledge and virtue are necessary for society. That is, he gave importance to ethics.   As mentioned earlier, philosophy before Socrates was highly nature-centric but Socrates focused more on the betterment of society. He caused a shift in philosophy to be more society-centric in nature.   At the time, most of the Sophists lacked objectivity and placed emphasis on subjective opinions. They followed Protagoras, “Man is the measure of all things.” Ethical was divorced from epistemological, which caused issues in society.   Socrates was a moderate thinker, and was infatuated with finding the true meaning of Knowledge. To better understand it, he tried to establish why knowledge is important.   He emphasized that every person is supposed to examine and reflect upon themselves to improve. This is further more important for leaders, and he expressed moral and physical courage in politics and war.   Knowledge   Socrates believed knowledge to be the highest good. He emphasized the importance of knowledge for the improvement of one’s soul.           Knowledge is Morality       Understanding the meaning of Morality, as it is important for the proper functioning of state and society. In short, Moral Knowledge is important.            Knowledge is Virtue       He says that Virtue is good in itself, just like a happy man usually is temperate, brave, wise and just. He believed knowledge to be both the necessary and sufficient condition for virtue.       A teacher can transmit knowledge, values and rules. But Socrates believed that Virtue cannot be taught. That is, virtues are to be practiced and values are to be discovered. Every individual person must reflect on ones’ self and use the knowledge gained to better themselves.       A teacher’s role is to cure social ills such as alchoholism and the such.       Athenians however, weren’t ready to accept and criticize themselves. This was because Athens was a very wealthy city-state; due to which people were ego-istic and self-centric in nature.       Socrates believed that humans are reasonable, and stated that experts are required to understand what is just and unjust. He also emphasized the importance of establishing such a institution in Athens.       Socratic Method   This method is a way of cross-examination, used to test the validity of an opinion. A dialogue is initiated, and skilful questioning is done to prove/disprove its importance.   The Socratic Method includes:           Scepticism of truth of the matter under discussion. This is provisional wheras Sophists’ scepticism was definite in nature            Conversation - Dialogue with the motive of discovering truth            Conceptual in nature            Empirical or Inductive reasoning; reffering to particular instances            Deductive reasoning; testing validity           Plato      Disciple of Socrates, Theory of Ideas    Plato felt that Socrates missed upon utilizing Ontology while discussing ethics, and took it upon himself to do so. He brought back Metaphysics into discussion. His theory of ideas tries to combine epistemology, metaphysics and ethics.   He believed that genuine knowledge is unchanging and absolute; but our senses and experiences keep changing. (Latter was inspired by Heraclitus) He emphasized that true knowledge could be comprehended through reason.   Metaphysics - Idea of a form   The concept was originally introduced by Paramenides. It refers to the “being-ness” of an object. (What makes a Banyan Tree, a banyan tree?)   Although plato did not agree with everything said by Parmenides, he did use this concept in his works.                  Paramenides       Plato                       Idea is not a thought       It may be an object of thought                 Only one idea       Multiple, indivisible ideas exist                 Not known to us, as human knowledge is not absolute       Can be grasped using reason           Plato argues that ideas are universal and eternal, and that they aren’t subject to change. Ideas correspond to an abstract concepts, and cannot be seen directly.   Ideas are:     non-temporal   existing in an independant realm   participate in particulars   Plato believed that Forms are not related to the Physical world, that they existed in their own plane of existence.   That is, the idea of horseness is independent of the original horse it might have been based on; and exists on even after the original horse has died.   Therefore, Plato’s originality lies in raising the issue of universals when talking about particulars. For example, beautiful objects are related to the universal, beauty.   Dialectical Method   This method is used for proper comprehension, and is used to try and comprehend the form of an object/concept. We first try to generalize scattered particulars into a single idea, and then we try to classify this broad general idea into specific smaller classes. That is, this method consists of:      Generalization   Classification   This allows for clear and consistent thinking. This is also called as Thinking in Concepts.   He introduced this because he felt that the Sophists’ ideas were too cluttered and this caused confusion amongst the people regarding the true nature of things.   This requires that language has general words such as “similar” and “before”, instead of just proper nouns. That is, you cannot say that a Bamboo Banyan Tree and a Oak Tree are similar without knowing what similarity is.   Metaphysics and Epsitemology   (It was then believed that soul was responsible for thinking)   Plato believed that knowledge is latent in the soul, bringing both these branches of philosophy together. He said that the soul can contemplate on pure eternal ideas.   Idea of Unity and Diversity, all horses are united under the concept of “Horseness”; wheras the presence of various horses is diversity.   He believed that the idea of good is LOGOS. That is, he believed the idea of good to be the supreme idea.   That is, plato believed that experience is not the source of concepts. Rather, conceptual knowledge is the only genuine knowledge.   He believed in a Hierarchy of Knowledge, where doxa had the lowest position. He believed genuine knowledge to be infalliable in nature, because forms are unchanging and eternal. This is different than what Heraclitus and Protagoras were saying. The knowledge attained via senses is thus falliable in nature.   Therefore, attaining genuine knowledge requires reflection upon senses by the mind and soul.   Platonic Ethics   Plato believes in the pre-existence of soul, and that the body-soul relation is akin to the relation between a chariot and a charioteer.   Tri Partite Division           Rational Faculty: Intellectual activities are handled by this faculty            Spirited Faculty: Responsible for decisions and will. Irrational in nature.            Appetitive Faculty: Desires for pleasure, wealth and the such       He also describes two types of pleasures; intellectual and sense-driven.   Psychological Relativity: Not being driven by either pleasure or pain   Moral perfection can only be attained only when control over spirited and appetitive faculty is acheived. The purpose of life is to be good, not to chase pleasures. He says that this is possible only upon self-reflection.   ","url": "http://localhost:4000/notes/hs301/socr-per"
    },{
      "title": "Finances",
      "excerpt":"Financial Markets   Institutions where a person who wants to wants to save can directly supply funds to a person who wants to borrow.   Bond Market - Debt Finance   Bond is a certificate of indebtedness, and IOU from the company to the buyer. The amount borrowed is called the principal and the time when the bond matures is called date of maturity.   Bonds have four parameters which indicate how valuable they are:      Term: time until a bond matures. Long term =&gt; more risk   Credit Risk: How likely is it that the company will not declare bankruptcy and default the bond.   Tax Laws   Inflation Protection: Are the bonds indexed?   Stock Market - Equity Finance   A stock is a partial ownership of the company. The gains and losses are shared by the stock owners proportional to the number of shares that they own in the company.   Stock Index: a measurement of overall stock prices.   Financial Intermediaries   Institutions where a borrower can indirectly take funds from a saver. There are two financial intermediaries.   Banks   ….what we all know and love….   Mutual Funds and Index Funds   Mutual Funds operate with stocks and bonds, and offer indirect purchases at a fee. A benefactor for a mutual fund accepts the returns on the selection (portfolio) of shares bought by the funds, be it a profit or a loss.   Mutual Funds do active trading wheras Index funds just buy and hold from the companies with high index values.   Mathematics of Savings   We have already seen the expenditure approach for calculating the GDP of a nation. Consider the following equations for a closed economy: \\(\\begin{align*} \t\\text{Y} &amp;= \\text{C}+\\text{I}+\\text{G} \\\\ \t\\text{Y}-\\text{C}-\\text{G} &amp;= I \\\\     \\text{Savings} &amp;= \\text{Investment} \\\\ \\end{align*}\\) That is, the savings and investment are equal in a closed economy. Note that this is true for the economy as a whole, and not for every individual household. Also, a person buying bonds and stocks is not “investing”, but rather is “saving” his money as no capital is being purchased.   Define T to be the Taxes minus the Transfer payments. For this, we can manipulate the identity as follows: \\(\\begin{align*} \\text{Savings} &amp;= \\text{Y} - \\text{C} - \\text{G} \\\\ &amp;= (\\text{Y}-\\text{T}-\\text{C})+(\\text{T}-\\text{G})\\\\ &amp;= \\text{(Private Savings) + (Public Savings)}\\\\ \\end{align*}\\)  When Public savings are negative, the economy is said to be in Budget Deficit whereas when the public savings are positive, we say that the economy is in a Budget Surplus. The accumulation of deficits is called as Government Debt. Also, when Public Savings is 0, we say that the government has a balanced budget.   Market for Loanable Funds   “Loanable Funds” are the funds which are available to fund private investment. It can be clearly seen that it would be equal to the savings of the government, the national savings. The supply-demand graph for this hypothetical market has interest rates on the y-axis and funds people are willing to deposit on the x-axis. The supply of loanable funds are the national savings, and demand is the amount people/companies wish to borrow for investment. As interest increases, more people would like to save, increasing supply.      Taxation on Investment Returns: Shifts the demand curve to the left as taxation is increased.   Taxation on Savings: Shifts the supply curve to the left as taxation is increased.   Budget Deficit: **Public savings are negative, meaning that the government sells bonds which reduces the funds available for investment. This phenomenon is called **crowding out, and would cause the supply curve to shift to the left.   ","url": "http://localhost:4000/notes/hs101/mic4"
    },{
      "title": "Page Not Found",
      "excerpt":"Whoa there, fella! What’d you do? If this was caused by me linking the pages incorrectly, please let me know in the feedback.   To be redirected to the home page, press here.  ","url": "http://localhost:4000/404.html"
    },{
      "title": "Aristotle",
      "excerpt":"Aristotle is a philosopher from the Socratic period as well. He was the disciple of Plato, and tried to be more systematic. He was responsible for:      Division of Philosophy into branches (eg Metaphysics and Physics)   Systematic casual explanations      Philosophy as love of wisdom must be consistent and scientific    Aristotle’s work was focused on the systematic reconstruction of philosophy, and the development of ethics.   He disliked the lack of scientific explanation in Plato’s philosophy, especially regarding forms/ideas. He denied the division between ideal and material.   Aristotle and Knowledge   Aristotle believed experience to be the basis of knowledge, and true knowledge could only be obtained after knowing reasons or causes for an occurence.   (That is, he was neither an rationalist nor an empiricist) *(in my opinion)     He classified science into four categories:      Logic   Theoretical Sciences - Abstract knowledge like chemistry   Practical Sciecnes - means of conduct, ethics, politics   Productive Sciences - aesthetics, poetics   Inherence of Forms   Forms are not apart from things, but are inherent in them. They are eternally together; and matter combines with form to constitute individual things.   Plato thought the phenomenal world to be a shadow of the ideal, real world. Aristotle on the other hand advocated that the phenomenal world was not a copy of the real world. Rather, it is the real world.   This take by Aristotle advocated Realism and led to the progression of Natural Science.   Hierarchy of Forms   Aristotle believed that plurality of individual substances exist in a hierarchial order, with matter at the bottom and Pure form/God at the top. He believed matter to be the primoridal stuff.   Every individual substance is a mixture of matter and form, and matter is responsible for giving an object its uniqueness and particularity.   Human reasoning can discern the form.   Explanation of Change   (Potentiality and Actuality)   Plato’s idea of forms does not tackle the concept of change as he believed form to be non-temporal. Aristotle’s ideas challenge this notion.   He believed there to be 4 reasons for a substance changing. Using clay for making a pot as an example, these reasons are:      Material Cause: Clay is the basic, primordial stuff        Efficient Cause: Agent bringing change to primordial stuff (potter)       Formal Cause: Structure/Form of the pot   Final Cause: Use of the pot   Formal and Final cause are closely interlinked with Teleology. Aristotle believed that the best way to understand why things are the way they are, is to understand the purpose that they fulfil.   Logic and Syllogism   Syllogism is demonstration in the form of deduction. It utilizes properly formed arguments to arrive at a conlclusion from premises reagrding a middle term.   Propositions being used in an argument are called as Premises. There are four kinds of propositions:      Universal Affirmative (A) - “All X is Y” (distribute subject; X)        Particular Affirmative (I) - “Some X is Y” (distribute none)       Universal Negative (E) - “No X is Y” (distributes both subj and pred; X, Y)   Particular Negative (O) - “Some X is not Y” (distribute pred only; Y)   Similarly, Major Premise is universal in nature and Minor Premise is particular in nature.   Structure of Syllogism      Terms   Propositions   Moods - Depends on the type of proposition   Figures - Depends on the position of the middle term   Rules   A proposition contains three terms: Subject, Predicate and the Copula(relates to both subject and predicate, “is/are”).   Consider the below argument.      All games are better than League of Legends       Some games are unfinished       Therefore, some unfinished games are better than League of Legends    The mood of the above argument is AII. The middle term appears as subject in both the premises, defining the Figure of the argument.   There are four relations between propositions:      Contradictory: A/O, I/E (negations)   Contrary: A/E (“hard opposites”)   Sub-Contrary: I/O (“soft” opposites)   Subaltern: A→I, E→O (Superset)   Rules      The fallacy of the undistributed middle: Middle term must be distributed atleast once   Illicit Major: Middle term not distributed in major premise   Illicit Minor: Middle term not distributed in minor premise   The fallacy of two exclusive premises: Two negative premises       Ethics and Aristotle   He believed ethics to be Anti-Hedonistic, and Intuitionsistic in nature. He justified anti-hedonism by making a distinction between the pleasure of mind and pleasure of the body.   Aristotle says that man is a combination of body and soul. The soul is an inorganic unit, and is responsible for moving the body and for perceiving information.   He stated the following parts of a soul:      Rational: Capable of rational thought (man has all three)   Sensible: Capable of sense (animal have sensible and nutritional)   Nutritional: Capable of nutrition (plants have only nutritional)   Aristotle believed mind to be higher than the soul. It is independant of substance, and is immortal. Mind is the power to think.   That is, the soul could die and decay alongwith the body but mind doesn’t do so.   He also talks about the nature of reason, and categorized reasons into different types. They are;           Active Reason       Arises in the course of soul’s development with the other psychic functions. This refers to thinking about the consequences of an action.       This can be identified with universal reason. (is the action being performed desirable?) This is the divine mind coming to the soul.            Creative Reason       Here all concepts are actualized, and thoughts and objects are one. (How is a pot shaped from clay?)       Creative reason is immortal, imperishable and is not bound to the body.       Eudaimonia   Aristotle aimed to answer Socrates’ answer of highest good. The “good-ness” of an object depends upon the realization of its specific nature. That is, a knife is good if it performs its duty of cutting stuff well.   Similarly, a human being can be good only upon the habitual exercise of the function which makes him a human being. That is, rational thinking and virtuous actions have to be exercised.   Pleasure usually accompanies virtuous activities. Pleasure can be included as the highest good, but is not the highest good itself. This indulgence with virtuous activities is known as Eudaimonia.   Virtue   Aristotle maintains that there are two types of virtue; intellectual and moral.      Intellectual Virtue is related to wisdom, knowledge and perfection   Moral Virtue is more emotional in nature, regarding doing the right thing   Phronesis is also considered, and it is the practical virtue   He stated that virtues are moderation between excess and deficiences. For example, modesty is a mean between bashfulness and shamelessness. This mean does differ from individual to individual. A virtuous man should be able to make this choice after assessing the situation.   Justice   Aristotle felt that self-realization does not include selfish individualism. He tried to be altruistic in nature, and said that a virtuous man must love goodness for its own sake.   Justice is altruistic by nature, as justice is doing good for the society. A just society is where lawfullness and fairness are emphasized. Justice should be inclusive of all virtues.   Justice is giving man his due.   He believed that the highest happiness is a scpeculative activity, an activity which takes the form of contemplation.  ","url": "http://localhost:4000/notes/hs301/aris"
    },{
      "title": "Unemployment",
      "excerpt":"The population mass is divided into three categories for studying unemployment.      Employed: This category includes those who work as paid employees, work in their own business, or work as unpaid workers in a family member’s business. Both full-time and part-time workers are counted. This category also includes those who were not working but who had jobs from which they were temporarily absent because of, for example, vacation, illness, or bad weather.   Unemployed: This category includes those who are not employed, but are available for work, and have tried to find employment during the previous four weeks. It also includes those waiting to be recalled to a job from which they have been laid off.   Not in Labor Force: Students, home-makers, retirees etc. Home-makers are NOT unemployed!   Normal Unemployment is the usual rate of unemployment present in an economy. Cynical Unemployment refers to the variations around this mean rate. This is closely associated with the short-term economic activities and events.   Note that it is difficult to distinguish between the unemployed and the people not in the labor force. It is common for people to interchange between both states, such as a student finishing his studies or an early retiree being forced to look for a job to make ends meet. Also, Discouraged Workers the people not in the labor force, but wiling to work (they’ve left the force due to continuous rejections).   Duration and Types of Unemployment      Most spells of unemployment are short, but most unemployment observed at any given time is long-term.    That is, over time, most of the unemployed people find a job within a short period of time. However, at a given point of time, the number of unemployed who have not found a job for a long time is larger.   There are a few types of unemployment defined which try to answer this question.      Frictional Unemployment: This is caused due to the time taken to match an employer to the prospective employee with the right skill set. This is thought to be the main cause of short-term unemployment.   Structural Unemployment: This occurs when the wages are higher than the equilibrium price, causing excess in the labor that is willing to do the job (look at the demand-supply graph). The wages are usually set higher because of minimum-wage-laws, unions, and efficiency wages.  ","url": "http://localhost:4000/notes/hs101/mic5"
    },{
      "title": "Modern Philosophy",
      "excerpt":"Modern Philosophy primarily consists of two schools of thought, Rationalism and Empiricism. Their epistemological discourse varied, with Rationalists claiming reason to be the primary source of all knowledge and empiricists claiming experience to be the primary source instead.   The spirit of modern philosophy emphasized the spirit of independence, and was marked by an awakening of the reflective spirit and quickening of criticism. Further more, there was:     Demands for freedom in thought   Revolt against absolutism and collectivism   Revolt against authority   Focus shifted from contemplation of the supernatural to reflection on natural things. Reason was started to be taken as authority in science and philosophy. For example, the introduction of Heliocentric theory was in this era. Practical application of science in technology and medicine was also observed.   Rationalists and Empiricists debated on the nature of truth as well. Rationalists believed truth to be absolute, and be a priori in nature. The empiricists on the other hand felt truth to be a posteriori in nature, and disagreed that truth was absolute. The empiricists also do not believe in a world beyond our own, such as heaven.                  Philosopher       School of thought                       Francis Bacon       Empiricism                 Thomas Hobbes       Empiricism                 Rene Descartes       Rationalism                 John Locke       Empiricism                 David Hume       Empiricism                 Francis Bacon      Father of Empiricism    Introduced the method of Induction. He believed that human knowledge must be based on a firm foundation, and emphasized on the importance of social order in science and technology.   His obsession with order led him to be the first to document the Scientific Method. To summarize, the model of knowledge is natural science, the method is induction, and the goal is invention.   The Scientific Method      Novum Organum    The method introduced by Aristotle was teleological, where only the purpose of the object mattered. This method on the other hand utilizes mathematics and methodological observation. (math was ignored before)   Inductive methods are used to deduce mathematical laws. Idolas, or prejudices were encouraged to be ignored for an un-biased observation. Some examples of Idolas are:      Idola of Tribes - general tendencies to be deceived, such as a tendency to overgeneralize   Idola of the den (Specus) - individualistic dispositions regarding education and authority   Idola fori (market) - association of words and names of actual objects   Idola of theatre - result of false theories or philosophies   Induction   Not a case of simple enumeration. Investigate a group of entities with the goal centered around finding out a general cause. Notice that this method is closely related to the cause-effect relationship.     Deductive reasoning is only concerned with intuitively conceptualizing statements, and not about facts. Induction is concerned with utilizing experience obtained by observation to verify facts.   Implicitly, investigation is centered around finding the cause, meaning that a causal relationship was believed.   There are three forms of induction:           Mill’s method of Agreement - Finding out qualities from positive instances            Mill’s method of Difference - Observing that a quality is lost when a form is taken away            Mill’s Concomitant Variation - The object of out inquiry is present in greater or lesser degree             Thomas Hobbes   Gave a mechanistic explanation of reality, and tried to utilize science and philosophy practically. He had a heliocentric worldview, and argued in favor of materialistic philosophy.   Thomas Hobbes was an Empiricist.   He believed that faith and feeling had to take a backseat as the source of knowledge.   Theory of Knowledge   He states that senses are the origin of all thoughts - it persists and is retained in memory. Reasoning is a conceptual arithmetic, and thoughts are regulated by desire.   He believed that all knowledge was conditional, and also that the mechanism of body was responsible for senses and mind. (not some metaphysical stuff)   Hobbes tried to impart more practical utility to science and philosophy. He argued in favor of a materialistic philosophy (as opposed to Descartes). He also supported the heliocentric view of Copernicus and Galileo. Hobbes’ theory of knowledge says that reasoning is conceptual arithmetic, with logical addition/subtraction of concepts. The origin of all thoughts is senses, since sensation persists and memory is retained. These thoughts are regulated by desire. The mechanism responsible for causing sensuous experiences is the body.         Rene Descartes      “Father of Modern Philosophy”    Rene Descartes was a Rationalist. He didn’t outright reject the notion of god and the metaphysical like Hobbes, rather, he tried to prove the existence of god through logical arguments.   He did believe that faith and feeling had to take a backseat as the source of knowledge, just like Hobbes.   He utilized mathematics as the model for his philosophy. He used both the deductive methods and inductive methods in his works, which is a strength of his theory.   He classified the sciences as follows:           Metaphysics is the principle of knowledge concerning God, immateriality of the soul and all clear and simple ideas that we possess            Physics is the principle of material things, such as the things examined in general          “Philosophy as a whole is like a tree with metaphysics as its roots and physics as its trunk. All other sciences comprise of the branches from this trunk.”    He also followed Methodological Skepticism, where Deductive and Inductive reasoning are used to arrive at a certain and indubitable truth. He was not a skeptic, however, as he wanted to use skepticism to reach certainty.   He doesn’t doubt truth itself, he rather doubts the methods used to arrive at the truths. He states the following as reasons for skepticism:      truths in traditional scholastic systems are based on mass-divergent opinions   experiences, as senses are deceiving   difficult to distinct between dreaming and waking   memory can be deceptive   evil agents can be deceptive   at times, demonstrations of mathematics can be deceptive (calculation mistakes…)   Doubting involves thinking. Therefore, there must be a doubter to doubt. Thus, “I doubt, therefore, I think, therefore, I am.”      “Dubito ergo cogito ergo sum”    Proof for Existence of God   The motive of evils is to deceive. Humans are imperfect, and this implies the existence of a perfect and infinite being. God cannot deceive, since deception is evil and God cannot be evil.   Descartes said that the idea of God is not derived from an external world via senses. That is, God being a perfect being and thus, cannot be caused by the world itself. He felt that senses only affected us, but they did not reveal anything to us. Therefore, he concluded it to be an innate idea or an a priori idea.   Note that Descartes was before Newton, so he did not know how objects moved. It was all attributed to God being an Immovable Mover.   Proof for the Existence of World   We know that the world exists because of our feelings. The feelings of pleasure, pain, hunger and instinctive reactions are all proofs that the external world exists. However, these sensations can be deceiving. Such delusions can be corrected by the power of intellect endowed by god.   Existence of Bodies   Descartes was a dualist, he believed that mind and body were two separate substances which existed independently of one another.   The essence of mind is thought, and the body is divisible whose essence is extension. Descartes firmly states that the mind and body are distinct, and are conceived independently of each other. The occasional causal relationship between mind and body is mediated through the pineal gland.        John Locke   He was a Bri’ish empiricist, who did not agree with the ideas presented by Rene Descartes. That is, he did not accept the notion of innate ideas. Instead, he believed that all knowledge is accumulated post-birth via experience. He compared the mind with a blank slate, with knowledge being written on it. He is also well known for his work in proposing the Social Contract theory.   Ideas   He believed there to be two sources of ideas:     Sensation   Reflection   He also categorized ideas into two types; Simple and Complex. Simple Ideas are the ones which enter the mind by only one sense. Complex Ideas on the other hand are received by reflection by the mind on the ideas that it already has.   For example, “something black” is a simple idea. Deducing that the black thing is a crow based on previous experiences and current state is a complex idea.   Some ideas are perceived through both sensation and reflection. Examples include pleasure, pain, power, existence, uneasiness, unity and so on.   Qualities   A quality of an object is the power that it has to produce ideas.     Primary Quality - solidity and extension, inherent in the object   Secondary Quality - sound, taste, colours   Secondary qualities are the qualities of an object which can be changed easily, whereas the primary qualities are relatively not changeable.   Modes   A simple mode consists of variations of the same idea, repeated in different combinations without any mixture of the other. For example, a day or a month is a mode of representing a single idea, namely, time.   Mixed modes are compounded simple ideas of several kinds put together to form a complex one. For example, beauty consists of many factors like colour, figure and so on.         David Hume   Hume is an empiricist. He agreed with Descartes, Hobbes and Locke that genuine knowledge must be self-evident, but he finds no such knowledge anywhere other than mathematics, which merely analyses its own concepts. Humes was:      Empirical: knowledge has source in experience   Positivistic: knowledge is limited to world of phenomena   Agnostic: we know nothing of ultimates, substances, soul.etc   Humanistic: human mental world is the only legitimate sphere of inquiry   Hume felt that all sciences had relations to human nature, and subdivided the science of human nature into three:      Logic - Reasoning   Moral and Criticism - tastes and sentiments   Politics - humans are dependent on society   Humes stated that True Metaphysics is the science of understanding, and that moral philosophy provides foundation to all other sciences.       Impression and Reflection   All the materials of our thinking are derived from outward/inward impressions. “Impressions” are the more lively perceptions, such as when we hear or see or feel or love, namely all our sensations, passions, and emotions as they make their first appearances in the soul. The inward perceptions are those that are internal (like the sensation of pain or hunger), and the outward perceptions are those that have external origin (like cold or heat). Desire, aversion, hope, and fear are impressions of reflection.   All our thoughts are copies of such impressions. Thoughts are the less lively perceptions such as the faint/feeble perceptions of which we are conscious when we recall/reflect on sensations that arise in the soul from unknown causes, while inward impressions are frequently occasioned by ideas. Knowledge results from compounding the materials due to senses and experiences. We can also associate ideas by observing certain connections, regularities, or contiguities in time/place.   The idea of causation is not reached by any a priori reasoning. The mind cannot deduce the effect from a certain cause, and the study of cause and effect is a study of speculation and practice. We cannot demonstrate that a certain cause has a certain effect. The knowledge of causation is attained through observation and experience. This experience eventually produces a belief, which is an operation of the mind.   All reasoning concerning matters of fact are dependent on cause-and-effect relationships. The evidence is compared with mathematics. The uniformity of nature is a self-evident knowledge (the sun will rise tomorrow). However, even this does not constitute absolute certainty. Hume says that nothing is necessary, and we move from this to probabilities of occurrences instead. Practice is the best cure for skepticism, since being skeptical of everything would lead to no practical good.   Impressions and sensations appear and reappear in our experience. All we can do is to limit ourselves to these, compare our ideas and note their relations, then reasoning about these relations and attaining some kind of demonstrative knowledge. It finally boils down to noticing regularities in observations, leading to a custom, and drawing inferences from them. Passion is the source of understanding the world.   That is, David Hume was a skeptic and he did not believe in the theory of causation.  ","url": "http://localhost:4000/notes/hs301/modern"
    },{
      "title": "Monetary Systems",
      "excerpt":"Money is a set of assets that people in a community use to exchange goods and services. Money has three functions:      Medium of exchange:  an item that buyers give to sellers when they want to purchase goods and services   Unit of account:  the yardstick people use to post prices and record debts   Store of value an item that people can use to transfer purchasing power from the present to the future   Liquidity refers to how easily an asset can be converted into money, and wealth refers to all monetary and non-monetary assets.      Commodity Money has intrinsic value, and is used as the medium of exchange. Gold Standard is an economy that uses gold as its commodity money, or an economy where money can be easily converted to ownership of gold.   Fiat Money has no intrinsic value, and its success depends upon the socio-economic status of the country.   Measuring Money Stock   Money Stock is the total amount of money that is present in an economy. This includes not only the currency (money in hands of the public) but also other assets which have a reasonably high liquidity. We divide money stock into categories M1 and M2. Here, M2 is more relaxed, meaning that assets can have lower liquidity than the assets in M1. (Do understand that all assets that belong to M1 will belong to M2 as well.)   Credit Cards do not count as money or assets!      M1: Currency, Travel deposits, cheque-able deposits .etc   M2: Everything in M1, Savings Deposits, Mutual Funds’ deposits, small time deposits .etc   Banking and money supply   Banks have the following balance sheet made, in order to identify assets and liabilities; and to see how much funds are present in each sub category. (assets = liabilities as a whole by definition)                  Assets       Liabilities                       Reserves       Deposits                 Loans       Companies’ debt                 Securities (buy stocks/bonds)       Owner’s equity (capital)           Reserve Ratio (R)  is the fraction of reserves held out of the total assets. A minimum RR is mandated by the government, but banks can have excess reserves to ensure that they can pay back the deposits. Money Multiplier is the maximum multiplier that the money supply can increase to.  \\(\\text{Money Multiplier} = \\frac{1}{R}\\)   Central Bank   The central bank is an institution which oversees the banking system and regulated the amount of money in the economy aka money supply. It does so by altering monetary policies as the need arises. The central bank firstly acts as a last resort lender of money for banks in financial crisis, a bank’s bank. The interest rate offered to banks by the central bank is called as the Discount Rate.      A higher discount rate decreases the money supply   A lower discount rate increases the money supply   It also alters the money supply predominantly by Open Market Operations (OMOs).      To decrease the money supply, the central bank sells bonds. The acquired funds are out of the hands of the public.   To increase the money supply, the central bank buys bonds. The new funds are in the hands of the public, and a new dollar as currency increases the money supply by 1$ but a new dollar in a bank increases the money supply by $1$\\times 1/R$ .   The reserve ratio (R) is altered by the central bank in these ways:      Altering Reserve Requirement: Self explanatory, change the minimum required R. A larger requirement decreases the money supply.        Changing Reverse Repo Rate: The central bank can offer interest on the reserves that bank have, encouraging the banks to have larger reserves. A larger interest would reduce the money supply in the economy. Measured in Basis Points, where 1 basis point corresponds to $0.01\\%$. The Repo Rate is the rate at which banks borrow money from central bank after depositing securities (unlike discount rate).       Change Federal Funds Rate: The federal funds rate is the interest rate that a bank-to-bank loan has. The loans are for very short durations of time, such as one day. These ensure that a bank can repay its depositors if it runs out of reserves. Increasing the interest rate would decrease the money supply.  ","url": "http://localhost:4000/notes/hs101/mic6"
    },{
      "title": "Indian Philosophy",
      "excerpt":"Indian philosophy is essentially spiritual in nature. The enquiry in the western world was centered around man, and making the society well developed. Indian philosophy on the other hand provided an epistemological perspective.   Indian philosophy being religious in nature has been looked down upon as the western world was concerned with a scientific pursuit of the true nature of everything. However, it emphasizes the practical realization of truth. See the self (Atma va are drustavyah) is the keynote of all schools of Indian Philosophy.       Vedas and Upanisads   Vedas are known as shrutis or knowledge. Poetic hymns of Vedas matured into philosophical thinking called Upanisaads. Vedas are written in a poetic form, and they are supposed to be eternal and author-less.   Vedic matras were practiced by Brahmanas and Arnyakas. Upanisadas is a recording of intellectual, and philosophical discourse.   A transition from naturalism and anthropomorphic polytheism to transcendent monotheism was noted in the pre-upanisadic philosophy.       Purusa and Rta      “Ekam satya vipra bahudha badanti” - Rig Veda translation: “The one is real, the wise declare many”    Purusa is the universal consciousness. It is supposed to be perfect, and omnipresent. It is the custodian of Rta, the cosmic moral order, the binding soul of the universe (pure consciousness). This emphasis on the universal consciousness is called Monistic Idealism.   Purusa is interpreted differently in different vedas, like Ukta in Rig Veda, Agni in Yajur Veda and Mahavrta in Sam Veda.       Atman and Brahman   Atman is the the spiritual life principle of the universe, centered around an individual. That is, its essentially the human soul. Brahman on the other hand, is the universal principle. It has been interpreted differently in different upanisads.   These concepts were introduced in the Chhandogya Upanisad. There were said to be three modes of being for a human being:     Walking Sleep - Bodily self   Dreaming Sleep - Empirical Self   Dreamless Sleep - Absolute/Metaphysical self   They believed one who is dreaming to be free, and there being no self to feel. The self appears to be unconscious, and is said to be a desire-less state. Body is not the self, it rather exists for the self.      “Self can be known to no object” - Brhadarnyaka Upanisad        Mandukya Upanisad      Vishayanubhuti: Self in walking state enjoys the world   Taijhasa: One imagines themselves in dreaming state   Prajña: Transcendental consciousness, above the dichotomy of dreams and desires   Turya: Foundation of all knowledge, can be realized directly or indirectly       Katha Upanisad   Uses the chariot analogy to convey purpose of each body.                  Chariot Part       Actual Part                       Road       Objects                 Chariot       Body                 Horses       Senses                 Reins       Mind                 Charioteer       Intellect                 Enjoyer       Ego                 Lord sitting on the Chariot       Atman               Taitreya Upanisad - Koshas   Brahman is the source of all evolution. Self is the Sarvaphutama, the self-consciousness.      Annamaya - Matter   Pranamaya - Life   Manomaya - Consciousness   Vignanamaya - Self-consciousness, ability to understand what/why we do stuff   Antaryamin - Self   Brahman is non-dual and transcendent. When evolving, the lower state of brahman is not lost, but is instead transformed.       Orthodox philosophical systems accept the authority of Vedas and Upanisads. Examples include Sankhya, Mimanasa and Nyaya-Vaiseshuika. Heterodox question the authority, and include Charvaka (Materialism), Buddhism and Jainism.       Charvaka - Indian Materialism   This school of thought is against excessive monkdom. It was felt that the idealism in Upanisads was unsuited to the common-folk of the society, due to ritualism and societal exploitation. Brhaspati was a heretical teacher, and the founder of Indian materialism.   Charu Vak means eat, drink and merry. Loakatya is a low man, an arch heretic. Perception is accepted by the Charvakas as a source of knowledge. However, Inference and other sources of pramana were not acceptable to them. This is because inference proceeds from the known to unknown, which they didn’t accept.   Criticism of this school of thought arises with the point that perceptual knowledge is often misleading.      Matter creates mind as the liver secretes bile    They believed consciousness to be emergent as a result of bodily activities. They felt the soul to be the living body, and consciousness to be the mere product of matter/material elements. They felt the body to be comprised of earth, fire, water and air. (not ether!)   Criticism arises with the point of self-consciousness, why are humans self-conscious but animals are not?   They do not accept Dharma and Moksa, and only accept Kama and Artha. They did not believe in after life.         Sankhya Philosophy   It is described as being:     pluralistic spiritualism   atheistic realism   uncompromising dualism.   Dualism is when two metaphysical principles are accepted, such as in Descartes’ mind-body dualism. Similarly, sankhya takes prakruti and purusa as the principles. Prakruti is primordial (pradhana), and purusa is inactive but conscious (a witness).   Atheistic realism because it doesn’t talk about God. However, it is still orthodox in nature and accepts upanisads and vedas as the authority.   Pluralistic Spiritualism because Sankhya believes purusa to be existing in many forms and things. Example, multiple human beings are conscious.   Sankhya Theory of Causation   Satkaryavada says that the effect is an explicit manifestation of whatever pre-existing in the cause. Pakrtiparinamavada says that all effects are modifications of parimana or prakruti.   This theory provides 5 arguments for Satkaryavada:           Asadkaranat: If the effect does not pre-exist in cause, then it would be a mere non-entity            Upadanagrahanat: The material cause is invariably connected, like how the pot pre-exists to some extent in clay which the potter brings out            Sarvasambhavabhavat: Everything cannot produce everything, oil comes from oil seeds and not anything else            Saktasya Sakhyakaranat: Actualization of potentiality connected in material cause, recognizing that seeds can produce oil and witnessing its manifestation (like the efficient cause)            Karanaabhavat: Effect is in essence of its material cause, like how specific steps have to be followed for making a pot from clay       Prakruti   There are five main characteristics of prakruti;      Pradhana: The first principle of universe   Avyakta: Imperceptible   Jada: Unconscious and unintelligent   Shakti: Ever active with unlimited power   Rajas: Inherent motion   There are five main arguments for the existence of prakriti.           Prakriti is the source of universe. All individual beings are finite, but the world is infinite. finite cannot cause infinite, hence prakriti must exist.            All worldly things posses certain common characteristics such as feeling pain, pleasure and indifference. This implies they share a common source. (Trigunas: the three common feelings)            All effects arise from activity of the potent cause, which must be inherent in the world.            The effect differs from the cause, and hence the limited effect cannot be its own cause. Effect is explicit and cause is implicit which needs to be inferred.            Unity of universe points to a single point, which is prakriti       Gunanam Samyavastha   There are three constituents of Prakriti; Sattava (pleasure), Rajas (pain, principle of motion) and Tamas (indifference, principle of inertia). Collectively, these three are called as Trigunas.   Although the gunas are constituents of prakriti, they are still subservient to Purusa because prakriti and purusa are intertwined.   The trigunas are inseparable from prakriti. They conflict yet co-operate with each other, and the composition of things is determined by these gunas. These gunas are dynamic, and ever-changing.   Change is either Homogeneous (Swarupa Parimana) or Heterogeneous (Virupa Parimana). Prakriti is ever-changing in nature.   Evolution is creation, its cyclic and not linear. It also is Teleological in nature and not mechanical.   Evolution is possible only if Prakriti and Purusa are together. Prakriti needs purusa to be known (darshanartham), and purusa needs prakriti to be enjoyed (bhoga, kaivalyartham).   This is why Sankhya is atheistic, it believes only in the co-operation of prakriti and purusa with each other.   Purusa   It is the principle and pure consciousness. Purusa is neither the body, nor the senses. It is not a substance which possesses consciousness, it is consciousness itself. Consciousness is not a feature of purusa!   Do note that Sankhya believes in plurality of purusas.   There are several arguments for the existence of Purusa, mentioned below.           Sanghataparthavat: Prakriti evolves to serve purusa’s end, Teleological proof.            Trigunadiviparyat: Trigunas imply the existence of nitalgunya - beyond gunas            Adhisthanant: Purusa is ontological, all knowledge implies the existence of self. It is the base of empirical knowledge.            Bhoktarbhavat: non-intelligent prakriti cannot enjoy the fruits, purusa is needed            Kaivalyartham pravartteh: The desire for liberation from the suffering of the world. Aspiration pre-supposes the aspirant, this proof is mystical and religious in nature.             Yoga   A spiritual union between the individual soul and the universal soul. It is a spiritual effort to attain perfection through control of body, senses and mind through the right discrimination between purusa and prakriti.   Yoga as a philosophical system accepts metaphysics and epistemology of Sankhya.   There are four stages for performing yoga.           Samadhi Pada: nature and aim of concentration            Sadhana Pada: means to realize the end            Bibhuti Pada: super-natural powers attained through yoga            Kaivalya Pada: nature of liberation and transcendental self       Yoga aims to cease modification (Vrtti) through concentration, and to return Purusa to its original perfection.   It is the modification (Vrtti) of Chitta, also known as Antakrana. Chitta appears to be conscious due to reflection of purusa.   Chitta can be modified in the following ways:      Pranama: Right cognition, perception, inference and verbal testimony   Viparyaya: Wrong cognition   Vikalpa: Verbal cognition or imagination   Nidra: Absence of cognition or sleep   Smrti: Memory   Chitta is the reflection of purusa in it, and there are five kinds of sufferings.      Ignorance: avidya   Egoism: amsita   Attachment: raga   Aversion: Dvesa   Abhinivesh: clinging to life and instinctive fear of death   Astanga Yoga is the eight fold path to discipline as explained by Yoga.           Yama: abstention via 5 vows; Ahimsa, Satya, Asteya (not stealing), brahmadharya (celibation), aparigraha (not possessing stuff)       Niyama: external and internal purification            shaucha       santosh (contentment)       tapas (austerity)       iswarparinidhana (devotion to god)                Asana: Sitting in a comfortable posture for meditation            Pranayama: controlled breathing            Pratyhara: controlled senses            Dharana: Fixing mind on object of meditation            Dhyana: meditation with undisturbed flow of though around the object of meditation       Samadhi: concentration         Buddhism - The Philosophy of Buddha      The light of Asia    Buddhism is called heterodox because it is against Vedic philosophy. Buddha was an ethical teacher and a social reformer than a theoretical philosopher. Buddha was strongly against the sacrificial practices which were being followed in the society.   Life is rooted in pessimism, and is a fight against misery and suffering.   Buddha’s teaching were mainly oral. He didn’t believe in the eternal, rather that everything is momentary. They have been written down in three parts (Tripitaka) called VinayaPitaka, SuttaPitaka and AbhidhamaPitaka.   Four Noble Truths           Duhkha - Suffering       Life is full of misery and pain. Even in wealth, there is the ever present fear of loss .:|:;            Duhkha Samudaya / Duhkhasya Karana - Cause of suffering       Everything has a cause, and every event depends upon its cause and conditions. Therefore, there must be a cause of suffering which can be obtained.            Duhkha Nirodha - The cessation of suffering       Once the cause is removed, the effect of suffering can be removed as well. His ideas were similar to Heraclitus, with regards to everything changing.            Duhkha Nirodhagamini Pratipat - Duhkha Nirodhasya Marga i.e. paths leading to the cessation of suffering       An eight-fold path was given by Buddha to cease suffering, called Arya Astanga Marg       Arya Astanga Marg - The Eight Fold path      Right faith (Samyag Drsti)   Right resolve (Sankalpa)   Right speech (Vak)   Right action (Karmanta)   Right living (Ajiya)   Right effort (Vyayama)   Right thought (Smrti)   Right concentration (Samadhi)   Pratityasamutpada - Theory of dependent origination      “Causality is always self-changing or becoming. The essence of a thing, its dharma, is the immanent law of relation.”    This theory suggests that everything is rooted to a singular cause, following from the law of causation. This is inherently related to the second noble truth, Duhkha Samudaya.   From the point of view of relativity, it is called Samsara. However, from the point of view of reality, it is called Nirvana.   Buddha revels that everything is relative, conditional and subject to birth and death, and therefore impermanent. Buddha identifies this as his dharma, it poses the middle path Madhyama Pratipat between the two extremes, reality and nothingness.   Nagarjuna says that Pratityasamutpada leads to cessation of plurality and bliss.   Dharma Chakra - The twelve links   You can start from any end, and move in a cycle to get the cause-effect relations.      Old age and death   Birth or rebirth (Jati)   Will to be born (Bhava)   Clinging to enjoyment (Upadana)   Thirst for sense of enjoyment (Trsna)   Sense - Experiences (Vedana)   Sense-Object contact (Sparsha)   Six sense organs including mind (Sadayatana)   Psycho-Physical organism (Nama rupa)   Initial consciousness of embryo (Vijnana)   Impressions of Cosmic forces (Samskarana)   Ignorance (Avidya)   Ignorance can be destroyed by knowledge. Right knowledge helps in understanding the causal power of each link (Artha Kriya - Karitva).   Since all beings are constituting, liberation is challenging and needs to be systematically worked out. Bhava Nirodha is the cessation of all activities.   Hinayana and Mahayana   Buddhism is a religion without God, karma takes the place of God. Liberation is the state of Arhathood, an ideal saint who obtains personal salvation - Nibbana.   Hinayana considers liberation to be negative in nature. Mahayana on the other hand, considers liberation to be a positive state of bliss, and not a negative cessation of misery.   Bodhisattva   Buddha is considered a God from a religious point of view. His divinity is worshiped, and is believed to have transcended reality by possessing the power of renunciation. An absolute self running through the individual selves.   Bodhisattava is an ideal saint who defers his own salvation in order to work for the salvation of others. He is inspired by the love for all beings and is ready to suffer gladly.   Hinayana - Theory of Momentariness   This theory is called Ksanabhangavada. Everything is momentary with ceaseless flow - Sanghatavada - becoming is real.   The theory of aggregates discusses how the stream of becoming is conceptualized, and is the aggregate of Five Skandhas. (Pancha Skandhas)   The Pancha Skandhas say that the soul is a bundle of:      Rupa - Matter   Vedana - Feeling   Samjana - Perception   Samskara - Dispositions   Vijnana - Consciousness   This theory states that change is universal and liberation is the extinction of all desires and passions.      Sarvam Duhkham - Everything is sorrow   Sarvam Anatma  - Everything is devoid of self   Sarvam Ksanikam - Everything is momentary   There are a few western philosophers whose ideas aligned with Buddha’s:      Heraclitus - change is real - “You cannot bathe twice in same river”   David Hume - Bundle theory of self - “I can never catch myself”   William James - “Passing thought itself is the thinker”   Bergson - “Everything is the manifestation of the flow of E’ Lam Vital”  ","url": "http://localhost:4000/notes/hs301/indian"
    },{
      "title": "Inflation",
      "excerpt":"Quantity Theory of Money   Let the price level according to CPI be $\\text{P}$ in an economy. This means that a standard basket of goods and services can be bought using an amount $\\text{P}$. From this, we can say that the value of an individual unit of money in terms of the standard basket is $1/\\text{P}$. This is the value of money in this economy.   We draw a demand-supply graph. The supply is the money supplied by the central bank, and the demand is the liquid cash that people wish to have. The demand depends significantly on the price level, as people would need more money the higher prices are. X-axis is the quantity of money supplied, and left Y-axis is the value of money in the economy.   From this, we can see that inflation occurs when the quantity of money in the economy is increased. That is, when people have larger quantities of money, they will offer more than competitors to get services first.   Classical Dichotomy   Every quantity is divided into two types, real and nominal. Real variables measure quantities and physical stuff whereas nominal variables measure made-up stuff like money. For example, price of a CD and nominal GDP are nominal variables and number of cds is a real variable.   Relative prices are real variables as well. “One copy of Titanfall is worth fifty CoD copies” is an example of relative prices. In the long run, price changes affect only nominal variables leaving real variables almost unchanged. This is called as the Monetary Neutrality of real variables.   Quantity Equation   We define the velocity of money in an economy to be the number of times it changes hands on average over a given period of time. It is calculated as the ratio of nominal GDP with money supply. Let the price index of the economy (GDP Deflator) be $\\text{P}$, real GDP be $\\text{R}$, the velocity be $\\text{V}$ and the money supply be $\\text{M}$. Then: \\(\\begin{align*} \\text{V} &amp;= \\frac{P\\times R}{M}\\\\ \\implies \\text{V}\\times\\text{M} &amp;= \\text{P}\\times\\text{R} \\end{align*}\\) The below equation is called the quantity equation. It has been observed that velocity is nearly constant when money supply changes, and it is clear that money supply cannot change the value of real GDP (real variable). Therefore, an increase in money supply is cancelled by an increase in price index, leading to inflation.   Inflation Tax:  The “tax” levied on every person with money (as its value has dropped) due to the government increasing its supply is called as the inflation tax. Seigniorage is the income that is generated by printing money.   Fischer Effect   \\[\\text{Nominal Interest} = \\text{Real Interest} + \\text{Inflation}\\]  From monetary neutrality, we can say that inflation and nominal interest are closely tied together.   Costs of Inflation      Inflation Fallacy: The idea that inflation erodes the income which people have. Real income doesn’t depend on inflation, although nominal income does.   Shoeleather Costs: During high inflation, we would want to exchange money for goods and services as quickly as possible; or store it in banks. The costs incurred for this effort come under this category.   Menu Costs: The costs for changing menus frequently during periods of high price instability.   Taxes: Taxes are based upon nominal incomes, and inflation causes more percentage of the real income to be given away than usual.   Hyper-Inflation   Inflation of over $1\\%$ per day. When the govt doesn’t have enough money to pay debts, and buyers do not have confidence in its bonds (due to socio-economic reasons); the govt has no option other than seigniorage to get the money required.  ","url": "http://localhost:4000/notes/hs101/mic7"
    },{
      "title": "Indian Philosophy",
      "excerpt":" ","url": "http://localhost:4000/notes/hs301/indian_2"
    },{
      "title": "Aggregate Demand and Supply",
      "excerpt":"Recession: Periods of mild economic decrease in incomes and increase in unemployment   Depression: Periods of strong economic decrease in incomes and increase in unemployment   Economic Fluctuations   1. Economic Fluctuations are irregular and unpredictable   The fluctuations in the economy are called as the Business Cycle. This is a misleading term because the fluctuations are very irregular and are also very difficult to predict.   2. Most Macro Economic variables vary together   However, the amount of change in each variable depends on the variable under consideration. That is, even if real GDP and Income decrease during a depression, they would decrease by (obviously) different amounts.   3. As output falls, the rate of unemployment rises   Obviously, when there is less to produce, companies lay off workers.   Model of Aggregate Supply and Aggregate Demand   In the short run, the assumptions of classical dichotomy and monetary neutrality do not hold. We cannot separate our analysis of real and nominal variables. We build a new model to see how they interact with each other. Similar to a demand supply curve, we draw a graph between the price level on the y-axis and the real GDP on the x-axis. The aggregate demand is the amount of goods and services that people and companies wish to consume at a given price level and the aggregate supply is how much goods and supplies can be produced at a given price level.   Do keep in mind that the curves have been drawn assuming that all other factors such as money supply are fixed.   Why is Aggregate Demand Sloping downwards?   \\[\\text{Y} = \\text{C}+\\text{I}+\\text{G}+\\text{NX}\\]  Assume that $\\text{G}$ is fixed by a government policy. We discuss the effect of decrease in Price Level $\\text{P}$ for the three cases.      Wealth Effect: $\\text{P}$ decreases, causing consumers to become more richer by which they would tend to buy more goods.   Interest Rate Effect: Decrease in $\\text{P}$ would cause a decrease in interest rates, incentivizing people to borrow money and invest.   Exchange Rate Effect: Decrease in $\\text{P}$ causes the currency to depreciate, meaning that prices have fallen on global scale for the economy which would cause other countries to buy from our economy, increasing net exports.   The aggregate demand curve can shift due to the following reasons:      Change in Consumption: When the government increases taxes, it discourages people to spend and thus the curve shifts to the left. Similarly, when people become obsessed with savings, the curve shifts to the left as well.   Change in Investment: Investment Tax Credits, promising new technology and a increase in money supply shift the curve to the right.   Change in Government Spending: Governments having new projects and stuff shifts curve to the right.   Change in Net Exports: self-explanatory   Long-run Aggregate Supply Curve   The long run aggregate supply curve is a vertical line because the theory of classical dichotomy and monetary neutrality hold in this case. The value of the supply in the long run is called the Natural Level of Output. The long run supply curve can shift due to variety of reasons. (733/734 of Mankiw)   Stagflation: A period in which both production falls (stagnation) and the prices rise (inflation). It can be caused when the aggregate supply curve shifts to the left, due a sudden increase in costs of production and the effect is self-reinforcing.  ","url": "http://localhost:4000/notes/hs101/mic8"
    },{
      "title": "Fiscal Policies on Aggregate Demand",
      "excerpt":"Monetary Policy: The policies regarding the money supply and interest rate set by the central bank.   Fiscal Policy: The policies regarding taxation and stuff, set by the president and the Vice-President.   Theory of Liquidity Preference   The contribution of Wealth Effect and Exchange rate effect is small when compared with the interest rate effect for the downward slope of the aggregate demand curve. This theory explains how fiscal policies affect the interest rate affect and the curve. This is for a short-term, and we thus assume the rate of inflation to be constant. We draw a graph between interest rates on the y-axis and the money supply on the x-axis. The Supply curve in this case would be a vertical line, equal to the value fixed by the central bank. The Demand curve would be the quantity of money demanded for holding by the people as money is the most liquid asset.   As interest rate rises, the demand falls because the cost for holding the money increases. Equilibrium is achieved when people want to hold as much money that the central bank has supplied. When the interest is above or below the eq. value, it settles back due to there being lower and higher demand respectively.   We can now explain why an increase in price level increases rate of interest. $\\text{P}$ increases, causing the demand of money to increase, shifting the curve to the right and thus causing the interest rate to rise.   When money supply increases, this causes the interest rates to fall. This causes the aggregate demand curve to shift to the right, increasing production. (Assuming that the price level remains the same) the fk?   Zero Lower Bound   The monetary policy essentially reduces the interest rates directly or indirectly (by increasing money supply) to stimulate economic activity. However, what can the central bank to when the interest rate becomes as low as it can possibly go?      Forward Guidance: Keep interest rates low for as long as possible to stimulate economic activity.   Quantitative Easing:  Instead of just buying short-term government bonds, the central bank can buy long-term government bonds and mortgage backed securities as well.   Inflate the economy, so that the value of nominal interest is larger when compared to the real interest so that the value of the lower bound decreases.   Fiscal Policies and Aggregate Demand   Fiscal policies affect the aggregate demand directly. Suppose that a government orders the development of aircraft from a company for 20 Million. The demand curve shifts to the right, but the shift value need not be equal to 20 Million.   Multiplier Effect   An increase in the sales by the company would cause the employees to receive more wages and thus, indirectly increase the consumer spending. This increment can also cause the wages of the store workers to increase in the second wave. This reinforced increment is called the investment accelerator.   The fraction of excess money spent by the households is called Marginal Propensity to Consume (MPC). Therefore, if the money spent by the government is $\\text{G}$, the max increase in the aggregate demand $\\Delta\\text{AG}$ is:   \\[\\Delta\\text{AG}_ \\text{max} = \\frac{\\text{G}}{1-\\text{MPC}}\\]  Note that the multiplier effect can work against the economy as well. Suppose the case where NX falls, the decrease in aggregate demand would by larger by the same factor. Also, the effect need not be only for G, it can be for any case where aggregate demand changes.   Crowding out effect   An increase in government spending increases the interest rate, which in turn decreases investment incentive causing a decrease in the value by which the aggregate demand increases.  ","url": "http://localhost:4000/notes/hs101/mic9"
    },{
      "title": "Context Free Languages",
      "excerpt":"        This is a methodology for representing languages. It is more powerful than DFAs and RE, but is still not enough to define all the possible languages. The basic idea is to use “variables” as stand-ins for sets of strings.   Formally, the terms described below define a CFG.   \\[G = (V,\\Sigma, R,S)\\]     Terminals ($V$)- The symbols of the alphabet   Variables / Non terminals ($\\Sigma$) - A finite set of other symbols   Start Symbol ($S$)- The variable whose language is being defined   Production ($R$)- A rule of the form “$\\text{variable} \\rightarrow \\text{variables} ∪ \\text{terminals}$“   “Productions of A” refers to the set of all strings that are derivable from $A$ using the rules defined by the CFG. A string $\\gamma$ is said to be derived from $\\alpha$ iff the application of a rule results in $\\gamma$. This is represented as $\\alpha\\implies\\gamma$.   Derivation using 0 or more steps is represented by $\\overset{*}{\\implies}$.   Any string derived from the start string is called as a Sentential Form. It essentially is a string “on its way” to becoming part of the language.   A language defined by a CFG is called as a Context Free Language.   BNF Notation (Backus-Naur-Form)   This is a method of representing the rules of CFG in a programming way.                  CFG Part       BNF Representation       Example                       Variable       &lt;word&gt;       &lt;state&gt;                 Terminals       multicharacter strings in bold or underlined       while, while, while                 $\\rightarrow$ in rule definitions       $::=$       &lt;state&gt; ::= a                 “or” in rule definitions       $\\vert$       &lt;state&gt; ::= a$\\vert$b                 (not) Kleene Closure ($a^+$)       “…”       &lt;state&gt; ::= &lt;\\state&gt;…                 Optional Elements       […]       &lt;state&gt; ::= &lt;statea&gt; [or &lt;stateb&gt;]                 Grouping $(ab)^*$       {…}       &lt;state&gt; :== {&lt;statea&gt;&lt;stateb&gt;}…           Leftmost and Rightmost derivations   A problem with CFGs is that every word may have more than one derivations possible. This makes verification and confirmation difficult. A derivation is said to be “Leftmost” when only the leftmost non-terminal is operated on in every step of the derivation. The same definition works for rightmost derivations as well.       Parse Trees   Trees whose nodes are labeled by the symbols of a particular CFG. The leaves are labeled by terminals or $\\epsilon$, Interior nodes by variables and the root by the start symbol.   The yield of a parse tree is given by concatenating the leaves in a left-to-right order. (The order of preorder traversal)         For every parse tree, there is a unique leftmost derivation. Conversely, for every leftmost derivation, there is a unique parse tree with the same yield.    The forward part can be proven using strong induction on the height of parse tree, and the backwards part can be proven using strong induction on the length of the derivation. Also, it can be seen from the proof that obtaining a parse tree doesn’t depend on the derivation being “leftmost”.      For any derivation, there exists a unique parse tree with the similar yield.    Ambiguous Grammar and LL(1) Grammar   Leftmost and Rightmost derivations had been introduced to alleviate confusions caused by multiple derivations. However, it is not guaranteed that a unique leftmost (or rightmost) derivation exists for a given string.   For example, consider grammar with the following rule. Two parse trees exist for the string “()()()”   \\[S\\rightarrow SS\\vert (S)\\vert ()\\]     From the theorems discussed earlier, it can be concluded that two leftmost and rightmost derivations would exist for this grammar. This is thus called Ambiguous Grammar.   Note that ambiguity is a property of the grammar, and not the language itself. That is, we can write equivalent unambiguous grammar for the above example.   \\[\\begin{align*} B \\rightarrow &amp;(RB\\vert\\epsilon \\\\ R \\rightarrow &amp;)\\vert (RR \\end{align*}\\]  Grammar such as the above example, where we can figure out the production to use in a leftmost derivation by looking at the string left-to-right and looking at the next one symbol is called $LL(1)$ grammar. Leftmost derivation, Left-to-right scan, 1 symbol of lookahead.   $LL(1)$ grammars are never ambiguous.   Note that some languages are inherently ambiguous, meaning that there does not exist an unambiguous grammar that accepts the CFL’s language. For example, ${0^i1^j2^k\\vert i=j\\text{ or }j=k}$ is inherently ambiguous. This can be understood intuitively by looking at $0^n1^n2^n$, which could have been obtained from either the first check or the second check.       Eliminating Useless Symbols   Useless symbols are of two kinds:      Symbols which do not derive a terminal string   Symbols which are not reachable from the start   Nullable Symbols   Unit productions   We shall look at methods for elimination of such symbols.   Eliminating Symbols which do not derive a terminal string   Consider the following set of rules:   \\[\\begin{align*} S&amp;\\rightarrow AB \\\\ A&amp;\\rightarrow aA\\vert a\\\\ B&amp;\\rightarrow AB \\end{align*}\\]  It can be said that $S$ derives nothing and the language is empty. The algorithm for deducing this is quite simple and is based on induction.   Algorithm   Basis - If there is a production $A\\rightarrow w$ where $w$ is composed of terminals, then $A$ derives a terminal string   Induction - If $A\\rightarrow \\alpha$ and $\\alpha$ consists only of terminals and variables known to derive terminals, then $A$ derives a terminal string.   For a given set of rules, we perform the above induction and mark all the variables which generate a terminal string. We can then remove all the rules which include a variable which has not been marked.      Eliminating Unreachable Symbols   The symbols which are unreachable from the starting symbol are said to be unreachable. We can remove all rules which involve such strings as they are not relevant. The algorithm for removing unreachable symbols is similar to the algorithm for eliminating the symbols that do not derive a terminal string;      Mark the start state, and iterate by marking all the variables that are derivable by the start state   Rules involving unmarked variables can be eliminated   Epsilon Productions   A production of the form $A\\rightarrow \\epsilon$ is said to be an $\\epsilon$-production.      If L is a CFL, then $L-{\\epsilon}$ has a CFG with no $\\epsilon$-productions.    A variable $A$ is said to be nullable when it can derive $\\epsilon$ in a finite number of steps. That is, $A\\overset{*}{\\implies}\\epsilon$. Eliminating nullable variables uses the same algorithm discussed earlier. Variables of form $N\\rightarrow \\epsilon$ are marked, and induction is performed. A variable $B\\rightarrow\\alpha$ is nullable iff all variables in $\\alpha$ are nullable as well.   However, we cannot simply remove all rules which the nullable symbols are involved in. The following steps have to be followed:      Remove all direct $\\epsilon$ transitions ($N\\rightarrow \\epsilon$)   Replace nullable variables on RHS of a rule with two cases, one unchanged and one with it being removed   Example:      Unit Productions   A production whose right side consists of exactly one variable is called a unit production. This is redundant and can be eliminated. That is, if $A\\overset{*}{\\implies}B$ by a set of unit productions and $B\\rightarrow\\alpha$ is a non-unit production; we can drop all the unit productions and just add $A\\rightarrow\\alpha$.       Chomsky Normal Form      Theorem     If $L$ is a CFL, there exists a CFG for $L-{\\epsilon}$ that has:          No useless symbols     No $\\epsilon$ productions     No unit productions      A CFG is said to be in CNF iff every production in the grammar is one of these two forms:      $A\\rightarrow BC$ (RHS is two variables)   $A\\rightarrow a$ (RHS is a terminal)      Theorem     If $L$ is a CFL, there exists a CNF for $L-{\\epsilon}$    Converting CFG to CNF      “Clean” the grammar by removing the previously mentioned four kinds of “useless” variables   For each of the rules where the RHS isn’t a terminal, convert the RHS to be all variables. For example, $A\\rightarrow Ab$ can be written as $A\\rightarrow AA_b, A_b\\rightarrow b$.   In right sides of length larger than 2, write them as a “chain” of rules. That is, consider $A\\rightarrow BCD$. This can be rewritten as $A\\rightarrow B E, E\\rightarrow CD$.       Push Down Automata   PDA is defined by a 7-sized tuple:   \\[(Q, \\Sigma, \\Gamma, \\delta, q_0, Z_0, F)\\]                 Symbol       Meaning                       $Q$       States                 $\\Sigma$       Input alphabet                 $\\Gamma$       Stack alphabet                 $\\delta$       Transition function                 $q_0$       Start state                 $Z_0$       Start symbol                 $F$       Set of final states           The transition function takes three inputs; NFA state, input alphabet and the top of the stack and it outputs the resulting NFA state and the action to be performed on the stack. The actions available on the stack are either pushing multiple stack variables onto the stack, or popping the top of the stack.   Instantaneous Descriptions (ID) of a stack are given by the triplet mentioned above. Note that the stack variables are written in a top-first manner (top of the stack is at the left and the bottom is at the right).   $ID_1 \\vdash ID_2$ is read as “ID1 goes to ID2”, and means that one transition rule has been applied. This can be extended to define $\\vdash^*$ to mean application of zero or more transition rules.       Pumping Lemma for CFL   The statement of the pumping lemma is as follows.      For every context-free language $L$ there exists an integer $n$, such that for every string $z$ in $L$ such that $\\vert z \\vert\\geq n$ it can be written as $z = uvwxy$ such that:          $\\vert vwx \\vert \\leq n$     $vx&gt;0$     For all $i\\geq 0, uv^iwx^iy\\in L$      This extended pumping lemma can be used to prove that $L= { 0^i10^i10^i\\vert i\\geq 1 }$ is not CFL in nature.   Proof   Let $n$ be the corresponding constant for $L$. Consider the string $0^n10^n10^n$ which belongs to $L$. There are two cases for $v,x$.      $v,x$ contain no $0$s. This implies that they contain at least one $1$, implying that $uv^iwx^iy$ has more than two $1$s and thus is not in the language.   $v,x$ contains at least one $0$. Because $\\vert vwx\\vert \\leq n$, it is too short to extend to all the three blocks, and is localized to the middle block. Therefore, $uv^iwx^iy$ would not have equal number of $0$s in all blocks and is thus not in the language as well.   Shown that $0^n10^n10^n$ is cannot satisfy Pumping Lemma in mutually exclusive and exhaustive cases. Therefore, $L$ is not a Context-Free Language and cannot be represented by a CFG.       Decision Properties of CFL   The same drill as what we had seen previously with regular languages. However, there exist many questions that were decidable for regular languages but are not for CFLs.      Are two CFLs the same?     Are two CFLs disjoint?    The above questions are decidable for regular languages, but there exists no algorithm in the case of context free languages.   Testing Emptiness   This is easy enough, we just eliminate all the variables which do not create a terminal string. If the start symbol is one such variable then the CFL is empty.   Testing Membership - CYK Algorithm   CYK algorithm runs in $\\mathcal{O}(n^3)$ time, and uses dynamic programming, where $n=\\vert w\\vert$. We assume that the provided grammar is in CNF form. The working of the algorithm is given below.      Construct a $n\\times n$ triangular array, called $X$. $X_{ij} = {A\\vert A\\overset*\\implies w_iw_{i+1}\\ldots w_j }$ That is, it contains the set of all variables that derive this substring.   The basis would be filling up all $X_{ii}$ with the variables that can generate that letter.   Induction: $X_{ij}={A\\vert \\exists A\\rightarrow BC, k\\text{ where } i\\leq k&lt;j\\text{ such that } B\\text{ is in }X_{ik}\\text{ and }C\\text{ is in}X_{k+1,j}}$   After induction, check if $S\\in X_{1n}$      Testing Infiniteness   This is quite similar to the idea for regular languages. If there exists a string in $L$ of length in between $n$ and $2n-1$, then the language is infinite; otherwise not.       Closure Properties of CFL   Note that CFLs are closed under union, concatenation and the Kleene closure. Moreover, they are closed under reversal, homomorphism and inverse homomorphisms.   However, CFLs are NOT closed under intersection, difference and complement!   ","url": "http://localhost:4000/notes/cs310/cfl"
    },{
      "title": "CS213",
      "excerpt":"  Important Notes      In if statements; if(k&gt;0) is preferred over if(k) because k&gt;0 returns a bool while k doesn’t, thus depending on the definition of if   Passing Big Vectors or Arrays as parameters of a function would cause a lot of time loss, because copying the huge data takes place; Especially during Recursion. Using Global Arrays is better!   In creating header files, don’t do using namespace std; because the person using the header may or may not want to use that namespace   \\n is faster than endl when needing to output a large number of lines. However, use endl at the end of the output to flush the output and refresh the buffer.   C++ Libraries     Vectors  - Some Functions to be known         Running Lecture Notes    Lecture on 10th Sept           Addition is not a Basic Operation! Addition requires changing all the bits in worst case scenario; making it O(logn) on average. This can be made even faster by using parallel computing by predicting the carry of the bit, while simultaneously adding.       (How did we compute runtime as O(logn) here – Number of bits is log(n) where n is the input)       Given string S find all s1 and s2 such that interweaving both gives S   Break this down to first finding the number of distinct subsequences for s1 and s2.           If there is no repetition; we can write num_dist_subseq = 2^n.            For a case with rep, define N[i] = number of distinct sub seq from A[0] to A[i]; where A is the original string. we can define a recursive relation as A[i] = 2*A[i-1] + A[j-1].        Lecture on 14th Sept   Lab Solution for previous Thursday           The Problem is reduced to: \\(F(A {\\displaystyle \\cup} \\{n-1\\}) = F(A) + F(A {\\displaystyle \\cup} \\{n-1\\})\\)            The second part of the RHS is what caused you trouble; but what you didn’t realize was that SHIFTING THE f-Array by (2^n) unites to the left! Therefore, you solve this problem in a Divide and Conquer method.            What you were trying to do was overly complicated; you were trying to go from F(0) to F(A) by using Dynamic Programming and stuff.            Part 2; suppose all the values given were F. Reverse the steps done above, meaning you would do: \\(f(A_i) = F(A_i) - F(A - A_i)\\) And Dynamic programming can be implemented here to make the code even more faster.         Lecture on 15th Sept   213 Assignment Discussion      Avoid using namespace std; because the user who includes your header file may not want that namespace. To use cin and cout; do std::cin and std::cout.   to_array() must NOT return a pointer to the internal Array. You must make a deep copy of the Array, and return a pointer to that  array. This is what you did u idiot   const1 operator*(const2 permutation perm) const3{}            const2:- perm is a constant; i.e.; in a*b, b is a constant.       const3:- in a*b, a is a constant. This won’t work for overloading = because in a=b, a must not be a constant.       const1:- The returned permutation is a constant.             Lecture on 17th Sept      We mostly use Worst-Case analysis for checking the growth rate of an Algorithm. Average-Case makes sense in some cases, like the QuickSort Algo. These algorithms usually have a Randomization incorporated, such as the Pivot in the QuickSort.   For a Randomized Algo, the Running time is a Random Variable. We therefore, represent the runtime of the algorithm for a given input size n, as the Expected Value of the Random Variable. This is usually misleading, because the real runtime might vary and be more than the expected value, but usually it performs near the Expected Runtime.   Hindsight, you didn’t check the algo in today’s lab and submitted the O(n^2) algorithm     Lecture on 21st Sept   Previous Lab Solution      endl flushed the output a single value at a time. Therefore, use \\n when needed to print output on different lines, in the for loop; but DON’T FORGET endl at the end to flush the output completely! Read abt it here.   For the second part, we’re gonna be building f_i from prev known. We can find the greatest to the left (say j) quickly, and that will be the  limit to which we’d need to search. For k, j&lt;k&lt;=i to be a starting point; it must be the smallest among that proper subset.   What we do here is, we build a vector. (Write this logic down in OneNote, its really good.)     Lecture on 22nd Sept   Interweaving of Strings      try something… Didn’t pay attention here   Previous day’s Quiz        ","url": "http://localhost:4000/notes/cs213cn/"
    },{
      "title": "CS251 - Python",
      "excerpt":"Introduction     Python is an interpreted language; meanwhile C++ is a compiled Language.   Compiling: Check Syntax -&gt; Semantic Analysis -&gt; Optimizations -&gt; Convert optimized to x86-64 program. This x86-64 program can be run as many times as needed. Semantic Analysis is Static in nature.   Interpretation: Syntax Analysis -&gt; ByteCode Generator -&gt; ByteCode Optimization. This ByteCode runs on a Virtual Machine Simulator which runs (gets Interpreted) on your laptop. The Semantic Analysis in this case is dynamic, meaning that it occurs when the ByteCode is being executed.   ByteCode Analysis     import dis is the module to be imported; and to view the ByteCode of a function, do dis.dis(&lt;function_name&gt;).   The output of the corresponding code should be in the format of 5 columns.                  Python Line       &gt;&gt;       Functions being done       ???       Variable name                       Points to the line in the original Python File to which the stack corresponds to       Present means that a pointer (of sorts) is stored to the particular line. Done when a “jump” to the line is needed to be done; usually for for, while or else cases.       All the stuff that the Virtual Machine does at the corresponding line, in sequence.       Address of the variable or smth?       The corresponding variable name in the Python File, or the value of the corresponding constant in the Python File.             To run a python script from the Python Shell; do exec(open('&lt;YourFile&gt;.py').read()). After the script has been executed; you can print any variables in the environment without adding print() statements and running it again.   This might help in debugging the code faster.     Operator Hacks      x = n // 2 does integer division of n and 2. That means this is similar to int(n/2).   Usage of Basic Functions      print(var, end = '&lt;ending char&gt;') – Print n and the character in end after it. By default; end = \\n.   input(&lt;string&gt;) – Prints &lt;string&gt; and waits for input in the same line.  ","url": "http://localhost:4000/notes/cs251py/"
    },{
      "title": "Macroeconomics",
      "excerpt":"There are only two parties in macro economics - Households and Firms.   Microeconomics and Macroeconomics   Microeconomics focuses on the how money is managed by households and firms individually; while macro focuses on how they interact with one another in the market.   Ten Principles of Economy   10 Principles of Economics, marginal changes and rational people, social costs   Principle-1 People face trade-offs   To get what we want, we need to give up on something. This is called as a trade-off. A very prominent example of this is Efficiency V Equity.   Efficiency means that society greedily gets the most that it can from the scarce resources. Equity is when the benefits of the resources are distributed among all the members of the society fairly.   Principle-2  Cost of an item is what you give up to get it   Opportunity cost of an item is what you have given up to obtain it.   Principle-3 Rational People think at the margin   Marginal Changes are small incremental changes that affect the existing plan of action. People make decisions by comparing costs and benefits at the margin.   Principle-4 People respond to Incentives   As an extension to #3, having incentives motivates people to consider a certain product over another for the same opportunity cost.   Principle-5 Trade makes everyone better off   People can gain the goods and capital required from trading. it also allows for specialization in a certain area to better help out society.   Principle-6 Markets are usually a good way to organize Economic Activity   A market economy is a decentralized economy which allocates resources via the decisions of many firms and households. Adam Smith has made an observation that firms and households act as if they are guided by an “Invisible Hand”, which in reality is just the rational parties trying to maximize their profit.   Principle-7   Governments can sometimes improve Market Outcomes   Property rights are important for functioning of free market economies. They are the ability of an individual to own and exercise control over a resource.   Markets can work only when property rights are properly enforced. When society/political situation in a country is unstable, upholding property rights is difficult. In such cases, the government can intervene. India has a mixed market, where the railway market is controlled purely by the government and other markets such as telecom are controlled by firms.   Market Failure is the inefficient distribution of goods and services by the free market. It can be caused by:      Poor property rights   Externality - Impact of a firm or a person on the well-being of a by-stander. These are not taken into account in the free market, needing the intervention of the government.   Market Power - The ability of a person or firm to unduly influence market price. That is, if a firm has a monopoly on a raw material in the country, government intervention is needed to make sure that they don’t take advantage of this in such a way that damages the country.   Economic Models   Similar to all sciences, we have specific terminologies in this field. We use a few justified assumptions to create an Economic Model, and try to reason out useful data from this model which (hopefully) reflects real-life data pretty well. We do this to bring the attention toward a problem in the society.   Model1 - Circular Flow Diagram   This is a visual economic model which shows how money flows through the market amongst households and firms.   Factors of Production      Land - Where the factory is being put up   Labor - Workers   Capital - The money required for all operations   Entrepreneurship - Group of people to lead the firm   All factors of production are controlled by the households. There are two types of markets in such a case, a market for factors of production(M1) and a market for goods and services(M2). Firms buy in M1 and sell in M2; households buy in M2 and sell in M1.      Model2 - Production Possibilities Frontier   Not including monetary considerations here, it is a physical feasibility exercise.   This is a graph which shows the maximum possible production number of various resources given the constraint of the available factors of production (other than capital). The graph is called as Production Possibilities Frontier. A country operating on the PPF is using the factors of production optimally. Being inside the curve indicates that the production isn’t efficient.      Efficiency   Trade-offs   Opportunity Cost   Economic Growth   A shift in the PPF is observed when an innovation in the production is done to reduce wastage of material or new source of raw material is obtained.   Statement Classes   Positive Statements or Descriptive Statements describe the factual statements.   Normative Statements or Prescriptive Statements describe an opinion about the world, need not be factual.   More about Markets   Perfect Competition - A market where there are many buyers and sellers selling a homogenous product so that each person has a negligible impact on the market price and no bargaining power. Buyers and sellers are price takers, not price makers. Example: Fruits market and such   Oligopoly - Few sellers are present in the market, so the sellers have considerable market power when compared to the buyers. Sellers have luxury of differentiating their product. Example: Telecom Market   Monopoly - Only one seller, where the seller is a price maker not a price taker   Monopolistic Competition - There are many sellers with slight differentiation of the product, with each of them setting a price for their product. Very different from a monopoly, but does have some of its characteristics.   Demand and Supply   Law of Demand   The quantity demanded of a good falls when the price of the good rises. Quantity demanded is the amount of a good that buyers are willing and able to purchase. Not applicable every time, for example, the trend is opposite for paintings.   Demand Schedule - A table which shows the quantity of good demanded against the price of the good.   Demand Curve - Its just the demand schedule being graphed, with price on the Y-AXIS and Quantity demanded on the X-AXIS.   Market Demand - The horizontal summation of the individual demands of all the buyers of a product in the market would be the market demand of that product.   The Law of Demand considers that the price of the product is the only variable in play. A change in these factors shifts the demand curve. The constant factors that are assumed by it are:      Income - As income increases, the demand for an Inferior good decreases and the demand for a normal good increases.   Prices of related goods - Substitutes are the goods when fall in price of one good results in a fall in price of the other good. Complements are the products whose prices are inversely related.   Tastes and preferences   Expectations of the Consumers   Number of Buyers   Law of Supply   All things equal, the quantity supplied rises when the price of the good rises. There would be an upper limit on how much the quantity can rise to. Similar to demand, we have a Supply Schedule and the Supply Curve. Also, we assume that the other parameters are constant here.   The parameters of the Law of Supply are:      Input Prices   Technology   Expectations   Number of Sellers   Law of Supply and Demand   The claim that the price of any good will finally settle at the Equilibrium Price at which the supply of the good equals the quantity demanded. The Equilibrium price would vary depending upon the current changes in the parameters that we’ve assumed to be constant initially.   Here, the Taste and Preference parameter of the customer changed according to the weather, causing a  shift in the demand curve which in turn changes the Equilibrium Price.   Elasticity is the measure of how much buyers and sellers respond to changes in market conditions. It is relative, and is represented in percentage terms.   Price Elasticity in Demand   \\[\\text{Price Elasticity of Demand} = \\frac{\\% \\text{ change in demand}}{\\% \\text{ change in price}} = \\frac{(Q_2-Q_1)/[(Q_1+Q_2)/2]}{(P_2-P_1)/[(P_1+P_2)/2]}\\]  \\[x \\text{ Elasticity of }y = \\frac{\\%\\text{ change in }y\\text{ wrt midpoint}}{\\%\\text{ change in }x\\text{ wrt midpoint}}\\]  Notice that the percentages are calculated wrt the mean of the values and not the second value. There are a few factors which affect how elastic the demand of a quantity is:      Availability of Substitutes - If substitutes are absent, then the product tends to be inelastic.   Necessities vs Luxuries - Inelastic and Elastic respectively.   Definition of Market - (Read tb)   Time Horizon - More elastic when there is a longer time horizon.   We say that the demand is elastic if the absolute value of price elasticity in demand is greater than 1. Conversely, if the absolute value of price elasticity in demand is between 0 and 1, then we say that it is inelastic. Equal to 1 == Unitary Elastic. Similarly, if it is 0 we call it as Perfectly Inelastic and when it is infinite we call it to be Perfectly Elastic.   A straight line will not have the same elasticity throughout! This is because we have additional factors in the calculations other than just $\\Delta Q/\\Delta P$.   Income Elasticity of Demand   \\[\\text{Income Elasticity of Demand} = \\frac{\\% \\text{ change in demand}}{\\% \\text{ change in income}} = \\frac{(Q_2-Q_1)/[(Q_1+Q_2)/2]}{(I_2-I_1)/[(I_1+I_2)/2]}\\]  It can be seen that an increase in the price for a product that is Inelastic in Demand would result in the money obtained increasing. However, increasing the money for a product which is Elastic in demand causes the money obtained to decrease. Ideally, we would like to be at the point where the good is Unitary Elastic.   Controls on Prices   Are usually enacted when policy makers believe the optimal functioning of the free market is being unfair to a single party.   Price Ceiling - The legal maximum on the price that the good can be sold for. Example - MRP   Price Floor - The legal minimum on the price that the good can be sold for. Example - Minimum Wage Law   The Price ceiling is binding if it is imposed to be lesser than the equilibrium price. This causes a shortage of that particular good in the market. Similarly setting the price floor above the equilibrium price causes an excess of goods to be present in the market.   Taxes   Tax Incidence - The manner in which the tax burden is borne by the two parties.   Taxes cause a shift in the equilibrium, the profit made by the buyers and the sellers is reduced. Taxing the sellers causes the supply curve to shift upwards, and taxing the buyers causes the demand curve to shift downwards by the size of the tax in both the cases.   With respect to the demand curve, levying a tax on the sellers and the buyers has the same equilibrium price. The burden of the tax is shared by both the parties, and the share of burden depends on the slope of the curves. The inelastic party usually bears most of the taxation.   Costs of Production   Modern Microeconomics is all about Supply, Demand and Market Equilibrium.   Total Revenue - The amount of money received for selling the goods   Total Cost - The total cost of production, this includes all the opportunity costs that are needed for the production of goods and services. That is, both Explicit and Implicit costs need to be considered. For example, for an entrepreneurship, the explicit costs include the money of and stuff wheras the implicit cost is the money that could’ve been obtained by having a corporate job. \\(\\begin{align*} \\text{Economic Profit} &amp;= \\text{Total Revenue} - \\text{Total Costs} \\\\ \\text{Accounting Profit} &amp;= \\text{Total Revenue} - \\text{Explicit Costs} \\\\ \\end{align*}\\)   Production Function - A relationship between the amount of inputs needed to make a good and the amount of said good produced.   Marginal Product - The increment in the amount of good produced when a particular input is increased by one unit. Diminishing Marginal Productivity refers to the tapering of the marginal product when the input increases.   The production function is represented graphically as a Total Cost Curve which plotted between the cost (y-axis) and the quantity of output (x-axis). Diminishing Marginal Product leads to a sharp increment in the slope.   We shall introduce a few terms to make talking about costs more efficient.   Terminologies      Total Costs (TC), Total Fixed Costs (TFC) and Total Variable Costs (TVC)   Average Total Costs (ATC), Average Fixed Costs (AFC), Average Variable Costs (AVC)   \\[AC = TC/(Quantity)\\]     Marginal Costs (MC) - The increment in Total Cost needed to increase the quantity of a good produced by one unit. This tends to increase at higher costs due to diminishing marginal product. ATC is rising when MC is greater than ATC. Similarly, ATC falls when MC is less than ATC. MC crosses ATC at the Efficient Scale, where the costs are minimized. Also note that MC crosses AVC, AFC at their lowest points. \\(\\text{MC} = \\frac{\\Delta \\text{TC}}{\\Delta \\text{Q}_\\text{output}}\\)   Short and Long Run   The fixed costs in short run become variable in the long run, because everything can be changed as need be in the long run. The long run average total cost has to be always lesser then or equal to the short run average total cost.   Economies of Scale   Economies of scale - The Long Run ATC of the good falls as the quantity produced is increased.   Constant returns to scale - Long Run ATC is constant with respect to the quantity of goods produced.   Diseconomies of scale - Long Run ATC increases with the quantity of good produced   Perfectly Competitive Markets   It has many buyers and sellers selling identical products so that both buyers and sellers are price takers. Firms can freely enter and exit the market. In such a condition, the Average Revenue (AR), and the Marginal Revenue (MR) are both equal to the price of the product. \\(\\begin{align} \\text{Marginal Revenue} &amp;= \\frac{\\Delta(Total Revenue)}{\\Delta(Quantity)} = \\frac{\\Delta TR}{\\Delta Q} = P \\\\ \\text{Marginal Cost} &amp;= \\frac{\\Delta (Total Cost)}{\\Delta (Quantity)} = \\frac{\\Delta TC}{\\Delta Q}  \\\\ \\end{align}\\)   \\(\\text{Profit is maximized when Marginal Revenue equals Marginal Cost (above the ATC)!}\\) That is, if the price falls below the AVC in the short term, it is better to just produce zero output (Shutdown), as a loss will be incurred. Shutdown is a short term decision to halt production for a while, wheras Exit is the long term production stoppage. In the long run however, we exit when the price falls below the ATC.   The portion of the Marginal Cost curve above AVC in short run is called the firm’s Short run supply curve. Similarly in the long run, the portion of the marginal cost curve above ATC is the firm’s long run supply curve. The supply curves of the market and the firm remain the same, albeit with a factor difference in the quantity, because we assume that all firms are rational. The factor would be the number of firms present in the market.   In the long run, price equals the minimum of the Average Total Cost, i.e. , at the Efficient Scale. This can be understood by the law of supply and demand. assume that the price is lower than the minimum of ATC, this would cause firms to exit the market and shift supply curve leftwards, which increases the demand and price. If it is higher than ATC, many firms come in to make a profit, and summarily shift supply curve towards the right, decreasing the cost to min of ATC.   In the long run, an increase in demand only increases the quantity supplied, leaving the price unaffected.   Monopoly   A good is being produced by a single firm without any close substitutes. Monopolies fundamentally arise because of a barrier to entry in the market. The examples for such barriers are:      Ownership of a key resource. This is rarely how monopolies arise, in practice.   Only legal producer of the good, approved by the government. Patenting and copyright laws are two important ways in which this is done.   Costs of production make a single producer much efficient than others in the market. That is, as the number of consumers increases, the cost decreases (Economies of Scale!). For such a setting, Monopoly is optimal to reduce the costs of production. Such monopolies are called as Natural Monopolies, for obvious reasons.   The revenue calculations for monopoly would be: \\(\\begin{align*} \t(Price) \\times (Quantity) &amp;= (Total Revenue) \\\\     (Average Revenue) &amp;= (Total Revenue)/(Quantity) = (Price) \\\\     (Marginal Revenue) &amp;= \\Delta(Total Revenue)/\\Delta (Quantity) \\end{align*}\\) Unlike perfect competition, Marginal revenue is not equal to the Price but it is variable. The maximization condition is $\\text{Marginal Revenue equals Marginal Cost.}$ In a monopoly, the $Marginal Revenue$ is always lesser than the Price, and this can be seen because the demand curve for the firm is sloping downwards. At profit maximization, the Price would be greater than the marginal cost as well, as MR=MC.   Price Discrimination   This is the practice of selling the exact same good at different prices to different customers, even though the cost of production is the same. Examples being Pawn shops and Wholesale/Retailers. This is not possible if goods are being sold in a perfect competition, as the sellers have no buying power.   Perfect Price Discrimination   This refers to the situation when the monopolist knows each consumer personally and charges everyone differently based upon their willingness to buy products. It can be seen clearly that this is just theoretical in nature, and is done to increase the profits of the firm.   Welfare Economics   The study of how allocation of resources affects economic well being. The equilibrium in a market maximizes the welfare of both the buyers and the sellers.   Surplus      Consumer Surplus = (Amount willing to pay) - (Amount actually paid)   Producer Surplus = (Amount received for goods) - (Amount willing to sell)   Graphically, consumer surplus is the area below the demand curve and the price line. Similarly, producer surplus is the area above the supply curve and the price line.   Total Surplus = ConsumerSurplus + ProducerSurplus = (Value to Buyers) - (Cost to Sellers)   Efficiency is the way of resource allocation to maximize total surplus. In the case of a free market, the equilibrium price and quantity are the most efficient combination to maximize total surplus. Therefore, the social planner can leave the market alone. This is referred to as Laissez Faire in French.   This is present only in perfectly competitive markets though; in a monopoly the distribution of price is not efficient causing there to be deadweight losses (Triangle between monopoly quantity, demand curve and MC curve). This inefficient allocation may also be a cause of externalities.   Theory of Consumer Choice   This falls under the theory of constraint optimization, because consumers have a limited access to resources when compared to firms. This is called as the Budget Constraint of the consumer. This can be represented using linear equations. Note that the consumer in this discussion is a Price Taker, that is, the price of the good is unchanged.   For example, assume that Good1 is 10 dollars and Good2 is 2 dollars. Let the income of the consumer be 100, then the linear equation would be: \\(10\\cdot G_1 +2\\cdot G_2 = 100\\) Theoretically, we should be having a $\\leq$ in the above equation, because savings might be present. However, we are assuming that the problem is static in nature. That is, we assume that this is a one-period problem where saving money is irrelevant in the given time frame.   The slope of the Budget Constraint graph gives the rate at which one good can be exchanged for another. This is referred to as the relative price of the good.   We show the consumer’s preferences via an Indifference Curve. This is a family of curves. This curve plots the quantity of two goods which give the same amount of happiness. Mathematically, it is the locus for which happiness = k. The slope of the Indifference curve is called as the Marginal Rate of Substitution for the given pair of goods.      Higher Indifference curves are preferred to lower ones   Indifference curves are downward sloping   Indifference curves do not cross   Indifference curves are bowed inward   Perfect Substitutes - The Indifference curve is a straight line. That is, the marginal rate of substitution is a constant.   Perfect Complements - Two goods with right angle indifferent curves. These are used together, so having an excess of one adds no satisfaction.   Maximizing Happiness   Maximizing the happiness of a consumer requires the slope of the indifference curve to be equal to the slope of the budget constraint curve. That is, the Marginal rate of substitution should be equal to the relative price. At this optimal point, the person’s valuation equals the market valuation of the good. We shall now see how a change in factors cause the curves to change.      An Increase in income - The budget constraint line shifts outwards. We can say if a good is normal or inferior based on how the quantity bought changes.   A Change in Price - The equation of the budget constraint curve is $P_1x + P_2y=I$, therefore we can see that the curve changes.   Substitution Effect   A product becoming cheaper causes people to buy more of it, even if they move along the same indifference curve.   Income Effect   A product becoming cheaper will cause people to buy more of it, causing people to move to a higher or lower indifference curve. If its a normal good, more of the product is bought. Inferior goods will be bought less.   Giffen Goods   These goods are inferior in nature with an upward sloping demand curve. That is, the income effect dominates over substitution effect, in violation of the law of demand. For such goods, the demand rises as the price increases. (inferior is a misnomer for Giffen goods!)  ","url": "http://localhost:4000/notes/hs101/mac/"
    },{
      "title": "Deep Learning and CNNs",
      "excerpt":"       These are the notes made by me while I was trying to implement this paper. A rigorous mathematical approach is not followed (mainly to streamline the process of note making), rather, I have noted down the concepts and the intuition behind the concepts. Mathematical analysis of the topics covered can be found here.   Use the navigation ui on the left to browse through my notes. The results of various netowrks constructed are summarized below.   Classification Networks’ Structure  Three networks based on the structure of AlexNet, VGGNet and ResNet have been constructed for the MNIST dataset. Their architecture, and graphs comparing their loss with epochs are shown below.                                                                                                                                Graphs for AlexNet, VGG-Net and ResNet respectively                                                                                              Modified AlexNet and VGG-Net Architectures       ResNet’s architecture is identical to the one described in the above linked paper.   ResNet has been implemented below. You can press the button to generate two random images and classify them in real time.   (This might take upwards of 30 seconds because heroku app would need to boot up. I only did this because I thought that executing a script in a section called “Executive Summary” was funny for some reason. That’s an entire weekend that I am never getting back.)       Classify!      Generative Adversial Networks’ Architecture   One GAN has been implemented for the MNIST dataset so far. The architectures of the generator and the discriminator are displayed below, along with the “Loss vs Epochs” graph obtained during the training. Do notice that this GAN needed to be trained for much longer than any of the above classification networks.      Again, you can click on the button below to obtain a random output of the network. Not all the generated images resemble a digit, but most of them do.       Generate!     ","url": "http://localhost:4000/notes/dl/intro"
    },{
      "title": "Database and Information Systems",
      "excerpt":"       ","url": "http://localhost:4000/notes/cs317/intro"
    },{
      "title": "Foundations of Intelligent and Learning Agents",
      "excerpt":"      The notes of CS747 have been divided weekly, and they can be found below; or accessed via the sidebar.      Week1   Week2   Week3   Week4   Week5   No lectures for Week6 owing to Midsems      Week7  ","url": "http://localhost:4000/notes/cs747"
    },{
      "title": "Numerical Analysis",
      "excerpt":"       \\(e = \\lim_{n\\to\\infty}(1+1/n)^n\\) The value of $e$ is approximated using the infinitely differentiable exponential function $\\exp(x)$ and the Taylor’s Theorem. This series can be used to obtain the value to a required degree of precision. Note that $c\\in (x,a)$. \\(f(x) = f(a)+f'(a)(x-a) +\\cdots+\\frac{f^k(a)}{k!}(x-a)^k + \\frac{f^{k+1}(c)}{(k+1)!}(x-a)^{k+1}\\) The final term when approximating the value of $e$ would be: \\(\\frac{e^c}{(n+1)!}\\) We know that $e^c&lt;3$ when $c\\in (0,1)$. We can thus obtain the value of $e$ to a required degree of error by choosing an appropriate value of $n$. We can similarly compute the value of $e^a$ to any required degree of accuracy.   ","url": "http://localhost:4000/notes/ma214/lec1"
    },{
      "title": "Finite Digit Arithmetic",
      "excerpt":"        Computers do not have memory to store infinitely many digits. This can lead to errors if not taken care of properly. For example, $\\sqrt{3}$ is irrational and thus cannot be stored in a computer exactly. For most cases, we chose a rational number whose square is not exactly equal to 3 but is reasonably close to 3 to pass off without any problem. This may lead to Round Off Errors.   Representing Real Numbers   64 bits are used to represent a number.      The first bit is a sign indicator, denoted using $s$   The next 11 bit exponent is called the characteristic $c$   The remaining 52 bit fraction $f$ is called the mantissa   The final value of the exponent is given by $(c-1023)$ to ensure that negative exponents are allowed as well. \\(\\text{Number }= (-1)^s2^{c-1023}(1+f)\\) $m$ has 52 bits, meaning that the precision of this method of representation is 16 digits.      The smallest positive number that can be represented by this notation would be given by $(s,c,f) = (0,1,0)$. The number itself would be $2^{-1022} \\approx 0.22251\\times 10^{-307}$. Note that both $(0,0,0)$ and $(1,0,0)$ correspond to $0$. Numbers smaller than this result in underflow.   Similarly, the largest number would be $2^{1023}\\cdot(2-2^{-52})$. Numbers larger than this result in overflow.       Floating Point Representation   We will use numbers of form \\(\\pm 0.d_1d_2\\ldots d_k \\times 10^n \\qquad 1\\leq d_1\\leq9, 0\\leq d_i\\leq 9\\) Converting a number $y$ which has more than $k$ decimal digits can be done in two ways;      Chopping, wherein the additional digits are simply dropped   Rounding, where we add $5\\times10^{n-k-1}$ and then drop the additional digits.   Let $\\rho$ be the real number and $\\rho^*$ be the approximation.                  Absolute Error       Relative Error                       $\\vert \\rho - \\rho^* \\vert$       $\\frac{\\vert \\rho-\\rho^* \\vert}{\\rho}$               Significant Digits   We say $\\rho^*$ approximates $\\rho$ to $t$ significant digits if $t$ is the largest non-negative integer for which \\(\\frac{\\vert \\rho-\\rho^* \\vert}{\\rho} &lt; 5\\times 10^{-t}\\)   Finite Digit Arithmetic                  Operation       Meaning                       $x\\oplus y$       $fl(fl(x)+fl(y))$                 $x \\ominus y$       $fl(fl(x)-fl(y))$                 $x \\otimes y$       $fl(fl(x)\\times fl(y))$                 $x (\\div) y$       $fl(fl(x)\\div fl(y))$           ","url": "http://localhost:4000/notes/ma214/lec2"
    },{
      "title": "Finite Digit Arithmetic",
      "excerpt":"       ","url": "http://localhost:4000/notes/ma214/lec3"
    },{
      "title": "Image Denoising with MRF Prior",
      "excerpt":"        We shall only concern ourselves with noise models which are independent of the image’s distribution, and that the pixel intensities are IID in nature.   Let the random variable $X$ model the observed intensity at a pixel/voxel. The SNR (Signal to Noise Ratio) is given by:   \\[\\text{SNR} = \\frac{E[X]}{SD[X]}\\]  Gaussian Distribution is the most common noise model in medical imaging. This is because of the Central Limit Theorem. Also, the thermal noise caused by thermal agitation happens to be similar to gaussian distribution. Brownian motion adds gaussian noise when dealing with liquids.   Poisson Distribution is used as the noise model when measurement is made by accumulation of photons (photography, x-rays, CT, light microscopes)   \\[P(X=k;\\lambda) = \\frac{\\lambda \\exp(-\\lambda)}{k!}\\qquad E[X]=Var[X]=\\lambda\\]  Compound Poisson is a sum of Poisson distributions, and can be approximated with gaussian distributions.   \\[Y = \\Sigma_iX_i \\text{ where } P(X_i)=Poisson(X_i,\\lambda_i)\\]  Rician (Rice)   MRI data is 2 dimensional $[a,b]$ with the noise in each component being zero-mean additive gaussian. Distribution over measured vector $x$ is Rician and:   \\[P(x\\vert \\nu,\\sigma) = \\frac{x}{\\sigma^2}\\exp(-\\frac{x^2+y^2}{2\\sigma^2})I_o\\left(\\frac{x\\nu}{\\sigma^2}\\right)\\]  $\\nu$ is the magnitude of the uncorrupted 2-vector signal.   $\\sigma$ is the standard deviation of the additive gaussian noise.   $I_o$ is the Modified Bessel function of First Kind with order 0   SNR increases with the addition of IID random variables. Although we get more data from taking more observations, this increase in SNR means that there is an inherent loss as well. Bayesian techniques are required for this reason.   \\[\\begin{align*} E[X_1+X_2] &amp;= 2E[X] \\\\ Var[X_1 + X_2] &amp;= 2 Var[X] \\\\ SD[X_1 + X_2] &amp;= \\sqrt{2} \\times SD[X] \\\\ SNR[X_1+X_2] &amp;= \\sqrt{2} \\times SNR[X] \\\\ \\end{align*}\\]  Using the Baye’s rule over this problem, we get:   \\[P(\\text{noiseless image}\\vert\\text{noisy image}) = \\frac{P(\\text{noisy image}\\vert \\text{noiseless image})\\cdot P(\\text{noiseless image})}{P(\\text{noisy image})}\\]  Let the distribution of the noiseless image be denoted by $X$, which is an MRF. Similarly, let the observed noisy image be $Y$. For additive gaussian noise with zero mean;   \\[P(Y\\vert X) = \\prod_iP(Y_i\\vert X_i) = \\prod_i G(y_i\\vert x_i,\\sigma^2)\\]  That is, $Y_i=X_i+G(0,\\sigma^2)$. However, noise models need not always be additive in nature, and independent of the actual pixel intensity. For example, Poisson and Rician noise models’ perturbations depends on the intensity of the pixel. Generally, $\\theta$ is used to represent all the parameters of the underlying noise model.   Once the noise model is known, we can try to apply a transformation on the observed data to make the variance of noise be independent of the underlying signal strength. For example, Anscombe transform $f(x) = 2\\sqrt{X+3/8}$ changes a Poisson random variable with mean $m$ to an approximately gaussian random variable with variance 1. This method is not preferred by us, however.       Optimization Algorithms   Assumptions      MRF parameters are user controlled   Noise level is already known (previous scans of blanks, background parts of image)   We shall first look at optimizing the intensity at pixel $i$ assuming that all the other pixel intensities are optimized already. We can then iterate this over all pixels, and make multiple passes over the image. We can stop when the change in image isn’t significant, such as the relative change in RMS being less then a threshold.   The posterior probability is now given by $P(X_i,X_{\\sim i}\\vert y,\\theta)$. It can be simplified as follows:      We would like to maximize the posterior probability, $\\max_{x_i}P(X\\vert Y,\\theta)$.      Here, the first term is the likelihood function or the noise model. The second function is the local conditional prior on $X_i$. This algorithm seeks the mode of local conditional posterior. This is why this algorithm is called Iterated Conditional Mode or ICM for short.   Passing over the image sequentially my introduce artifacts in the output image which is not ideal. We may choose to iterate over the image with a random sequence each time but the success of this work around with reducing artifacts is not great.   To solve this, we can try to update the local posteriors in parallel akin to a gradient ascent algorithm. However, increasing the local posteriors need not increase the overall posterior unless we monitor an objective function as well.   ==We shall be primarily using the gradient ascent solution==   ICM is usually applied after gradient ascent has stagnated to try and make the output even better.       Circularly Symmetric Gaussian Noise Model   The ICM optimization algorithm for this noise model would lead to the following equation:   \\[\\begin{align*} \\max_{x_i} P(x\\vert y,\\theta) &amp;= \\max_{x_i} P(y_i\\vert x_i,\\theta)P(x_i\\vert x_{N_i},\\theta) \\\\ &amp;= \\max_{x_i}\\left( \\log P(y_i\\vert x_i,\\theta) + \\log P(x_i\\vert x_{N_i},\\theta)  \\right)\\\\ &amp;= \\max_{x_i}\\left( -\\frac{(y_i-x_i)^2}{2\\sigma^2}- \\sum_{a\\in A_i}V_a(x_a) \\right)\\\\ &amp;= \\min_{x_i}\\left(\\frac{(y_i-x_i)^2}{\\sigma^2}+ \\sum_{a\\in A_i}V_a(x_a) \\right)\\\\ \\end{align*}\\]  Recall that $A_i$ is the set of all cliques which contains $x_i$ as a member. The first term is called the Fidelity term and the second term is called the Regularity term. Fidelity penalizes deviation of denoised image from data, and regularity penalizes irregularity of the denoised image. Note that this problem can be optimized using ANY 1D optimization algorithm, not just gradient descent.   For gradient based optimization using parallel updates, the derivative would be:   \\[P_i =  \\frac{\\partial}{\\partial x_i}\\text{log-likelihood} = \\frac{2}{\\sigma^2}(x_i-y_i) + \\frac{\\partial}{\\partial x_i}\\sum_{a\\in A_i}V_a(x_a)\\]  The $i$‘th element of the gradient vector would be given by $P_i$. Therefore, the update rule for gradient descent using parallel updates would be as follows where $\\tau$ is the step size.   \\[x^{n+1} = x^n - \\tau g(x)\\]  Similarly, the derivative and update step for Rician distribution would be as follows:      Speckle Noise model for Ultrasound   The Speckle noise distribution is given as follows;   \\[Y = X + Z\\sqrt{X}\\]  Where $Z\\sim \\mathcal{N}(0,\\sigma^2)$. The update rules can be derived as follows:       Re-weighing Fidelity and Regularity   The prior distribution is unknown. That is, we may not know the distribution itself OR we may know the model but not the terms associated with it. We may need to tune these unknown parameters to obtain better results.   There may be further algorithms down the pipeline which require an image to be more/less smooth. We would want to retune the parameters then as well.         That is, we essentially multiply the Regularity term with $\\beta$ and replace $\\sigma^2\\rightarrow\\sigma^2/\\alpha$ where $\\alpha+\\beta=1$.  ","url": "http://localhost:4000/notes/cs736/Lec4"
    },{
      "title": "Supervised and Unsupervised Learning",
      "excerpt":"        Supervised Learning is when a goal is achieved by learning from training data which contains true labels. Examples include linear regression and classification.   Unsupervised Learning is when objects similar to each other are grouped together. Examples include clustering and dimensionality reduction. The desired output is unobserved in the training data.   There are three canonical learning settings:     Regression - Supervised   Estimate parameters, such as least square fit      Classification - Supervised   Given parameters about an object, assign a label to it      Unsupervised Learning   Clustering, and dimensionality reduction are prominent examples       Supervised Learning   Formally, let $\\mathcal{X}$ be the input space and $\\mathcal{Y}$ be the output space. We would like to obtain a function $f$ belonging to the function family $\\mathcal{F}$ such that $y_i \\approx f(x_i)$, where $(x_i, y_i) \\in \\mathcal{X} \\times \\mathcal{Y}$.   In linear regression, $\\mathcal{F}$ is the Linear Function Space.   It is not guaranteed that the training data is error-prone. We would like the final estimator to be robust to errors, and one way to do this is Data Cleansing (pre-processing).   Error Function   The error function $\\mathcal{E}$ takes the curve and data as input and yields a real number as the output. This is used to quantitatively judge whether a function is a “good fit” for the given data.   Some examples of $\\mathcal{E}$ are $\\sum \\vert f(x_i)-y_i\\vert$ and $\\sum (f(x_i)-y_i)^2$. We would ideally want the error to always be positive (so that positive and negative errors doesn’t cancel out).   Using the error function $\\sum (f(x_i)-y_i)^2$ is known as the  Method of Least Squares, or Ordinary Least Squares (OLS).  ","url": "http://localhost:4000/notes/cs337/Lec1"
    },{
      "title": "Kernel Perceptrons",
      "excerpt":"        The update rule for a perceptron has been discussed earlier. It is given by:   \\[\\begin{align}   w^{k+1} &amp;= w^k + \\eta y'\\phi(x') \\\\   \\implies f^{k+1}(x) &amp;= sign\\left(f^k(x) + y'\\phi^\\text{T}(x')\\phi(x)\\right) \\\\   \\implies f^{k+1}(x) &amp;= sign\\left(f^0(x) + \\sum_{x',y'} y'\\phi^\\text{T}(x')\\phi(x) count(x')\\right)\\\\ \\end{align}\\]  $count(x)$ is the number of times that $x’$ has been misclassified so far. Notice that $\\phi^\\text{T}(x’)\\phi(x)$ is a dot-product, which is akin to a measure of similarity between them both.   We can redefine this to obtain a non-linear update function, as given below!   \\[f(x) = sign\\left( b + \\sum_i y_i \\alpha_i K(x_i, x) \\right)\\]  $\\alpha$ is a vector initialized to 0s, and corresponding element is incremented by 1 if it is misclassified. (It stores $count$)   $K(x_i, x)$ replaces $\\phi^\\text{T}(x’)\\phi(x)$, and it is the “relation” between the given two datapoints.   Some examples of kernels are:      Linear: $K(x,y) = x^\\text{T}y$   Polynomial: $K(x,y) = \\left(1+x^\\text{T}y\\right)^2$   Radial Basis: $K(x,y) = exp(\\vert\\vert x-y \\vert\\vert^2_2 / 2\\sigma^2)$   This is used to find non-linear partitions between classes. Kernels operate on an implicit space, and are usually easier to compute than the linear separator in higher dimensional space.       Gram Matrices   For a given dataset ${ x_1, \\ldots x_m }$, the Gram Kernel Matrix $\\mathcal{K}$ is defined as follows:   \\[\\mathcal{K} = \\begin{bmatrix}   K(x_1, x_1) &amp; K(x_1, x_2) &amp; \\ldots &amp; K(x_1, x_m)\\\\   K(x_2, x_1) &amp; K(x_2, x_2) &amp; \\ldots &amp; K(x_2, x_m)\\\\   \\vdots &amp; \\vdots &amp; &amp; \\vdots\\\\   K(x_m, x_1) &amp; K(x_m, x_2) &amp; \\ldots &amp; K(x_m, x_m)\\\\ \\end{bmatrix}\\]  Given a Gram matrix, the attribute vector can be obtained via Singular Value Decomposition. That is, we find a diagonal matrix $D$ and a matrix $U$ with $UU^T=I$ such that:   \\[K = UDU^T = (UD^{1/2})(UD^{1/2})^T = \\Phi\\Phi^T\\]  Therefore, for a Kernel matrix to be valid:     It needs to be symmetric   It needs to be Positive Semi Definite (for any $b\\in\\mathcal{R}^m$, $b^TKb\\geq 0$)   Mercer Kernel   A kernel which has the following property is said to be a Mercer Kernel. Do keep in mind that every mercer kernel is valid. (This follows from the mercer theorem showing that the matrix is positive definite, but the proof is not necessary for this course)   \\[\\int_x\\int_y K(x,y)g(x)g(y) dx dy \\geq 0 \\text{ for all square-integrable functions } g(x)\\]  Given two positive definite (mercer kernels) $K_1(x,y)$ and $K_2(x,y)$, it can be proven that:      $\\alpha K_1(x,y) + \\beta K_2(x,y)$ is Mercer as well for $\\alpha, \\beta \\geq 0$   $K_1(x,y)K_2(x,y)$ is mercer as well  ","url": "http://localhost:4000/notes/cs337/Lec10"
    },{
      "title": "Kernelized Logistic Regression",
      "excerpt":"        We know that Regularized logistic regression’s loss function is the cross entropy loss function with a regularization parameter. When the sigmoid activation function is used, we have already seen that the loss function becomes:   \\[E(w) = -\\frac{1}{m}\\left[ \\sum_i \\left(y_iW^\\text{T}\\phi_i - \\log(1+\\exp(W^\\text{T}\\phi_i))\\right) \\right] + \\frac{\\lambda}{2m}\\vert\\vert w\\vert\\vert^2_2\\]  Similar to the sigmoid activation, we define the activation function in logistic regression to be the following:   \\[\\sigma_w(x) = \\frac{1}{1+exp\\left[ -\\sum_j \\alpha_j K(x,x_j) \\right]}\\]  Using this function, the loss function in kernelized logistic regression with the regularization parameter turns out to be:   \\[E_D(\\alpha) = -\\left[ \\sum_i \\left( \\sum_j y^i K(x^i, x^j)\\alpha_j - \\frac{\\lambda}{2}\\alpha_i K(x^i, x^j)\\alpha_j \\right) - \\log\\left( 1+exp\\left[ -\\sum_j\\alpha_jK(x^i, x^j) \\right] \\right) \\right]\\]  ","url": "http://localhost:4000/notes/cs337/Lec11"
    },{
      "title": "Unsupervised Learning",
      "excerpt":"        The methods discussed so far, regression and classification were supervised in nature. This is because the $y_i$ is provided for every datapoint.   We shall now look at a few unsupervised learning methods, where only $x_i$ are provided to the model.       Dimensionality Reduction      “All other things being equal, pick the simplest solution” - Occam’s Razor    That is, we would like to find the “true dimensionality” of the given model. We like having fewer dimensions because:      Easier to learn   Solves overfitting   That is, we would like to convert $x\\in\\mathcal{R}^d$ to $z\\in\\mathcal{R}^k$ where $k\\text{ }«\\text{ }d$.   \\[z = U^T x\\]  $U$ is a $d\\times k$ Projection Matrix. We also want $U^TU = I_k$. Note that $UU^T$ need not be identity at all.   This implies that reconstructing a point from $z$ dimension is lossy, because the correlation is only approximate. That is, if $z = U^Tx$ and $\\tilde{x} = Uz$, then $\\tilde{x}$ need not be equal to $x$.       Principal Component Analysis   PCA-1 aims to minimize the squared loss error between the actual data and the reconstructed data. $U^TU = I_k$ must be satisfied.   \\[U = \\underset{U}{\\operatorname{argmin}} \\sum_x \\vert\\vert x - UU^Tx \\vert\\vert ^2\\]  Here is another way of approaching the same problem, dubbed PCA-2.   We assume that the datapoints are centered, with $\\mathcal{E}[x] = 0$. (Just shift the origin)   PCA-2 aims to maximize the variance of the data in the new hyperplane. Do note that $U^TU=I_k$ must hold here as well.   \\[U = \\underset{U}{\\operatorname{argmax}} \\mathcal{E}[\\vert\\vert U^Tx \\vert\\vert^2]\\]  This is related to the eigen value decomposition.The dimension $k$ is determined by the   It can be proven that both PCA-1 and PCA-2 are equivalent. Let $C =\\Phi\\Phi^T $ be the covariance matrix of the given data $\\Phi$. Note that $C$ is $n\\times n$, where $n$ is the number of datapoints given.   The vectors corresponding to the dimensional projection are the first $k$ Eigen Vectors of $C$ when arranged in descending order of their corresponding Eigen Value. The data needs to be then projected along these eigen vectors.       PCA with Kernels   Let the data matrix be given by $X$ ($\\Phi$). Assuming that $X$ is psd, the Singular value decomposition of $X$ can be given by $ X = ADB^T $. From this, we can say that:   \\[\\begin{align}   nC &amp;= XX^T &amp;= AD^2A^T \\\\   \\mathcal{K} &amp;= X^TX &amp;= BD^2B^T \\\\ \\end{align}\\]  Where $\\mathcal{K}$ is the kernel matrix. Therefore, we can see that the eigen vectors obtained after eigen value decomposition of the kernel matrix lead to the projection of the data on the principal components. (That is, we previously obtained the components and then projected the data along these components. In this method however, we get the projected data directly and we do not obtain the components themselves.)  ","url": "http://localhost:4000/notes/cs337/Lec12"
    },{
      "title": "Clustering",
      "excerpt":"        The  important parameters associated with clustering are given below.      Similarity Measure: $\\rho(d_1, d_2)$   Distance Measure: $\\delta(d_1, d_2)$   Number of clusters: $k$   In the same cluster, we would like $\\delta$ to be small and $\\rho$ to be large. This is called “Inter-Cluster”. Similarly, we would like large $\\delta$ and small $\\rho$ between two different clusters. (“Intra-Cluster”)   There are various methods of clustering the data. Some of these have been discussed below.   Bottom-Up Clustering   Simply put, the pair of points with the smallest distance between them are “merged” into a cluster at every iteration. A visual representation of this is called as a Dendogram.   Top-Down Clustering   We initialise $k$ arbitrary centroids, and iterate through the data and modify these centroids appropriately. We have a semblence of cluster representative in top-down, wheras bottom-up is just peer-to-peer in nature.   An idea is to perform bottom-up clustering until $k$ clusters are obtained, and using the means as the initial centroids for performing top-down clustering.       K-Means Algorithm   We will be dealing with the “hard” version of the K-Means algorithm. There are two main steps for this algorithm.      Keeping datapoints’ assignments the same, update the position of cluster center to the empirical mean.   Fix the cluster centers and assign datapoints to cluster with least euclidean distance.   The algorithm is terminated when the assignment of clusters to each datapoint is unchanged or when the cluster centers change by a very small value.       Kernel K-Means   The euclidean distance calculated in the normal K-Means algorithm is modified a little.   \\[\\begin{align}   d(x,y) &amp;= || \\phi(x) - \\phi(y) ||^2 \\\\   &amp;= ||\\phi(x)||^2 + ||\\phi(y)||^2 - 2\\phi(x)^T\\phi(y) \\\\   &amp;= K(x,x) + K(y,y) - 2K(x,y) \\\\ \\end{align}\\]  ","url": "http://localhost:4000/notes/cs337/Lec13"
    },{
      "title": "SVM",
      "excerpt":"        We’ve discussed the perceptron algorithm already. However, there are a few drawbacks for this method:           Does not find the best separating hyperplane       Soln: Large margin classification            Sigmoidal perceptron can separate only address linearly separable data       We shall tackle the first drawback now. The second drawback will be discussed in the next lecture.   Support Vector Machines   Provide breathing space to perceptron algorithm to have better separating planes.   \\[\\begin{align} w^T\\phi(x)+b \\geq +1 &amp; \\text{ for } y(x) = +1 \\\\ w^T\\phi(x)+b \\leq -1 &amp; \\text{ for } y(x) = -1 \\end{align}\\]  For such a case, the margin is given by $2/\\vert\\vert w\\vert\\vert$. The margin is the distance between both the displaced planes.   However, points need not be linearly separable, meaning the above equations are not guaranteed to hold. We thus add a new term called slackness represented by $\\xi$.   \\[\\begin{align} w^T\\phi(x_i)+b \\geq +1-\\xi_i &amp; \\text{ for } y(x_i) = +1 \\\\ w^T\\phi(x_i)+b \\leq -1+\\xi_i &amp; \\text{ for } y(x_i) = -1 \\\\ &amp; \\xi_i \\geq 0 \\\\ \\end{align}\\]  SVM tries to maximize the margin, and minimize $\\sum\\xi_i$.  ","url": "http://localhost:4000/notes/cs337/Lec14"
    },{
      "title": "Neural Networks",
      "excerpt":"        We’ll be tackling non-linear classification now. We’ve already discussed the usage of kernels for such classification. However, domain knowledge is required for selecting a proper kernel for the given situation, which limits the applications.   Neural networks, on the other hand, act as universal function approximators which do not require as much domain knowledge.   In general, a series of mappings are used to obtain the desired results.   \\[x\\xrightarrow{f}y\\xrightarrow{g}z\\xrightarrow{h}\\{c_1, \\ldots c_k\\}\\]      Activation Functions   \\[f(x) = g(w^Tx)\\]  Here, $g$ is called the Activation function. Some examples of activation functions are:      Sigmoid   Tanh   Linear   ReLU: $\\max(0,s)$   Softplus: $\\log(1+e^s)$, is the “differentiable version” of ReLU   The problem with sigmoid and tanh is that, their values are limited. ReLU and Softplus do not have this problem.       VC Dimensions   The cardinality of the largest set of points that $f(w)$ can shatter is its VC Dimension. A function is said to shatter a given set of points if, for all assignments of labels to those points there exists a $w$ such that $f_w$ perfectly evaluates the set.   The VC Dimensions of a linear separator in $\\mathcal{R}^2$ is 3.   The VC dimensions of a threshold classifier in $\\mathcal{R}$ is 1.       Designing Neural Networks   Neural networks have great expressive power owing to:      Non linearity of the activation functions   Cascaded non-linear activation functions   There are four main design choices that are to be considered while coding up a neural network. These are:      Input Layer   Number of Hidden layers and number of nodes per hidden layer   Output layer   Loss function   Do note that the activations at the hidden layers are not visible, even during training.  ","url": "http://localhost:4000/notes/cs337/Lec15"
    },{
      "title": "Neural Networks",
      "excerpt":"        Adding hidden layers to the neural network causes the objective function to become non-convex. Further more, since the activations of the hidden layers are unobserved, we do not know what the function itself is!   Backpropogation Algorithm   This algorithm uses the chain rule and memoization for differentiating output layers wrt the internal layers.   Consider a network with one hidden layer with two neurons, whose activation functions are represented by $g_1$ and $g_2$, and an output layer with activation $f$. For inputs $x,y,z,w$ the output could be:   \\[output = f(w^2_1g_1(w^1_1x+w^1_2y)+w^2_2g_2(w^1_3z+w^1_4w))\\]  The derivative wrt $w^1_1$ can now be written as follows using the chain rule.   \\[\\frac{\\partial o}{\\partial w^1_1} = \\frac{\\partial f}{\\partial w^1_1} = \\frac{\\partial f}{\\partial g_1}\\frac{\\partial g_1}{\\partial w^1_1}\\]  This is where memoization kicks in, we could store the values of these partial derivatives for use later speeding up the computations.   We memoize partial derivatives, products and sums of products.   Notation   The output layer is denoted as the $L$’th layer, and the hidden layers behind it are $l$’th, $l-1$’th and so on. (right to left)   $\\sigma^l_i$ is the $i$’th neuron in the $l$’th hidden layer.   $w^l_{ij}$ is the weight connecting the $i$’th neuron in the $(l-1)$’th layer and $j$’th neuron in the $l$’th layer. (zero indexing)   $\\text{sum}^l_i$ is the input argument to $\\sigma^l_i$, given by $\\sum_t w^l_{ti} \\sigma_t^{l-1}$.   Therefore, the L2 regularized objective function to be minimized is:   \\[\\begin{align} E(w) &amp;= -\\frac{1}{m}\\left[ \\sum_i \\sum_k y_k^{(i)}\\log\\sigma^L_k(x^{(i)}) + (1-y_k^{(i)})\\log(1-\\sigma^L_k(x^{(i)}))\\right] \\\\ &amp;+ \\frac{\\lambda}{2m}\\sum_l\\sum_i\\sum_j (w^l_{ij})^2 \\\\ \\end{align}\\]  That is, the objective function only looks at the output layer, but the regularization parameter penalizes ALL weights!       Equations for backpropogation   \\[\\frac{\\partial E}{\\partial w^l_{ij}} = \\frac{\\partial E}{\\partial \\sigma_j^l}\\frac{\\partial \\sigma_j^l}{\\partial \\text{sum}^l_j}\\frac{\\partial \\text{sum}^l_j}{\\partial w^l_{ij}}\\]  The values of each individual partial derivative have been given below.   \\[\\begin{align} \\frac{\\partial E}{\\partial \\sigma_j^l} &amp;= \\sum_p \\frac{\\partial E}{\\partial \\sigma_p^{l+1}}\\frac{\\partial \\sigma_p^{l+1}}{\\partial sum_p^{l+1}}\\frac{\\partial sum_p^{l+1}}{\\partial \\sigma^l_{ij}} \\\\ \\frac{\\partial \\sigma_j^l}{\\partial \\text{sum}^l_j} &amp;= \\text{(just differentiate the activation function)} \\\\ \\frac{\\partial \\text{sum}^l_j}{\\partial w^l_{ij}} &amp;= \\sigma_i^{l-1} \\\\ \\end{align}\\]  Note that the rhs of the first equation would have already been stored in the previous iteration of the algorithm.  ","url": "http://localhost:4000/notes/cs337/Lec16"
    },{
      "title": "Overfitting in Neural Networks",
      "excerpt":"        One can represent any smooth function to any desired accuracy using a single hidden layer of sufficient size.   We can counter this using already learnt methods such as L1 and L2 regularization. However for neural networks, we can avoid overfitting using smart network design. More specifically, we shall look at usage of Convolutional Neural Networks for image related tasks.       Convolutional Neural Networks   filters, stride, padding   We would like the learn the weights for each of the filters. Convolution improves upon fully connected layers in the following ways:      sparse interactions   parameter sharing   equivariant representations, with $f(g(x)) = g(f(x))$ where $f$ is convolution and $g$ is shift function       Number of parameters   Let the given input given by $M\\times N\\times D$, upon which convolution is performed using a patch of size $P\\times Q$ with stride $S$ and number of channels $C$, and padding $D$.           Number of parameters   \\((P\\cdot Q\\cdot D)\\times C\\)            Number of Neurons \\(\\left(\\frac{M-P+2D}{S}\\right)\\left(\\frac{N-Q+2D}{S}\\right)(C)\\)       ","url": "http://localhost:4000/notes/cs337/Lec17"
    },{
      "title": "Convolutional Neural Networks",
      "excerpt":"        The “depth” is typically increased as the size of the image is shrunk by pooling and filters to prevent loss of data. This increment is caused by using multiple filters, which are learnt by training.   Ultimately, we perform softmax in the final layer to get probability of each class.   Pooling layer   Usually, $\\max$ is used for dimensionality reduction via down-sampling of the input. This reduces overfitting and translational invariance only sends important information to the next layer.   Padding is usually not done because it doesn’t change the value. The output size is given by:   \\[\\left(\\frac{M-P}{S}+1\\right)\\times\\left(\\frac{N-Q}{S}+1\\right)\\]  ","url": "http://localhost:4000/notes/cs337/Lec18"
    },{
      "title": "Linear Regression",
      "excerpt":"        Regression is about learning to predict a set of output (dependent) variables as a function of input (independent) variables.   Consider the inputs to be of form $&lt;x_i, y_i&gt;$. Attributes of $x$ are (non-linear) functions $\\phi$ which operate on $x$. The form of the equation that linear regression optimizes is:      $$ Y = \\sum_{i=1}^n w_i\\phi_i(x) + b = W^T\\Phi(x) + b $$   Where $\\Phi$ is a vector of all attributes, and $W$ of all weights.   Do note that $b$ can be dropped by defining $\\widetilde{w}, \\widetilde{\\Phi}$ with one additional element being $b$ and $1$ respectively.   Linear regression is linear in terms of weights and attributes, and (generally) non-linear in terms of $x$ owing to $\\Phi$.   For example, $\\phi_1$ could be the date of investment, $\\phi_2$ could be value of investment and so on.   There are general classes of basis functions, such as:     Radial Basis function   Wavelet function   Fourier Basis   Formal Notation   Dataset $\\mathcal{D} = &lt;x_1, y_1&gt; \\ldots &lt;x_m, y_m&gt;$   Attribute/basis functions $\\phi_i$, and the general class of basis $\\Phi$ is given as shown below. Do note that we have redefined the value of $\\Phi$ now, and we shall be using this definition from here on.      $$\\Phi =    \\begin{bmatrix}     \\phi_1(x_1) &amp; \\phi_2(x_1) &amp; \\ldots &amp; \\phi_p(x_1) \\\\     \\vdots      &amp;             &amp;        &amp;             \\\\     \\phi_1(x_m) &amp; \\phi_2(x_m) &amp; \\ldots &amp; \\phi_p(x_m) \\\\   \\end{bmatrix}$$   With the above redefinition, the linear equation for a given $W$ becomes $Y = \\Phi W$.   General regression would be to find $\\hat{f}$ such that;      $$\\hat{f} = \\min_{f\\in\\mathcal{F}} E(f, D)$$   Parameterized Regression is a bit more complex as it involves the optimization of weights in the above definition for a given $f(\\phi(x), w, b)$ for minimizing error.      $$ w^*, b^* = \\min_{w,b} \\left[ E(f(\\phi(x), w, b), D) \\right] $$   The error function determines the type of regression. Some examples are given below. These will be discussed later in the course.      Least Squares Regression   Ridge Regression   Logistic Regression   Least Square Solution   Formally, the solution is given by:      $$w^*, b^* = \\min_{w,b} \\sum_{j=1}^m \\left( \\left( \\sum_{i=1}^p w_i\\phi_i(x_j) + b - y_j \\right)^2 \\right) $$   If the “true” relation between $X$ and $Y$ was linear in nature, then 0 error is attainable. That is, $y = \\Phi W$ exists, or Y belongs to the column space of Phi. We can just solve linear equations to get the optimal value of $W$.   If $Y$ is not in the column space of $\\Phi$, the closed form solution for optimal weights $W^*$ is given by:      $$W^* = \\left(\\Phi^T\\Phi\\right)^{-1}\\Phi^TY$$   Do note that $\\Phi^T\\Phi$ is invertible iff it has full column rank. That is:     All columns are linearly independent of each other   The columns are not data driven   It can be proven that Gradient Descent converges to the same solution as well.  ","url": "http://localhost:4000/notes/cs337/Lec2"
    },{
      "title": "Probabilistic Linear Regression",
      "excerpt":"        We can model $Y$ to be a linear function as before, with an additional noise parameter $\\epsilon \\sim \\mathcal{N}(0,\\sigma^2)$. That is, the relation is      $$ Y = W^T\\Phi(x) + \\epsilon $$   Normal distribution has the maximum entropy amongst all distributions with a given variance. The $3-\\sigma$ rule also plays a vital role for picking this. That is, $68\\%$ of readings deviate less than $\\sigma$ from the mean, $95\\%$ less than $2\\sigma$ and $99.7\\%$ less than $3\\sigma$ from the mean.   The maximum likelihood estimate of $W$ is calculated in this case. (Remember that calculation is simplified by taking $\\log$ to get the Log-Likelihood)       Overfitting and Regularization   Increasing the degrees of freedom causes overfitting. The model essentially brute-forces all data points in training data, which isn’t very helpful. Overfitting is caused by $\\vert\\vert W_i\\vert\\vert$ being large, and regularization tries to suppress this.   Bayesian Linear Regression   The problem of overfitting is addressed by a prior. That is, the prior models $W$ by bounding the norm to be smaller than some $\\Theta$.   To understand this better, we shall first tackle a simpler coin-tossing example.   I have a newly minted coin which I believe to be fair. I now flip it four times and get 4 heads. The MLE would be 1, but this isn’t taking the prior belief into account. The posterior would be given by the baye’s rule.   The Prior is given by $P(H)$, and the Posterior is given by $P(H\\vert D)$. Therefore, the relation is given by:      $$P(H|D) = \\frac{P(D|H)\\times P(H)}{P(D)} \\propto P(D|H)\\times P(H)$$   We ignore the denominator as it is just a normalizing factor, and is constant as the data is known.   If $P(D\\vert H)$ follows distribution $d_1$ and the posterior and prior follow the same distribution $d_2$, then $d_2$ is said to be the conjugate prior of $d_1$.   Examples for distribution-conjugate prior pairs include:     Gaussian - Gaussian   Bernoulli &amp; Binomial - Beta   Categorical &amp; Multinomial - Dirchlet   Beta Distribution   Wikipedia page link: Beta distribution   In short, the beta distribution has two parameters $\\alpha, \\beta$. The value of the distribution always lies between 0 and 1. The pdf is given by:     $$\\begin{eqnarray}  \\text{Beta}(\\alpha, \\beta) &amp;= \\frac{x^{\\alpha-1}(1-x)^{\\beta-1}}{B(\\alpha, \\beta)}\\\\  B(\\alpha, \\beta) &amp;= \\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha+\\beta)}\\\\  \\end{eqnarray}$$      The advantage of conjugate distributions is that the posterior is always of the same family as the prior, with just the parameters changed. This makes calculations very easy.   In the above coin tossing example, we know that Bernoulli and Beta are conjuagtes. Therefore, we can model the prior of $p$ as a BETA distribution $\\text{Beta}(\\alpha, \\beta)$ for simplicity.   If there are $h$ heads occurring in $n$ tosses, then the posterior distribution turns out to be $\\text{Beta}(\\alpha+h, \\beta+n-h)$.   If we have no prior information regarding $p$, we can model it as a beta with both parameters equal to 1. (The beta is uniform in 0 to 1 when both parameters are 1, just what we intend to have)  ","url": "http://localhost:4000/notes/cs337/Lec3"
    },{
      "title": "Bayesian Linear Regression",
      "excerpt":"        As discussed earlier, we would like the norm of $W_i$ to be small. For the sake of simplicity, we limit ourselves to a univariate case.   Here, we just want the value of $W$ to be small. We can thus assume that the prior for $W$ is a gaussian distribution of 0-mean and variance $1/\\lambda$.   Recall that the product of two gaussians, $\\mathcal{N}(\\mu_1, \\sigma_1^2)$ and $\\mathcal{N}(\\mu_2, \\sigma_2^2)$ is also a gaussian distribution with the following parameters:      $$\\begin{eqnarray}     \\frac{1}{\\sigma^2} &amp;= \\frac{1}{\\sigma_1^2} + \\frac{1}{\\sigma_2^2} \\\\     \\frac{\\mu}{\\sigma^2} &amp;= \\frac{\\mu_1}{\\sigma_1^2} + \\frac{\\mu_2}{\\sigma_2^2} \\\\    \\end{eqnarray}$$   Consider the univariate case where a random variable $X$ is a gaussian $\\mathcal{N}(\\mu, \\sigma^2)$, and $\\sigma$ is known. We have prior distribution of $\\mu$ as well, $\\mathcal{N}(\\mu_o, \\sigma_o^2)$. Given $m$ datapoints of x, it can be quite easily seen that the posterior distribution of $\\mu$ would be $\\mathcal{N}(\\mu_m, \\sigma_m)$, where:      $$\\begin{eqnarray}     \\frac{1}{\\sigma_m^2} &amp;= \\frac{1}{\\sigma_o^2} + m\\frac{1}{\\sigma^2} \\\\     \\frac{\\mu_m}{\\sigma_m^2} &amp;= \\frac{\\mu_o}{\\sigma_o^2} + m\\frac{\\mu}{\\sigma^2} \\\\    \\end{eqnarray}$$   Extend to Multivariate Case   The above result can be extended to a multi-variate linear regression case as well. In the equation, $Y = W^T\\Phi + \\epsilon$, let the $\\epsilon$ distribution’s prior be given by $(\\mu_o, \\Sigma_o^{-1})$ where $\\Sigma_o$ is analogous to $\\sigma^2$. (It is the covariance matrix)   Similarly, data would be given by $(\\mu_{mle}, \\Sigma^{-1})$ (assuming that $\\Sigma$ is fixed like in univariate), and $m$ data points are available to us. The posterior distribution can be calculated to be $(\\mu_m, \\Sigma_m)$ where      $$\\begin{eqnarray}     \\Sigma_m^{-1} &amp;=  \\Sigma_o^{-1} + m\\Sigma^{-1} \\\\     \\mu_m\\Sigma_m^{-1} &amp;= \\mu_o\\Sigma_o^{-1} + m\\mu_{mle}\\Sigma^{-1} \\\\   \\end{eqnarray}$$   However, the value of $\\Sigma$ was arbitrarily introduced by us. We have been taking $\\epsilon\\sim\\mathcal{N}(0,\\sigma^2)$. We can use this and the closed form solution to the equation, $W = (\\Phi^T\\Phi)^{-1}\\Phi^TY$ to replace its value, to obtain the final solution given below.      $$\\begin{eqnarray}     \\Sigma_m^{-1} &amp;= \\Sigma_o^{-1} + (\\Phi^T\\Phi)/\\sigma^2 \\\\     \\mu_m\\Sigma_m^{-1} &amp;= \\mu_o\\Sigma_o^{-1} + (\\Phi^TY)/\\sigma^2 \\\\   \\end{eqnarray}$$   We had taken the prior on $W$ to be $\\mu_o = 0$ and $\\Sigma_o^{-1} = \\lambda I$. By substituting these values in the above equation, we get that      $$\\begin{eqnarray}     \\Sigma_m^{-1} &amp;= \\lambda I + \\Phi^T\\Phi/\\sigma^2 \\\\     \\mu_m &amp;= (\\lambda\\sigma^2 I + \\Phi^T\\Phi)^{-1}\\Phi^T Y   \\end{eqnarray}$$  ","url": "http://localhost:4000/notes/cs337/Lec4"
    },{
      "title": "MAP and Bayes Estimates",
      "excerpt":"        The posterior distribution’s mean and variance have been calculated in the previous lecture. Because the posterior is a gaussian, we can clearly understand that      $$ \\hat{w}_{MAP} = \\hat{w}_{Bayes} = \\mu_m $$   MAP is the $w$ where a global maxima is attained, and Bayes is the expected value of $w$.   Pure Bayesian   Didn’t understand anything, kek   Regularized Ridge Regression   The Bayes and MAP estimates obtained for linear regression coincide with regularized ridge regression as well.      $$w_{ridge} = \\text{arg }\\min_w \\vert\\vert \\Phi W - y \\vert\\vert^2_2 + \\lambda\\sigma^2\\vert\\vert W\\vert\\vert^2_2$$   In case of polynomial regression, increasing $\\lambda$ tends to decrease the curvature, as 2-norm is being limited.   Replacing 2-norm with 1-norm is called as Lasso Regression, and with 0-norm is called as Support Based Penalty.   The additional term is usually represented as $\\Omega(w)$, and this formulation is known as Penalized Formulation. The original problem of wanting to limit 2-norm of $w$ to be less than or equal to $\\theta$ is called Constrained Formulation.         Claim.       For any Penalized Formulation with a particular $\\lambda$, there exists a corresponding Constrained formulation with a corresponding $\\theta$.    Lasso Regression tends to yield solutions that are sparser. That is, it is more likely for elements of $w$ to be 0 using this method. (many corners)   This follows a Laplacian distribution, and has no closed form solution. Therefore, iterating with gradient descent is the only way for Lasso regression. The algorithm for lasso regression will be explained in the next lecture.  ","url": "http://localhost:4000/notes/cs337/Lec5"
    },{
      "title": "Iterative Soft Thresholding Algorithm",
      "excerpt":"        This algorithm is used for fitting the model using lasso regression.   While the relative drop in Lasso error across $t=k$ and $t=k+1$ is significant, the following two steps are done.           LS Iterate       $w^{k+1}_{LS} = w^{k}_{Lasso} - \\eta\\nabla E_{LS}(w^{k}_{Lasso})$            Proximal Step       If absolute value of $\\left[w^{k+1}_{LS}\\right]_i$ is less than $\\lambda\\eta$, then the $i^{th}$ element of $w^{k+1}_{lasso}$ is 0.       Else, just reduce its magnitude by $\\lambda\\eta$ while keeping the sign intact.           Evaluating Performance   Method 1: Training Error   This idea is not good enough. Error obtained via ridge regression will always be larger than the least squared error. Also, going by training error alone lets overfitting slip by.   Method 2: Test Error   Use a different dataset, seperate from the training data to evaluate loss of the model. This is good for finding if overfitting is taking place.   There tend to be three main sources of error:     Bias - Difference between true and fitted data function   Variance - Deviation of fits as sample data changes   Noise   Bias Variance Analysis   Before starting the analysis, we assume the following three points:      Noise is Additive: $y = g(x) + \\epsilon$   Noise has mean 0 and variance $\\sigma^2$, need not be gaussian   We are performing Linear fit via OLS (Ordinary Least Squares)   Let $g$ be the true function, and $f$ be the model. If $&lt;\\hat{x},\\hat{y}&gt;$ is data, then the expected value of Least Square error is:      $$\\begin{eqnarray}   \\mathcal{E}[(f-y)^2] &amp;= \\mathcal{E}[(f-\\bar{f})^2] + (\\bar{f} - g)^2 + \\mathcal{E}[(y-g)^2] \\\\   &amp;= \\text{Variance}(g) + \\text{Bias}(g)^2 + \\sigma^2    \\end{eqnarray}$$   Here $f$ is $f(\\hat{x})$, $g$ is $g(\\hat{x})$ and $\\bar{f}$ is $\\mathcal{E}[f(\\hat{x})]$. Also, remember that $\\sigma$ is the variance of noise.   We divide the dataset into three categories:     Train: to train the model   Validation: to tune the model’s hyperparameters   Test: for final testing  ","url": "http://localhost:4000/notes/cs337/Lec6"
    },{
      "title": "Classification using Perceptrons",
      "excerpt":"        We would like a classifier which maps a data point to one of the given classes. Linear regression is not viable here as the classes are NOT REAL!   A (bad) solution is to assign numbers to classes, but this imposes ordering to the classes, which is not ideal. We use perceptrons for this case, and the classes are modeled as a one-hot encoded vector.   To start off, we shall tackle a Binary Classification Problem where the classes are represented by $+1$ and $-1$.   For input attributes $\\phi$ and learnt weights $w$, the hyperplane which separates the two classes would be given by $w^\\text{T}\\phi = 0$. We look at the perceptron algorithm for learning the weights.   That is, $y_{pred}=1$ if $w^\\text{T}\\phi&gt;0$ and $y_{pred}=-1$ if $w^\\text{T}\\phi&lt;0$. Do note that $w^\\text{T}\\phi$ is the perpendicular signed distance from the hyperplane.   Perceptron Algorithm   We iterate over all the available data. For a given data point $&lt; x_i, y_i &gt;$;      If classified correctly, do nothing   Marginally correct weights if incorrectly classified.   \\[w^{t+1} = w^t + \\eta y \\phi\\]  Note that $y \\phi$ is unsigned distance as it is always positive. $\\eta$ is the learning rate here.   One of the drawbacks of perceptron algorithm is that it finds any hyperplane which correctly classifies the given data, not the best hyperplane. SVM is used to find the best hyperplane.  ","url": "http://localhost:4000/notes/cs337/Lec7"
    },{
      "title": "Analysis of Perceptron Algorithm",
      "excerpt":"        The following claim can be proven mathematically.   Claim. If $\\exists w^*$ such that the given data is linearly seperable, then the perceptron algorithm will converge to a value $\\hat{w}$ which classifies the entire data correctly.   Proof todo here       Stochastic Gradient Descent   In normal gradient descent, we compute the value of $\\nabla \\mathcal{E}(\\Phi W, Y)$ and update the value of the weight vector accordingly.   In Stochastic gradient descent, we iterate over the entire data and update the weight vector at the $i^{th}$ iteration according to $\\nabla \\mathcal{E}(W^\\text{T}\\phi, y_i)$. It can be seen that the perceptron update rule follows stochastic gradient descent with the Hinge Loss Function.   \\[\\text{Hinge Loss}(f_w(x), y)= \\max(0, -yf_w(x))\\]  Deciding final weight vector   Using the finally obtained weight vector is not always a good idea, because the process is iterative in nature. Usually, one of these two methods is employed:      Voted Perceptron: Take the vector which classified most of the weight vectors correctly   Averaged Perceptron: Calculate the weighted average of the weight vectors  ","url": "http://localhost:4000/notes/cs337/Lec8"
    },{
      "title": "Logistic Regression",
      "excerpt":"        We’ve been taking the label of an input to be predicted by $\\text{sign}(W^\\text{T} \\phi)$ in the binary classification problem. In logistic regression, we shall take it to be given by $\\sigma (W^\\text{T} \\phi)$.   \\[\\sigma(x) = \\frac{1}{1+e^{-x}}\\]  In general, $f_w(x) = \\Omega (W^\\text{T} \\phi(x))$ where $\\Omega$ is called the Activation function.   In the case of binary classification using logistic regression, we shall take the value of $\\sigma$ to be the probability of the label being 1. We can thus take the value of $w^*$ to be the MLE estimate via this assumption.   \\[\\begin{align}   w^* &amp;= \\text{arg}\\max_W \\prod_i \\sigma(W^\\text{T} \\phi(x_i))^{y_i}(1-\\sigma(W^\\text{T} \\phi(x_i)))^{1-y_i}\\\\       &amp;= \\mathcal{L}(Data, W)\\\\ \\end{align}\\]  We define Cross Entropy Loss Function $E$ to be equal to $(-1/m)\\log(\\mathcal{L}(Data, W))$. Notice that maximising Log-likelihood or minimising this loss function are essentially the same thing. Upon simplification, the loss function turns out to be:   \\[E(w) = -\\frac{1}{m}\\left[ \\sum_i \\left(y_iW^\\text{T}\\phi_i - \\log(1+\\exp(W^\\text{T}\\phi_i))\\right) \\right]\\]  No closed form solution exists for the cross entropy loss function. We thus have to resort to gradient descent. The update rules for Gradient descent and Stochastic gradient descent respectively are given below.   \\[\\begin{align}   W^{k+1} &amp;= W^k + \\eta \\left[\\frac{1}{m} \\sum_i (y_i - \\sigma_{w^k}(x_i))\\phi_i \\right] \\\\   W^{k+1} &amp;= W^k + \\eta\\text{ } (y_i - \\sigma_{w^k}(x_i))\\phi_i \\\\ \\end{align}\\]  Regularised Logistic Regression   Similar to what has been done in Linear regression, we just add an additional term to the regularised loss function. The loss function now would be:   \\[E(w) = -\\frac{1}{m}\\left[ \\sum_i \\left(y_iW^\\text{T}\\phi_i - \\log(1+\\exp(W^\\text{T}\\phi_i))\\right) \\right] + \\frac{\\lambda}{2m}\\vert\\vert w\\vert\\vert^2_2\\]  The above loss is obtained by assuming a gaussian prior on the weight vector, $W\\sim\\mathcal{N}(0,1/\\lambda)$   It can be easily calculated that each of the update rules now have an additional term corresponding to the derivative of the penalisation term. The update rules for normal and stochastic are:   \\[\\begin{align}   W^{k+1} &amp;= W^k + \\eta \\left[\\frac{1}{m} \\sum_i (y_i - \\sigma_{w^k}(x_i))\\phi_i - \\lambda W^k \\right] \\\\   W^{k+1} &amp;= W^k + \\eta\\text{ } (y_i - \\sigma_{w^k}(x_i))\\phi_i - \\eta\\lambda W^k \\\\ \\end{align}\\]  Extension to Multiclass   We’ve been discussing the case of binary classification, where the number of classes are 2. Now, let there be $K$ classes. We would need $K-1$ weight vectors in this case, as the $K^{th}$ class is just the negation of all the others. Let the $K-1$ weight vectors be given by $w_1,\\ldots w_{k-1}$.   \\[\\begin{align}   \\mathcal{P}(y=c\\vert \\phi) &amp;= \\frac{e^{-w_c^\\text{T}\\phi}}{1+\\displaystyle\\sum_{j=1}^{K-1}e^{-w_j^\\text{T}\\phi}} \\\\   \\mathcal{P}(y=K\\vert \\phi) &amp;= \\frac{1}{1+\\displaystyle\\sum_{j=1}^{K-1}e^{-w_j^\\text{T}\\phi}} \\\\ \\end{align}\\]  We can have $K$ weight vectors with a little modification to the above formula, but it usually is not necessary. The probability equation in this case is given by:   \\[\\mathcal{P}(y=c\\vert \\phi) = \\frac{e^{-w_c^\\text{T}\\phi}}{\\displaystyle\\sum_{j=1}^{K}e^{-w_j^\\text{T}\\phi}}\\] ","url": "http://localhost:4000/notes/cs337/Lec9"
    },{
      "title": "Recurrent Neural Networks",
      "excerpt":"      Recurrent Neural Networks (RNNs) are used in situations where the sequence of data is as important (if not more) than the data itself. These networks essentially have a hidden “state” which stores the data about the information seen so far.    \tRNNs see usage in Sequence generation, NLP Classification and NLP Generation.   RNNs consist of a looped network whose output is the “hidden state”. That is, every word in a sequence of $n$ words is sent one-at-a-time; and at each iteration, the current hidden state is concatenated with the next word in the sequence to form the input.   Sticking with the same example, the “hidden state” gives memory to the network. This is quite useful in the case of NLP, as the meaning of a word quite heavily depends on the words that might have come before it.     Drawbacks with RNNs    \tThere are two major drawbacks of RNNs; they have a very short-term memory due to vanishing gradient and they can \"look\" in only one direction.   Consider the sentence “I live in Germany. I speak __.” In this case, it is a very reasonable guess that the person speaks German. And it has been shown that RNNs predict the word “German” with a high degree of accuracy. However, in the sentence “I live in Germany. I am quite fond of watching movies and playing basketball. I am fluent in __.”   In this case, the word “Germany” is very far away from the blank, meaning that it is very likely for the RNN to have dismissed this information. Also, because the network has multiple iterations done, a problem of vanishing gradient arises which makes learning that “Germany” is important infeasible. This is a drawback of the technique.   Another  drawback lies in the very nature of training. The words are input sequentially, meaning that a RNNs do not have information to the data which comes AFTER the word in the sentence. This can cause issues as the meaning of a word depends on what words are present on both of its sides.    \t\"I speak _____. I am a German.\"   Also, training RNNs is slow because the words are input sequentially. GPUs’ strongest suit is parallel processing, and this is not taken advantage of because each word is sent one after another.    \tThe above problems are mitigated to some extent via utilizing LSTM networks and Bidirectional techniques such as BERT with Attention respectively.  ","url": "http://localhost:4000/notes/chemcat/rnn"
    },{
      "title": "Multi-Armed Bandits",
      "excerpt":"        Explore Exploit Dilemma   This dilemma can be stated as the question:      Do I use the experience gained so far to pick the most optimal move, or do I keep exploring and obtain more data?    Do note that it is not guaranteed for the move made by the agent (using limited data) to be optimal globally. Such a dilemma is observable in clinical trials, online advertising and packet routing in communication networks.   Formal Definitions and Notations   Stochastic Multi-armed bandit   Has $n$ arms, each of which is a Bernoulli Distribution. The $i^{th}$ arm has mean reward $p_i$, and the largest mean is $p_M$.   It is Stochastic because the distribution of each arm is known.   Algorithm   Operates on the multi-armed bandit, by deciding which arm is to be pulled; to get a reward for that action.   That is, at any time $t$, given the history ($h^t$) it chooses an arm to sample ($a^t$) to obtain a reward $r^t$.    \t$$ h^t = (a^0, r^0, a^1, \\ldots, a^{t-1}, r^{t-1}) $$   The maximum number of pulls allowed is called as the total sampling budget or horizon ($T$).   A deterministic algorithm maps the set of all histories to the set of all arms. Similarly, a randomized algorithm maps the set of all histories to the set of all probability distributions over the arms.    \t$$ \\mathbb{P}(h^t) = \\prod^{T-1}_{t=0} \\mathbb{P}(a^t|h^t)\\cdot \\mathbb{P}(r^t|a^t) $$   Using the above relation, we can see that $(2n)^T$ possible histories are possible for a deterministic algorithm acting on a stochastic multi-armed bandit, with horizon $T$.   Epsilon Greedy Algorithms   $\\epsilon$ is a parameter $\\in [0,1]$ which controls the amount of exploration. It is used in different ways, some explained below.      $\\epsilon$G1            For $t \\leq \\epsilon T$, sample an arm uniformly at random       At $t = \\lfloor \\epsilon T \\rfloor$ determine the action with the highest empirical mean, and choose this action everytime           $\\epsilon$G2            For $t \\leq \\epsilon T$, sample an arm uniformly at random       For $t &gt; \\epsilon T$, sample the arm with the highest empirical mean       That is, the value of the mean  is updated in this case after $\\epsilon T$ steps, wheras it wasn’t done previously           $\\epsilon$G3            With probability $\\epsilon$ sample an arm uniformly at random, and with $1-\\epsilon$ sample the arm with highest empirical mean           Evaluating Algorithms   This is visualized by plotting a graph between the expected reward and the time stamp of the algorithm. To gauge the performance, three horizontal lines are plotted; $p_M$, $p_{min}$, $p_{avg}$. We expect the algorithm to start out near the average and reach the max with increasing time steps.   The expected cumulative regret of the algorithm is defined as follows:    \t$$ R_T = Tp_M - \\sum_{t=0}^{T-1}\\mathbb{E}(r^t) $$   That is, the difference between the maximum possible reward and what you end up getting. Ideally, we would like the value of $(R_T/T)$ to tend to $0$ as $T$ tends to $\\infty$.   That is, we would like the algorithm’s regret to be sublinear.  ","url": "http://localhost:4000/notes/cs747/week1"
    },{
      "title": "Regret Optimization",
      "excerpt":"        Let’s analyze the performance of $\\epsilon$G1 and $\\epsilon$G2. The regret calculations are shown below:      $$\\begin{eqnarray}   R_T &amp;=&amp; Tp_M - \\sum^{T-1}_{t=0}\\mathbb{E}(r^t) \\\\   &amp;=&amp; Tp_M - \\sum^{\\epsilon T-1}_{t=0}\\mathbb{E}(r^t) - \\sum^{T-1}_{t=\\epsilon T}\\mathbb{E}(r^t) \\\\   &amp;\\geq&amp; Tp_M - (\\epsilon T)p_{avg} - T(1-\\epsilon)p_M \\\\   &amp;\\geq&amp; \\epsilon T(p_M - p_{avg}) \\\\   &amp;\\in&amp; \\Omega (T) \\\\   \\end{eqnarray}$$   That is, the regret is not sublinear, even in the best case scenario where the algorithm performs perfectly after exploration.   Now let’s do the same regret analysis of $\\epsilon$G3.      $$\\begin{eqnarray}   R_T &amp;=&amp; Tp_M - \\sum^{T-1}_{t=0}\\mathbb{E}(r^t) \\\\   &amp;\\geq&amp; Tp_M - \\sum^{T-1}_{t=0}(\\epsilon p_{avg} + (1-\\epsilon)p_M)\\\\   &amp;\\geq&amp; \\epsilon T(p_M - p_{avg}) \\\\   &amp;\\in&amp; \\Omega(T) \\\\   \\end{eqnarray}$$   That is, all three epsilon greedy algorithms discussed earlier are not sub-linear in nature.   Acheiving Sublinear Regret   There are two general heuristics which should be met for a sub-linear algorithm.           Every arm in the multi-armed bandit must be pulled infinite number of times as $T\\rightarrow \\infty$. (Infinite Exploration)            Let $exploit(T)$ be the number of pulls that are exploitative in nature. Then, for sublinear regret we need the following;            $$\\lim_{T\\to\\infty}\\frac{\\mathbb{E}(exploit(T))}{T} = 1$$     That is, nearly all of the pulls must be of exploitative behaviour. (Greedy Limit)   Now, let $\\bar{\\mathcal{I}}$ be a set of all bandit instances with reward means strictly less than 1. Then;   An algorithm L acheives sub-linear regret on all instances of $I \\in \\bar{\\mathcal{I}}$ iff the algorithm satisfies both the above mentioned conditions.   These conditions are called as GLIE in short, which stands for “Greedy Limit Infinite Exploration”.   Modifying epsilon greedy strategies   $\\epsilon$G1/2 can be modified slightly to make it “GLIE compliant”, instead of exploring for $\\epsilon T$ pulls, we explore for $\\sqrt{T}$ pulls.      C1 satisfied since each arm pulled $\\sqrt{T}/n$ times on average   C2 satisfied as $exploit(T)$ would be $T-\\sqrt{T}$   Similarly, $\\epsilon$G3 can be fixed by making epsilon a function of $t$, as $1/(t+1)$. It can be seen pretty easily that the conditions are satisfied, using the below equation.      $$\\sum^{T-1}_{t=0}\\frac{1}{n(t+1)} = \\Theta\\left(\\frac{\\log T}{n}\\right)$$   Lai and Robbins Lower Bound   This result establishes that the lower bound on the regret  attainable for a sub-polynomial algorithm is logarithmic in $T$.   It has been stated more formally below; note the little-o notation.   If $L$ be an algorithm such that for every bandit $I\\in\\bar{\\mathcal{I}}$ and for every $\\alpha&gt;0$, as $T\\rightarrow\\infty$      $$R_T(L,I) = o(T^\\alpha)$$   Then, for every bandit instance $I\\in\\bar{\\mathcal{I}}$ as $T\\rightarrow\\infty$      $$ \\frac{R_T(L,I)}{ln(T)} = \\sum_{a:p_a(I)\\neq p_M(I)}\\frac{p_M(I) - p_a(I)}{KL(p_a(I), p_M(I))} $$   Where, $KL(x,y) = xln(x/y)+(1-x)ln((1-x)/(1-y))$   (Notice that the RHS of second equation is constant for a given bandit)   Sublinear Algorithms   UCB   At every time $t$ and arm $a$, define $\\text{ucb}^t_a$ as follows:        $$\\text{ucb}^t_a = \\hat{p}^t_a + \\sqrt{\\frac{2ln(t)}{u^t_a}}$$     Where $\\hat{p}^t_a$ is the empirical mean of that arm, and $u^t_a$ is the number of times that arm has been pulled. (Pull all the arms once before calculating)   The algorithm samples the arm with the highest ucb. This acheives a regret of $O(\\log(T))$, the optimal dependance on $T$.   KL UCB   Although UCB is optimal order-wise, the constant is still different. KL-UCB fixes this by changing the definition of UCB slightly.        $$\\text{kl-ucb}^t_a = \\max\\{ q\\in[\\hat{p}^t_a,1]\\text{ where } u^t_a KL(\\hat{p}^t_a,q)\\leq ln(t)+cln(ln(t)) \\}$$         $$\\text{where } c\\geq 3$$     Notice that $KL(\\hat{p}^t_a,q)$ monotonically increases with $q$, easy to find value by binary search! This algorithm asymptotically matches the Lai and Robbins’ Lower Bound as well.   Thompson Sampling   This algorithm uses Beta Distribution, and it’s parameters are given below. Note that this distribution is always going to give values between 0 and 1.   \\[Beta(\\alpha, \\beta) \\rightarrow \\mu = \\frac{\\alpha}{\\alpha+\\beta}, \\sigma^2 = \\frac{\\alpha\\beta}{(\\alpha+\\beta)^2(\\alpha+\\beta+1)}\\]      At time $t$, let arm $a$ have $s^t_a$ successes and $f_a^t$ failures. Then, $Beta(s^t_a+1, f_a^t+1)$ represents a belief about the true mean of that arm.   For every arm $a$, draw a sample $x^t_a \\sim Beta(s^t_a+1, f_a^t+1)$ and chose the arm which gave the maximal $x^t_a$ (and update the distribution).   This acheives optimal regret, and is excellent in practice. Usually, it performs slightly better than KL-UCB as well.   All these algorithms are examples of optimism in face of uncertainity principle.  ","url": "http://localhost:4000/notes/cs747/week2"
    },{
      "title": "Hoeffding's Inequality",
      "excerpt":"        Let $X$ be a random variable in $[0,1]$ with $E[X] = \\mu$. Now, let’s say that we draw $u$ samples from this distribution, each $x_i$, and the empirical mean is $\\bar{x}$.   Then, for any fixed $\\epsilon&gt;0$ we have      $$\\begin{eqnarray}     \\mathcal{P}\\{ \\bar{X} \\geq \\mu+\\epsilon} \\leq \\exp{-2u\\epsilon^2} \\\\     \\mathcal{P}\\{ \\bar{X} \\leq \\mu-\\epsilon} \\leq \\exp{-2u\\epsilon^2} \\\\   \\end{eqnarray}$$   Intuitively, as $u$ increases, the probablity that empirical mean deviates should decrease. Here, $\\epsilon$ acts as Confidence Interval around the true mean (which is unknown).   Doubt in Slide 4, point 2; It doesn’t seem right   Hoeffding’s Inequality can be extended to a random variable bounded in $[a,b]$ by defining a new random variable $y = \\frac{X-a}{b-a}$, and applying to this. We get;      $$\\begin{eqnarray}     \\mathcal{P}\\{ \\bar{X} \\geq \\mu+\\epsilon} \\leq \\exp{\\frac{-2u\\epsilon^2}{(b-a)^2}} \\\\     \\mathcal{P}\\{ \\bar{X} \\leq \\mu-\\epsilon} \\leq \\exp{\\frac{-2u\\epsilon^2}{(b-a)^2}} \\\\   \\end{eqnarray}$$   KL Inequality   This gives a tighter bound for empirical mean. Note that $\\mu\\pm\\epsilon$ must be positive in RHS of both the inequalities.      $$\\begin{eqnarray}     \\mathcal{P}\\{ \\bar{X} \\geq \\mu+\\epsilon} \\leq \\exp{-uKL(\\mu+\\epsilon, \\mu)} \\\\     \\mathcal{P}\\{ \\bar{X} \\leq \\mu-\\epsilon} \\leq \\exp{-uKL(\\mu+\\epsilon, \\mu)} \\\\     \\\\     KL(p,q) = pln(\\frac{p}{q})+(1-p)ln(\\frac{1-p}{1-q}) \\\\   \\end{eqnarray}$$   Note that both the inequalities are examples of Chernoff bounds.       Analysis of UCB   We shall use the above inequalities to prove that the UCB algorithm is logarithmic in nature.   Notation      $\\Delta_a = p_M - p_a$ - The difference between this arm and optimal   $Z^t_a$ - An event when arm $a$ is pulled at time $t$   $z^t_a$ - Indicator for the above event (1 if event occurs, 0 otherwise)      $$\\mathcal{E}\\[z^t_a\\] = \\mathcal{P}\\[Z^t_a\\]$$      $u^t_a$ - Number of times arm $a$ has been pulled, till time $t$     $$u^t_a = \\sum_{i=0}^{t-1}z^i_a$$      $\\bar{u}^T_a$ - A constant used in the proof, sufficient number of pulls of arm $a$ for horizon $T$     $$\\bar{u}^T_a = \\ceil{ \\frac{8}{(\\Delta_a)^2}ln(T) }$$   This proof truly be a mind = blown moment :/       Thompson Sampling  ","url": "http://localhost:4000/notes/cs747/week3"
    },{
      "title": "Markov Decision Problem",
      "excerpt":"        A Markov Decision problem $M$ is given by the tuple $(S, A, T, R, \\gamma)$. Each of these elements are defined below.      $S$ is the set of states. $\\vert S\\vert = n$        $A$ is the set of actions. $\\vert A\\vert = k$            $T$ is the Transition function. $T(s, a, s’)$ gives the probability of reaching $s’$ from $s$ by taking the action $a$. Note that $T(s,a,\\cdot)$ is a probability distribution over $S$.            $R$ is the Reward function. $R(s,a,s’)$ gives the reward for reaching $s’$ from $s$ via action $a$. We assume $R(s,a,s’)\\in [ -R_{max}, R_{max} ]$.       $\\gamma$ is the Discount factor.   Let $s^t$ denote the state at the $t^{th}$ time-step. Similarly, $a^t$ is the action taken at this time-step and $r^t$ is the reward for this action.   Trajectory is the history of the agent.   A policy $\\pi$ is a function which the agent follows to give an action at any state. For simplicity, let the agent look only at the current state for its policy. That is, $\\pi: S\\to A$. This policy is:     Markovian - Looks only at the current state and not at history   Deterministic - A single action is given, not probability distribution   Stationary - The output doesn’t change with time   $\\Pi$ is the set of all policies. In this case, $\\vert \\Pi\\vert = k^n$. We would like a policy which maximizes the overall reward obtained.   Value of a State   The value of a state under a policy $\\pi$, $V^{\\pi}(s)$ tries to tell us how “good” the state is for accumulating a large reward.      $$ V^{\\pi}(s) = \\mathcal{E}_{\\pi}\\left[ r^0 + \\gamma r^1 + \\gamma^2r^2 + \\ldots \\vert s^0 = s \\right] $$   Large $\\gamma$ means a large “lookahead”. The agent looks further ahead into the future to decide upon the value of the state. Do note that $\\gamma\\in [0,1)$       MDP Planning Problem   It can be mathematically proven that every such MDP has an optimal Markovian, deterministic, stationary policy $\\pi^*$ such that      $$ \\forall\\pi\\in\\Pi, \\forall s\\in S: V^{\\pi*}(s)\\geq  V^{\\pi}(s)$$   That is, the value of every state is larger in the optimal policy. This policy is NOT unique! The MDP Planning Problem is centered around finding this optimal policy $\\pi^*$.       Alternative Formulations   Reward Function   The reward function can be defined differently in literature. For example, it could be stochastic instead of deterministic. It might be just dependant on $(s,a)$ instead of $(s,a,s’)$.   Some authors minimize cost instead of maximizing reward. The core idealogy remains the same, however.   A multi-armed bandit is an MDP with a single state and each of the actions being associated with a uniform distribution as a reward!   Episodic Tasks   Our discussion used continuing tasks, where trajectories are infinitely long. Episodic tasks have a terminal/sink state from which outgoing transitions are absent.   There should be non-zero probability of reaching the sink state from every non-terminal state, meaning that trajectories would surely be finite.   Value function   The definition used by us is called as Infinite Discounted Reward. There are other choices as well:      Total Reward - Can only be used on episodic tasks (Obv)   Finite Horizon Reward - Set a horizon $T$ and sum up rewards till here. Optimal policies for this need not be stationary.   Average Reward - Exactly what the name suggests       Policy Evaluation   The process of calculating the value of each state given the MDP is called as policy evaluation. Bellman’s Equations are used for this procedure.   These equations are used to find the values of every state for a given MDP. For every state $s\\in S$;      $$V^{\\pi}(s) = \\sum_{s' \\in S} T(s, \\pi(s), s')\\left[ R(s, \\pi(s), s') + \\gamma V^{\\pi}(s') \\right]$$   There are $n$ such linear equations, and $n$ unknowns.     unique solution guaranteed if $\\gamma &lt; 1$   for episodic, if $\\gamma=1$, solution guaranteed if value of terminal state fixed as $0$.       Action Value Functions   The naive method of finding $\\pi^*$ for a given MDP would be to use Bellman’s equations for each policy and get the minima. This would take $\\text{poly}(n,k)\\cdot k^n$ time.   We define the Action Value function $Q^{\\pi}(s,a)$ as the first step for obtaining a more efficient solution.   $Q^{\\pi}(s,a)$ is the expected long term reward for starting at state $s$, taking action $a$ at $t=0$ and then following $\\pi$ for $t&gt;0$.   Its value is given by the equation:     $$ Q^{\\pi}(s,a) = \\sum T(s,a,s')\\left[ R(s,a,s') + \\gamma V^{\\pi}(s') \\right] $$   We know that all optimal policies have same optimal value function, and by extension, all optimal policies will have the same action value function as well.  ","url": "http://localhost:4000/notes/cs747/week4"
    },{
      "title": "MDP Planning ALgorithms",
      "excerpt":"        We would like to solve the MDP Solving problem efficiently. Before stating the algorithm, we shall first go over a bunch of theoretical results.       Banach Space   A complete, normed vector space is called as a Banach Space. That is, both the vector space and norm are well defined. Additionally, every Cauchy sequence must have a limit in that vector space.   A Cauchy sequence is a convergent sequence. Example, rationals are not Banach because an irrational number can be written as a sequence of rational numbers ($\\sqrt{2}$)   Contraction Mapping   A mapping $T:X\\to X$ is called a contraction mapping with contraction factor $L$ if $\\forall u,v\\in X$,   \\[\\vert\\vert Tv-Tu\\vert\\vert \\leq L\\vert\\vert v-u\\vert\\vert\\]  $x^*$ is a fixed point of $T$ if $Tx^*=x^*$.   Banachs Fixed-Point Theorem   This theorem states that for a given contraction mapping and a contraction factor $L\\in[0,1)$:      There exists a unique fixed point   For $x\\in X, m\\geq 0; \\vert\\vert T^mx-x^*\\vert\\vert \\leq L^m\\vert\\vert x-x^*\\vert\\vert$       Bellman Optimality Operator   $B^* : \\mathcal{R}^n\\to\\mathcal{R}^n$ for an MDP $(S,A,T,R,\\gamma)$ is given as follows;    $$ (B^*(F))(s) = \\max_{a}\\sum_{s'}T(s,a,s')\\left[ R(s,a,s') + \\gamma F(s') \\right] $$   That is, we have used notation to redefine the domain of value function to be $\\mathcal{R}^n$, and $B^*$ maps value functions.   Theorem. $(\\mathcal{R}^n, \\vert\\vert\\cdot\\vert\\vert_{\\infty})$ is  a Banach space, and $B^*$ is a contraction mapping in this space with contraction factor $\\gamma$.      $$ \\vert\\vert F\\vert\\vert_{\\infty} = \\max (\\vert f_1\\vert, \\ldots \\vert f_n\\vert) $$   Bellmans Optimality Equations   The fixed point can be obtained by solving the equation:    $$ V^*(s) = \\max_{a}\\sum_{s'}T(s,a,s')\\left[ R(s,a,s') + \\gamma V^*(s') \\right] $$   There are $n$ variables and $n$ unknowns, but the equations are non-linear in nature. These equations are called as the Bellman’s Optimality Equations for the given MDP. Algorithms for solving these equations will be discussed below.   We shall also prove later that $V^*$ is an optimal value function for a given MDP.       Value Iteration   This is a very simple algorithm. We apply the optimality operator iteratively until the difference is negligible. We know this works from the Banach’s Fixed point operator.   Also, notice that $V^*, Q^*, \\pi^*$ have a cyclic relationship.      $V^*\\to Q^*$: definition of $Q^*$   $Q^*\\to \\pi^*$: $\\text{arg}\\max_aQ^*(s,a)$   $\\pi^*\\to V^*$: Solve bellman equations for $\\pi^*$       Linear Programming   This is a technique where a Linear objective function of variables under given linear constraints is maximized. We can frame the Bellman’s optimality equations as a linear programming problem.   From the Bellman optimality equations, we can say the following. Notice that there are $nk$ linear constraints and $V^*$ staisfies all these constraints.    $$ V(s) \\geq \\max_{a}\\sum_{s'}T(s,a,s')\\left[ R(s,a,s') + \\gamma V^*(s') \\right] $$      $B^*$ preserves $\\succeq$   From this, we can say that $V \\succeq V^*$, and can thus linearise the result to be $\\sum_s V(s)\\geq \\sum_s V^*(s)$.   Therefore, the objective function to be maximized is given by    $$ -\\left( \\sum_s V(s) \\right) $$   This LP problem has $n$ variables and $nk$ constraints. Do note that a dual is possible which solves for $\\pi^*$ with $nk$ variables and $n$ constraints.  ","url": "http://localhost:4000/notes/cs747/week5"
    },{
      "title": "Reinforcement Learning",
      "excerpt":"        Looking at the MDP from the agent’s point of view, it is not guaranteed to know the transition probabilities and the rewards associated with an action.   That is, the entire MDP (S, A, T, R, $\\gamma$) is provided to the agent in the planning problem. In a learning setting, the agent knows only (S, A, $\\gamma$) and sometimes R. It has to make inferences on T from experience.   Let a t-length history be defined as follows:   \\[h^t = (s^0, a^0, r^0 \\ldots s^t)\\]  Learning Algorithm   A Learning Algorithm L is a mapping from the set of all histories to set of all probability distributions over arms. We would like to construct L such that:   \\[\\lim_{T\\to\\infty}\\frac{1}{T}\\left( \\sum_{t=0}^{T-1} \\mathcal{P} \\[ a^t\\sim L(h^t) is optimal for s^t \\] \\right)\\]  The above problem is also known as the Control Problem.  ","url": "http://localhost:4000/notes/cs747/week7"
    },{
      "title": "Prediction Problem",
      "excerpt":"        This problem is usually the first step in solving the Control problem. Let the learning algorithm be $L$. At each step $t$, let the estimated value of the states by the agent be $\\hat{V}^t$. Given a policy $\\pi$ that the agent follows, with value function $V^\\pi$, we would like $L$ such that:   \\[\\lim_{t\\to\\infty} \\hat{V}^t = V^\\pi\\]  A few assumptions have to be taken before we can begin to tackle this problem.   Irreducibility   Given an MDP $M$ and policy $\\pi$; if there is a directed path between every $s$ and $s’$ with non-zero policy, then The MDP is said to be irreducible under the policy.   If $M$ is irreducible for all $\\pi\\in\\Pi$, then $M$ is irreducible.   Intuition: We do not want the learning algorithm’s policy to be stuck in some subset of states   Aperiodicity   whaaatt       Ergodicity   An MDP which is both Irreducible and Aperiodic is said to be Ergodic in nature. An Ergodic MDP induces a steady state distribution $\\mu^\\pi: S\\to(0,1)$ for every policy $\\pi$.   That is, let the probability of current state being $s$ after $t$ timesteps be $p(s,t)$. This value must also depend on the initial state. However, ergocidity states that as $t\\to\\infty$, the distribution tends to $\\mu^\\pi$, which is independent od the initial state.       Model-Based Approach   A model is an estimate of the MDP by the agent. Let the estimated MDP be $(S, A, \\tilde{T}, \\tilde{R}, \\gamma)$, we wish that this converges to the true MDP.   As like with bandits, we would like to visit every state-action pair infinitely many times.   The algorithm we shall propose is the following:     Take uniform random actions until every state-action pair has been taken atleast once, the model is said to be valid now   If model is valid, take a random action with probability $\\epsilon$ and calculated optimal with probability $1-\\epsilon$   Update the values of (S, A, T, R) to be their empirical means after every iteration   We shall state without a proof that for convergence to optimal behaviour, the algorithm only requires irreducibility.   An algorithm is said to be “model-based” if if uses $\\theta(n^2k)$ memory. Similarly, we call a model to be model-free if it is using only $\\theta(nk)$ memory.   Monte-Carlo’s Method   Given an episodic task, we observe the agent taking uniform random actions for $e$ episodes. This method is used to estimate the value of each state given the history for each episode.   We define the following:      $G(s,i,j)$ : Discounted reward of $s$ from its $j$’th visit in episode $i$   $1(s,i,j)$ : 1 if $s$ has occured atleast $j$ times in episode $i$   First Visit Monte-Carlo   \\[\\hat{V}(s) = \\frac{\\sum G(s,i,1)}{\\sum 1(s,i,1)}\\]  Every Visit Monte Carlo   \\[\\hat{V}(s) = \\frac{\\sum_i\\sum_j G(s,i,j)}{\\sum_i\\sum_j 1(s,i,j)}\\]  Second Visit Monte-Carlo   \\[\\hat{V}(s) = \\frac{\\sum G(s,i,2)}{\\sum 1(s,i,2)}\\]  Last Visit Monte-Carlo   \\[\\hat{V}(s) = \\frac{\\sum G(s,i,times(s,i))}{\\sum 1(s,i,times(s,i))}\\]      $\\hat{V}(s)$ for the first three given above converges to the ideal value, BUT THE LAST ONE DOESN’T!  ","url": "http://localhost:4000/notes/cs747/week8_1"
    },{
      "title": "Reinforecment Learning",
      "excerpt":"        We shall focus on the first-visit Monte Carlo method for now. We’ve already defined the estimate of the value of the state to be given by:   \\[\\begin{align} \\hat{V}^t(s) &amp;= \\frac{1}{t}\\sum_{i=1}^t G(s,i,1) \\\\ &amp;= \\frac{1}{t}\\left( (t-1)\\hat{V}^{t-1}(s) + G(s,t,1) \\right) \\\\ &amp;= (1-\\alpha_t)\\hat{V}^{t-1}(s) + \\alpha_t G(s,t,1) \\quad \\alpha_t = 1/t \\\\ \\end{align}\\]  We know that $\\hat{V}^t(s)$ converges when $\\alpha_t = 1/t$. What about other values, does the value estimate converge then?       Stochastic Approximation   The value estimate converges ($\\lim_{t\\to\\infty} \\hat{V}^t\\to\\hat{V}^\\pi$) iff $\\sum_i\\alpha_t = \\infty$ and $\\sum_i \\alpha_t^2 &lt; \\infty$.   This is called as the On-Line implementation of the monte-carlo algorithm.       TD-0 Algorithm   In MC methods, we wait for the episode to end before updating. However, we could update the state just after making an action. That is, consider the following history:   \\[s_1, 2, s_2, 1, s_3, 3, s_T\\]  The updated estimate for $s_1$ would be given by:   \\[\\hat{V}^{t+1}(s_1) = (1-\\alpha_{t+1})\\hat{V}^t(s_1) + \\alpha_{t+1}\\[ 2+\\gamma+3\\gamma^2 \\]\\]  However, we can instead update the estimate of $s_1$ right after making the first action by using the estimated value of $s_2$ as follows:   \\[\\hat{V}^{t+1}(s_1) = (1-\\alpha_{t+1})\\hat{V}^t(s_1) + \\alpha_{t+1}\\[ 2+\\gamma\\hat{V}^t(s_2) \\]\\]  This estimate is called the Bootstrapped Estimate. Do note that $\\hat{V}(s_T)$ is set to 0, and is not updated for episodic tasks.   This is the main philosophy of TD(0) algorithms, the estimated value of the state is updated everytime an action is taken.   Usually, the data which is provided to us is small, meaning that the value obtained after iterating through it would depend on the random initial policy used. To circumvent this, we iterate over the entire dataset multiple times, and the value function obtained like this is given by $\\hat{V}^t_{batch_TD}(s)$.       Convergence of MC and TD-0 Algorithms   It can be seen quite easily from the definitions of MC algorithms that the MC algorithms converge least square solution which minimizes a cost function.   \\[\\begin{align}   \\hat{V}^t(s) &amp;= \\frac{\\sum_i G(s,i,1)}{\\sum_i 1(s,i,1)} &amp;= \\underset{V}{\\operatorname{argmin}} \\sum_i 1(s,i,1)\\[ V(s)-G(s,i,1) \\]^2 \\\\ \\end{align}\\]  Similarly, it turns out that the value function the TD-0 algorithm converges to is the MLE estimate for the given data!       Control with TD Learning   Recall that in the control problem, we would like to reach the optimal policy and not just calculate the value functions. To tackle this, we maintain an Action-Value function instead of just a value function.      Q-Learning: $r^t + \\gamma\\max_a \\hat{Q}^t(s^{t+1},a)$   Sarsa: $r^t + \\gamma \\hat{Q}^t(s^{t+1},a^{t+1})$   Expected Sarsa: $r^t + \\gamma\\sum_a \\pi^t(s^{t+1},a)\\hat{Q}^t(s^{t+1},a^{t+1})$   Q-Learning is called Off-policy wheras Sarsa and Expected Sarsa are called On-policy learning algorithms.   If the underlying policy is $\\epsilon$-greedy, all three algorithms converge to the same Q function. However, if the policy is time-invariant, then Sarsa and Expected Sarsa converge to the policy but Q-Learning converges to its epsilon greedy equivalent!   Also note that these three algorithms are model-free!  ","url": "http://localhost:4000/notes/cs747/week8_2"
    },{
      "title": "",
      "excerpt":"Sed     Sed is a “Stream Editor”. Meaning unlike a traditional editor, examination window is limited to a single screen. Sed is called as sed &lt;options&gt; &lt;script&gt; input_files. The line which sed is currently reading is loaded into the “pattern space”. We then modify this line via the script, and then we either hold the line, or output the modified file. The modified line goes into std::out; but can be redirected.   Passing Scripts to sed   &lt;script&gt; variable is passed either in-line enclosed within quotes, or with the -f option and writing the script in a text file and doing sed -f &lt;script file&gt; input_files.   Regular Expression Syntax        /^x1/: Search for lines beginning with x1   Options      -n : Don’t print lines by default. That is, usually, sed in.txt would print all lines, and we don’t want that. So, if we do sed -n '/search/p' in.txt would only print lines with the word “search”. Similarly sed -n '1,10 p' in.txt would print the first 10 lines.   -s: Treats independent files differently. Meaning, sed -n '1,10 p' in1.txt in2.txt would only apply the script to in1.txt and not in2.txt. But if we wanted to print the first ten lines of both, then this option is used. sed -n -s '1,10 p' in1.txt in2.txt   -r: enables usage of extended regular expressions, for convenience   -e: Allows grouping of commands together, example given below.     Scripts   A script is of the form: [address] [!] command; [] represent that the argument is optional. If address is not given, command is applied for all lines. ! is the NOT operator.   Address Usage      sed -n '3 p' in.txt: prints the third line   sed -n '$ p' in.txt: Prints the last line   sed -n -e '10 s/__/  /' -e '10 p' in.txt: in s/__/  /; s stands for replace, and here we replace ‘__’ with ‘    ‘; after which that line is printed.   sed -n -r '/19D[0-9]{6}/ p' in.txt: Print lines in the form “19D123456”   Using regular expressions, we can do some powerful stuff. Suppose, I wanted to print the lines before line 15, which started with the word engi; we do this as sed -n -r '/^engi/,15 p' in.txt  and to print all statements which are between lines starting with x1 and x2; sed -n -r '/^x1/,/^x2/ p' in.txt   Nesting is done using the curly braces {}; for example, to show all statements between lines 4 and 20 beginning with x1 is done as sed -n -e '4,20{^x1 p}' in.txt   Commands      = : print the line number and p: print the line   Modify Commands           Insert i\\: Used to insert the given string before the line address. sed '10 i\\ /'This string is inserted at line 10'/' in.txt             Append i\\: Similar to insert, but the new line is AFTER the given line number. Insert and append dont work for ranged addresses.            change c\\: Change the entire matched line with given text. sed -n -r '/19D[0-9]{6}/ c\\ Dual Degree' in.txt would change entire row to just “Dual Degree”            delete d\\: Similar to Change, but deletes the entire line. Can be used for all types of addresses.            Substitute s:       s/x1/x2/[flags]; replaces the first occurrence of x1 with x2. If /g (global) is passed into flag; all occurrences are removed.       &amp;:   To find x1, and replace with x1+x2; do sed -n 's/x1/&amp;x2/' in.txt       n\\:  Suppose we wanna replace “Blue is very sus” to “Red is not sus” (Imagine we have roll no instead of colours and using -r)  sed 's/Blue( is )very( sus)/Red\\1not\\2' in.txt; \\1 holds the value in the first parentheses and so on.            Transform y Syntax: [address1],[address2]y/x1/x2/ : replace occurrences of x1 with x2; x1 and x2 must have same length       I/O Commands      n: (default) Read a line, without \\n char, execute commands, print if -n is not used   N: Read a line, add a newline, add to pattern space and execute. Used to join lines. sed -r 'N; s/\\n//' in.txt; joins lines 1,2; 3,4 and so on   p: Copies entire pattern space to output                 P: Prints only the first line of the pattern space   h: Overwrites pattern space onto hold space          H: Appends pattern space to hold space (pattern space unchanged in both)   g: Overwrites hold space onto pattern space           G: Appends hold space to pattern space(hold space unchanged in both)   d: deletes pattern space   r filename: reads file and sends to output. The file is not passed to pattern space.   w filename: writes to the file from the pattern space   b: Similar to goto statement; can be used for if-else statements ig, usage given below   q: Quit reading the file   Script Writing   #!/bin/bash #just put each indiv word in a new line sed -n -r ' /19D[0-9]{6}/ b save\t#branch to save; otherwise continue \tw others.txt \tb\t\t\t\t\t#branch to end \t \t:save \tw dd-students.txt ' in.txt    Awk     Scans every line, splits each line into field, compare fields to pattern, perform action. The difference between awk and sed is that awk has capability to remember history between files. The syntax of the script is very similar to C.   #Basic Awk Syntax awk [options] 'script' file(s)\t\t\t#Direct console awk [options] -f scriptfile file(s)\t\t#input via file   The script part is as follows: pattern [action]; if pattern is missing then action is done for all lines, and if action is missing, then matched line is printed. Either pattern or action must be given.   A line is divided into fields by awk by using a delimiter (default - whitespace); and each of these fields can be accessed by using variables, $1 refers to the first field and so on.   Pre-Defined Variables      FS – Field Separator, default: whitespace   RS – Record Separator, default: newline   NF – Number of fields in current record   NR – Number of the current record – idx   OFS – Output Field Separator, default: whitespace   FILENAME - Current Filename   ls -al | awk '{print NR, $9}'\t\t#Print all files with numbering   ##   Script structure   The scripts are divided into three parts; BEGIN, BODY, END. The statements in BEGIN are executed once(declare variables and stuff), the ones in BODY are executed for every line, and the statements in END are executed once at the end of the file (post processing).   #Structure of Body pattern {statement} pattern {statement; statement; ...} pattern { \tstatement \tstatement \t... }   Pattern Types           awk '/special/ {print}': prints the lines with the word “special”.            Instead of this, we can check if the field matches a regular expression as: awk -F, '$1~/19D[0-9]{6}/{print NR, $0}'; meaning if the student is dd, then print his name with numbering.            ~ stands for Matching; and !~ stands for not matching in this case            However, they need just be pattern matching; we can use boolean arguments as well such as $1 + $2 &gt; 5 {statement}       Expressions   #We are feeding the input from wc -c *; in which the first variable is the number of characters of the given line awk ' BEGIN{ \tlines=0; char=0; } { \tlines++; char+=$1; } END{ \tprint lines, \" lines read\"; \tprint \"Total characters: \",char; } '   Passing Variables to awk   To pass the variables from the shell script to the awk script, use the flag -v; as:- awk -v var1=\"$shell_var\" '{script}'   Associative Arrays   No need for pre-allocating, they get defined with use.   awk -F, ' \t#($9~/[A-Z][A-Z]/){ \t\tstate[$9]+=$10 \t} \tEND{ \t\tfor(i in state){ \t\t\tprint state[i], i; \t\t} \t} '   Output Statements      print: simple w/o no formatting   printf: print with formatting   sprintf: format a string   ","url": "http://localhost:4000/notes/cs251a_bash/"
    },{
      "title": "Basic Methods",
      "excerpt":"     Linear Regression   The main aim is to estimate a linear equation representing the given set of data. There are two approaches to this.      A closed form solution.  This can be directly obtained by solving the linear differential equation. Calculate the partial derivative of the error function wrt x, y and equate both of them to zero to get the values of the parameters which minimizes the error function.   An iterative approach.  This is similar to Gradient Descent. We try to obtain the minima (L1, L2 norm etc) by calculating the gradient at each point and moving in small steps along the gradient vector.  Refer to this video for more details.   Logistic Regression   Refer to the following link to see an example of logistic regression.  ","url": "http://localhost:4000/notes/dl/basic"
    },{
      "title": "Convolutional Neural Networks",
      "excerpt":"    These networks are best suited for image processing, and the number of connections in between the hidden layers is decreased by a significant factor. This decrease is possible because the “meaning” of a pixel usually depends only on its neighbouring pixels, and not the entire image. A few basic terminologies are discussed below.       Basic Terminologies          Convolution       This step involves having a kernel of fixed size move across the image (with a stride that need not be 1) and produce a feature map which makes it easier for the network to train. Many kernels can be operated on a single image, giving many feature maps.       Do note that the kernel must always be odd-sized.            Pooling       The size of a map is reduced by first dividing it into smaller parts, each with size m×m. Each of these smaller squares is replaced by a single pixel, usually by taking the max or the average of all the values in that cell.            ReLU       This introduces non-linearity in the system so as to make the network more flexible in representing a large variety of functions.            Fully Connected Layers       Once all the “pre-processing” of the image via convolution and pooling is done, the resultant values are passed into a fully connected layer for the neurons in that layer to train. The number of layers is variable, but the output layer must have the sam enumber of neurons as the number of possible classifications. (due to obvious reasons)       Types of Convolution          Dilated Convolution       Converting a 10x10 map to a smaller map of size 6x6 using a kernel of size 3 would take two consecutive convolutions. Instead of doing this twice, we can “inflate” the size of our original kernel to 5 by adding two additional rows and columns of 0s in between. This would require the convolution to be done only once, saving computational effort.       The number of rows/columns of 0s added is called as Dilation Rate.       Example:                        1 0 1 0 1    1 1 1           0 0 0 0 0    1 1 1   ⇒       1 0 1 0 1   1 1 1           0 0 0 0 0                    1 0 1 0 1                      Transposed Convolution       This is the reverse of convolution, where we increase the size of the map by padding it and applying the feature map. This is used to “estimate” what the original map might’ve been, and is used in encoder-decoder networks.           Famous CNNs          LeNet       A very simple yet efficient network. It was mainly designed for classification of MNIST data. The net consists of two succesive convolutions and pooling, followed by two dense hidden layers.            AlexNet       This is an improvement over LeNet. This is my implementation of this net using PyTorch. I have been able to obtain a classification accuracy of 95% after training the net for 15 epochs, but noticed that the training seemed to saturate after the first epoch itself.                     Classification by Modified Alexnet              VGG       This has a very similar implementation philosophy as AlexNet, we increase the number of feature maps while decreasing the size of each feature map. A small improvement here is that convolution layers are put successively, so as to save computational time.       That is, a 7x7 kernel over c sized map would need 49c² wheras having two succesive 3×3 kernels would need 27c² computations.       This is my implementation of VGGNet, and the results are shown below.                                                                                              Training and Evaluation of modified VGG Net               GoogleNet (2014 Winner of ImageNet challenge)       Convolution with kernel size larger than (or equal to) 3 can be very expensive when the number of feature maps is huge. For this, GoogleNet has convolutions using a kernel size of 1 to reduce the feature maps, followed by the actual convolution. These 1×1 feature maps are also called as Bottle Neck feature maps.       GoogleNet also utilizes Inception Modules.            ResNet (2015 Winner of ImageNet challenge)       Short for residual network, the net submitted for the challenge had 152 layers but the number of parameters are comparable to AlexNet (Which has just 4 layers). The problem of learning slowdown is tackled in a very novel way in this network.       ResNet acheives this by having “skip connections” in between the blocks of convolution. That is, let an input X be passed into a convolution block to get an output F(X). The skip connection is used to add X to this result, yielding X+F(X). The reasoning is that although the network might fail to learn from F(X); it will be able to learn from X itself directly.       My implementation of ResNet for the MNIST dataset has been shown here. I have been able to acheive an accuracy of 98%, but I do believe 99% is possible had I trained the net for longer. The feature maps and structure of the network has been modified a little to make it more easier to train, but the core idea remains the same.                                                                                              Training and Evaluation of modified ResNet       ","url": "http://localhost:4000/notes/dl/cnn"
    },{
      "title": "Numerical Differentiation and Integration",
      "excerpt":"One of the simplest and easiest ways to approximate the value of $f’(x_0)$ would be to use the definition of derivative.   \\[f'(x_0) = \\lim_{h\\to 0}\\frac{f(x_0+h)-f(x_0)}{h}\\]  Using a small enough $h$ should be sufficient to get a good approximation of the function’s derivative. But how small is “good enough”? To answer this question, we perform error analysis by assuming the line between the points at $x_0$ and $x_0+h$ to be a Lagrangian approximation. The error turns out to be:   \\[\\left\\vert f'(x_0) - \\frac{f(x_0+h)-f(x_0)}{h} \\right\\vert= \\frac{1}{2}\\vert h f''(\\xi(x))\\vert\\]  (n+1) Point Formula   Instead of using the naïve algorithm, the Lagrange polynomial can be differentiated to get an approximation of the derivative. Let the data be ${ x_0,\\ldots x_n }$ and we would like to compute the derivative at $x_j$. It would be given by:   \\[f'(x_j) = \\sum_i f(x_i)L_i'(x_j) + \\frac{f^{(n+1)}(\\xi(x_j))}{(n+1)!}\\prod_{i\\neq j}(x_j-x_i)\\]        It isn’t possible to reduce $h$ to an absurdly low value because round off errors become predominant in that case. The net error in the case of three point midpoint formula is given by $h^2M/6 + \\epsilon/h$.   Therefore, we conclude that the $(n+1)$ point formula is unstable in nature.       Numerical Integration   The most basic method to approximate an integral is called Numerical Aperture. It involves using a summation.   \\[\\int_a^bf(x)dx \\approx \\sum_{i=1}^n a_if(x_i)\\]  The Lagrange interpolating polynomial can be used to great effect here. For data ${x_0,x_1\\ldots x_n}$ the polynomial and the corresponding numerical aperture approximation would be given by:   \\[\\] ","url": "http://localhost:4000/notes/ma214/diff_int"
    },{
      "title": "Deep Learning",
      "excerpt":"       This book on deep learning has been followed, this section will contain nomenclature and a few key definitions I thought were important enough.      perceptrons: older way of representing neural networks. Can give an output of either 0 or 1, corresponding to if the value of $w\\cdot x+b$ is lesser than or greater than 0.   sigmoid neurons: we usually change the values slightly in perceptrons to slowly approach the required classification function. However, because perceptrons are binary in nature, small changes can cause drastic (and unintended) changes to the output. Sigmoid neurons try to minimize this issue.   The standard sigmoid function is given as follows:    \t$$ \\sigma (w\\cdot x+b) = \\frac{1}{1+exp(-w\\cdot x-b)} $$   That is, is is a smoothened out version of the step function. We can also see that the output changes linearly with changes in inputs (using partial derivatives). $(w\\cdot x+b)$ is called as the “Weighted input” for that particular neuron, and is represented by $z$.       MLP - Multi Layer Perceptrons   These have sigmoid neurons as layers in them. The neurons taking input are called input neurons and comprise the input layer. Similarly, we have the output neurons and the output layer. Neurons (and layers) which are neither input nor output are called as Hidden Layers. We will be using a feed-forward neural network, meaning that the output of a layer always leads to an input of another layer in a linear fashion without loops. If layers have loops, they are called Recurrent Neural networks or RNNs.   For example, a neural network responsible for detecting the number in a MNIST dataset can have just three layers; Input (28×28 neurons), hidden (variable $n$) and output (10 neurons). The network is trained using a training set, and the mean squared loss function is minimized by using gradient descent.       Gradient Descent   Given a function $f(x_1, x_2)$, the minima of the function can be computed empirically by taking its partial derivative and “walking” such that the function value is reduced.    $$\\begin{eqnarray} \\Delta f &amp;=&amp; \\frac{\\partial f}{\\partial x}(\\Delta x) + \\frac{\\partial f}{\\partial y}(\\Delta y) \\nonumber \\\\ &amp;=&amp; (\\triangledown f)\\cdot(\\Delta X) \\nonumber \\\\ &amp;=&amp; -\\eta \\parallel \\triangledown f \\parallel ^2 \\nonumber \\\\ \\end{eqnarray}$$   We have substituted $\\Delta X = -\\eta \\triangledown f$ to get the above equation, which is always negative.   $\\eta$ is called as the Learning Rate, and is directly proportional to how “large” the “steps” are.   In our case, we would be applying gradient descent and changing the values of all the biases ($b_i$) and weights ($w_i$) to minimize the cost function. A drawback of this method is that calculating the cost function requires the summation of the mean squared error over all values of training data, which would be ranging in the order of $10^5$. This causes the training to be very slow.       Stochastic Gradient Descent   Instead of taking all the $n$ values in the training data set, we create a subset called the “mini set” where each element is a random subset of size $m&lt;n$. We compute the cost function over every subset in the mini set, with the assumption that the “true” cost function and the empirically calculated cost function are nearly equal. This dramatically reduces the time required for training the network.   When the mini set is exhausted, an epoch of training is said to be completed after which the process is repeated again. This is to mark the progress of training.   ** Vectorizing sigmoid function **       Back-Propagation   Assumption1: The cost function for a set of inputs is equal to the average of the cost function for each individual input. This assumption holds for the Least-Mean-Squared cost function.   Assumption2: The cost function should be a function of the outputs of the neural network.   Given the cost function C, and the weighted input z for a neuron, we define error for this neuron δ as follows;    $$\\delta = \\frac{\\partial C}{\\partial z}$$   That is, if $\\delta$ is a large value, then a change in $z$ can bring about a change in $C$. If it is zero, then it means that $C$ is optimal wrt $z$.   There are four fundamental equations to back propogation, and they have been given below. $\\delta L$ is the $\\delta$-vector for the final layer.     δL = (∂C/∂a) ⊙ σ’(z)   δⁿ = (wⁿ⁺¹)ᵀ(δⁿ⁺¹) ⊙ σ’(z)   (∂C/∂b) = δ  ⇒  Delta of a neuron is equal to the derivative of the cost function wrt to its bias   (∂C/∂wⁿⱼₖ) = aₖⁿ⁻¹ * δⱼⁿ   (Do remember that in wⱼₖ, neuron k is in the n-1’th layer and neuron j is in the n’th layer)   This is how a single iteration of training a neural network is done:     Input a set of training examples.   Feedforward the values to get the output.   Calculate the error at the outer-most layer.   Backpropogate the error till the input layer.   Perform gradient descent, as partial derivatives wrt all biases and weights is known.       Learning Slowdown and the cross entropy cost function   We’ve been using the quadratic cost function so far. It does have a few issues, notably its derivative is very small when the value of $\\sigma(z)$ is close to 0 or 1. In gradient descent, the change in biases and weights is directly proportional to the derivative of the cost function, meaning it is possible for this function to learn very slowly when it is giving wrong results. We turn to the cross entropy cost function as a solution.    $$C = -\\frac{1}{n}\\sum_x[y\\log(\\sigma(z)) + (1-y)\\log(1-\\sigma(z))]$$   It can be checked mathematically that the derivative of this cost function wrt $b$ and $x$ is independant of $\\sigma ‘(z)$, meaning no learning slowdown occurs. Moreover, the derivative is proportional to error meaning that learning occurs faster when the model is more wrong, as we would like it.   The cross entropy cost function can be defined for an entire layer as well;-    $$C = -\\frac{1}{n}\\sum_x\\sum_y[y\\log(\\sigma(z_j^L)) + (1-y)\\log(1-\\sigma(z_j^L))]$$   Here zⱼᴸ is the j’th neuron in the final layer ‘L’.   Do note that a sigmoid function coupled with the cross entropy cost function is quite similar in terms of learning slowdown to the softmax function coupled with the log-likelihood cost function. (the derivatives wrt b and x have the same behaviour)       Avoiding overfitting           Increase the size of training data       Regularization            In L2 regularization, a new term is added to the cost function as shown below. The second summation is over all the weights in the network. $\\lambda$ is called the regularization parameter.                 $$C = -\\frac{1}{n}\\sum_x\\sum_y[y\\log(\\sigma(z_j^L)) + (1-y)\\log(1-\\sigma(z_j^L))] + \\frac{\\lambda}{2n}\\sum_w w^2$$               Similarly, L1 regularization is given below:                 $$C = -\\frac{1}{n}\\sum_x\\sum_y[y\\log(\\sigma(z_j^L)) + (1-y)\\log(1-\\sigma(z_j^L))] + \\frac{\\lambda}{2n}\\sum_w |w|$$               Dropout regularization is a technique wherina random half of the hidden neurons are ommited from the network for a single training iteration. The idea here is that “different networks can have different overfitting heuristics, and training them seperately can cause the averaging out of their errors.”           Artificially inflate the size of training data  In the case of MNIST, just rotate/blur the images by a small degree to get new training data!       Initializing the weights and biases   We have so far been initializing all weights and biases from a gaussian distribution of mean 0 and standard deviation 1. This isn’t optimal, as the standard deviation of $z=\\sum_i(w_ix_w)+b$ would be very large, proportional to the square of the umber of inputs the neuron has. This can cause the output of the sigmoid function to be nearly 0 or 1, causing stagnation as discussed earlier.   To solve this problem, we initialize $b$ as earlier but $w$ is initialized with mean 0 and standard deviation of $1/\\sqrt{n}$ where n is the number of inputs.       Universality of Neural Networks   This is a very important mathematical analysis (which I shall not write here for the sake of brevity) that neural networks (even simple ones with a single hidden layer) can compute any function with relative precision given enough time to train.   The approximation is better when there are more neurons used in the hidden layer. Also, we get an approximated continuous function as a result of estimating a discontinuous function by this method.  ","url": "http://localhost:4000/notes/dl/dl"
    },{
      "title": "Equations in One Variable",
      "excerpt":"        This part of the course focuses on efficient ways of finding the roots of a given function. That is, given a function $f$ we are interested in finding $x$ such that the following equation is satisfied.   \\[f(x) = 0\\]  One of the most basic methods of finding roots of an equation would be the bisection method.   Bisection Method   This method uses the intermediate value theorem, which states      If $f:[a,b]\\to\\mathcal{R}$ is a continuous function with $f(a)\\cdot f(b)&lt;0$ then there exists $c\\in[a,b]$ such that $f(c) = 0$    We “guess” the value of $c$ to be the midpoint of the current interval and check to see which of the halves the root lies by comparing signs of extremities.   Suppose that we wish to find the root with an error of $\\epsilon$. The bisection method is continued until $\\vert p_n - p_{n-1} \\vert &lt; \\epsilon$.   This method is very simple, but has a number of drawbacks. It is very slow to converge and a good intermediate approximation may get discarded at times. It is usually used at the start of approximation and more powerful methods are used after this.       Fixed Points   A fixed point of a function $f:[a,b]\\to\\mathcal{R}$ is a point $p$ such that $f(p)=p$. This problem can be converted to the root finding problem quite easily, and a number of theorems exist which make solving this problem a bit easier.      Fixed Point Theorem     If $f:[a,b]\\to[a,b]$ is continuous then $f$ has at least ONE fixed point.     If additionally, $f’(x)$ exists on $(a,b)$ and $\\vert f’(x) \\vert\\leq k&lt;1$ for all $x\\in(a,b)$ then $f$ has a UNIQUE fixed point in $[a,b]$.    Fixed Point Iteration   This is a simple an easy technique for finding the fixed point of a function. Given a continuous function $f:[a,b]\\to[a,b]$, we start with any $p_0\\in[a,b]$ and generate a sequence $p_n = f(p_{n-1})$.   IF ${p_n}$ is a convergent sequence, then we can easily see that the sequence would converge to the fixed point of the function. However, we cannot guarantee the sequence’s convergence. Even if it is convergent, it is not guaranteed that the function would converge rapidly.   We shall look at ways to manipulate the root finding problem into the fixed point problem which satisfies the fixed point theorem as well.   Newton Raphson Method   Given a twice differentiable function $f:[a,b]\\to\\mathcal{R}$, the 2-degree Taylor’s approximation of its root $p$ would be:   \\[0=f(p)=f(p_0)+(p-p_0)f'(p_0)+\\frac{(p-p_0)^2}{2}f^\"(\\xi)\\]  If we assume that $p_0$ is close enough to the true root, we get the following iterative sequence.   \\[p_n = p_{n-1} - \\frac{f(p_{n-1})}{f'(p_{n-1})}\\]  It can be tested out the this method is much better than the fixed point iteration method. However, it is important to remember the conditions upon which Newton-Raphson method works, $p_0$ is close enough and $f$ is twice differentiable.      Theorem     Let $f:[a,b]\\to\\mathcal{R}$ be twice differentiable.     If $p\\in[a,b]$ such that $f(p)=0$ and $f’(p)\\neq 0$ then there exists a neighborhood around $p$ such that any sequence with its start in this neighborhood converges to $p$ via the Newton-Raphson method.    Secant Method   Finding the value of $f’(p_n)$ at every step of the iteration is computationally intensive. We this approximate the value of the derivative in this method.   \\[p_n = p_{n-1} - f(p_{n-1})\\left(\\frac{p_{n-1}-p_{n-2}}{f(p_{n-1})-f(p_{n-2})}\\right)\\]  This method obviously takes more time to converge than the Newton Raphson method but makes up for it in being less computationally intensive.   Regula Falsi Method (Method of False Position)   In both Newton Raphson and Secant methods, it is quite possible that all our estimates lie on a single side of the root, making it hard to ascertain how close to the root we are. This method is a slight modification to have an accurate bounds between which the root exists.   We first choose $p_0$ and $p_1$ such that $f(p_0)\\cdot f(p_1)&lt;0$. An approximation $p_2$ is made using the secant method. $p_2$ will lie on either side of the root. If $f(p_2)\\cdot f(p_1)&lt;0$ we carry on with the pair $(p_1, p_2)$. Otherwise, we carry on with the pair $(p_0, p_2)$.   Note that all these methods are slight modifications of the Newton Raphson method, and thus require the function to be double differentiable and the estimate to be close to the actual root.       Order of Convergence   Let ${p_n}$ be a sequence that converges to $p$ with $p_n\\neq p$ for any $n$. We can say that the order of convergence of ${p_n}$ to $p$ is $\\alpha$ with asymptomatic error $\\lambda$ if:   \\[\\lim_{n\\to\\infty}\\frac{\\vert p_{n+1} - p \\vert}{\\vert p_n - p \\vert^\\alpha}  = \\lambda\\]  An iterative technique $p_n = g(p_{n-1})$ is said to be of order $\\alpha$ if the sequence converges with order $\\alpha$.   A sequence is said to be linearly convergent if $\\alpha =1 $ AND $\\lambda &lt; 1$. Similarly, a sequence is said to be quadratically convergent if $\\alpha=2$.   It can be seen quite easily from the Mean Value Theorem that Fixed Point Iteration converges linearly if $f’(p)\\neq 0$. This implies that we need $f’(p)=0$ for higher order of convergence.      Theorem     Let $p$ be a fixed point for the function $f$. Let $f’(p)=0$ and $f^”$ be continuous with $\\vert  f^”(x)\\vert &lt; M $ where $x$ is nearby $p$. We can then state that there exists a neighborhood around $p$ where the sequence $p_n=f(p_{n-1})$ converges at least quadratically. For sufficiently large $n$,   \\[\\vert p_{n+1}-p_n \\vert &lt; \\frac{M}{2}\\vert p_n - p \\vert^2\\]   None of this matters for us because the Newton Raphson method and Secant method require $f’(p)\\neq 0$. We thus try to come up with better methods to converge to fixed points faster.   Multiplicity of a Zero   Let $g:[a,b]\\to\\mathcal{R}$ be a function with $p\\in[a,b]$ being a root of $g$. We say that $p$ is a zero of multiplicity $m$ of $g$ if for $x\\neq p$, we can write the following. $p$ is said to be a Simple Zero   if $m=1$. \\(g(x) = (x-p)^mq(x)\\)   Newton Raphson method works well when $g$ has a simple zero at $p$, however, it may not converge quadratically when the order of zero is greater than 1.       Modified Newton Raphson Method   Given all the above problems that this method has, we modify it slightly. We shall try to modify $f$ to a new function $\\mu$ such that $p$ is always its simple root. We can show that $\\mu = f/f’$ works well for this case. Applying Newton Raphson to $\\mu$ and simplifying, we get the following sequence;   \\[p_{n+1} = p_n - \\frac{f(p_n)f'(p_n)}{f'(p_n)^2 - f(p_n)f^\"(p_n)}\\]  This sequence is guaranteed to converge quadratically, but requires computing the double derivative as well which can be quite expensive.   Aitken’s Method of Accelerating Convergence   Aitken’s $\\Delta^2$ method involves computing a sequence in parallel to our original sequence with the following equation. An example has been given as well, which shows the convergence of $Cos(1/n)$. It can be seen that $\\hat{p}$ converges faster to 1.   \\[\\hat{p}_n = p_n - \\frac{(p_{n+1}-p_n)^2}{p_{n+2}-2p_{n+1}+p_n}\\]     ","url": "http://localhost:4000/notes/ma214/eq_in_one_var"
    },{
      "title": "Contact Me",
      "excerpt":"I’d appreciate all the feedback given by you! The feedback can be about anything; such as the website’s style as an example. The form is linked here.   I reccommend mailing me (on the left) to get a response back. Preserve anonymity when mailing me by using an alternate email such as Proton Mail.    PS: Incase you're really bored, try looking through the comments of this site on different pages.    ","url": "http://localhost:4000/feedback/"
    },{
      "title": "Introduction to Numerical Analysis",
      "excerpt":"        We study the various methods of solving equations, approximating functions and study the errors corresponding to the aforesaid methods. For example, consider the approximation of $e$ using its definition and the Taylor’s Series.   \\[\\begin{align*} e &amp;= \\lim_{n\\to\\infty}(1+1/n)^n \\\\ f(x) &amp;= f(a) + f'(a)(x-a) + \\ldots+ \\frac{f^{(k)}(a)}{k!}(x-a)^k + \\frac{f^{(k+1)}(c)}{(k+1)!}(x-a)^{k+1} \\end{align*}\\]  Where $c$ is a real number between $x$ and $a$. We can also compute the number of terms that are required in the Taylor’s series to estimate its value with a given error using the final term.   Finite Digit Arithmetic   A $64$ bit representation is generally used for a real number. The smallest magnitude possible by this representation is when $(s,c,f) = (0,1,0)$. Note that the tuples $(0,0,0)$ and $(1,0,0)$ represent $0$ and not $2^{-2013}\\cdot 1$.                  Bits       Symbol       Name       Use                       1       $s$       Sign Indicator       Indicating whether the number is +/-                 11       $c$       Characteristic       Exponent of the number                 52       $f$       Mantissa       Fractional part of the number           \\[\\text{Number} = (-1)^s\\cdot 2^{c-1023}\\cdot (1+f)\\]        Floating Point Representation   We shall represent numbers in the form:   \\[\\pm 0.d_1d_2\\ldots d_k\\times 10^n \\qquad \\text{where }1\\leq d_1\\leq9, 0\\leq d_i\\leq9\\]  If a number has greater than $k$ digits, we can bring it down by two methods:      Chopping: All digits from $d_{k+1}$ are dropped   Rounding: We add $5\\times 10^{n-(k+1)}$ to the number and then drop the additional digits   There are two ways of finding the error of such approximations. Let $p$ denote the actual value and $p^*$ denote the approximated value.      Absolute - $\\vert p-p^* \\vert$   Relative - $\\vert p-p^* \\vert/p$   Relative error is usually the better measure as it takes the actual value of $p$ into account.   Significant Digits   We say that the number $p^*$ approximates $p$ to $t$ significant digits when $t$ is the largest non-negative integer such that   \\[\\frac{\\vert p-p^* \\vert}{p} &lt; 5\\times 10^{-t}\\]  That is, the number of significant digits NEED NOT BE EQUAL to the number of decimal places that a number has!   Errors Caused by Finite Digit Arithmetic   There are two main kinds of errors that are caused by the limitations of finite digit arithmetic, cancellation of nearly equal numbers and division by small numbers.   Consider the function $f(x) = 1-Cos(x)$ at $x = 1.2\\times 10^{-5}$. The value of $Cos(x)$ would be $\\text{0.999 999 999 9}$ with $10$ digits, however, the function value equates to $\\text{0.000 000 000 1}$. We had started with $10$ significant digits but ended up with only $1$ by the end. The relative error that we get in this case would be very large.   \\[\\frac{\\vert1-Cos(1.2\\times 10^{-5}) - \\text{0.000 000 000 1}\\vert}{1-Cos(1.2\\times 10^{-5})} = 0.3889\\]  Errors also get magnified when divided with small numbers. For example, let $a^* = a+\\epsilon$ be the approximation with error $\\epsilon$. When divided by $b=10^{-n}$ we can see that the error has increased drastically.   \\[fl\\left(\\frac{fl(a)}{fl(b)}\\right) = (a+\\epsilon)\\times 10^n\\]  In the quadratic roots formula, we can rationalize it to avoid errors incurred when $b^2$ is very large compared to $4ac$. We use the first formula if $b&gt;0$ and use the second formula if $b&lt;0$.   \\[\\frac{-2c}{b+\\sqrt{b^2-4ac}} \\qquad\\qquad \\frac{-2c}{b-\\sqrt{b^2-4ac}}\\]  Instead of getting $0$ outright, we now get $-2c/b$ which is an improvement. This is highly case-specific however, and more ways of avoiding errors will be discussed subsequently.  ","url": "http://localhost:4000/notes/ma214/intro_to_num"
    },{
      "title": "Generative Adversial Networks",
      "excerpt":"     GAN stands for Generative Adversial Networks, and they belong to the set of Generative models. These models are called “generative” because of their ability to generate new data, such as a picture of a human face.   The mathematical analysis and intuition behind these networks is given below.       Inverse Transform Method and its implications   This method is used to simulate drawing values from a complicated probability distribution, using a uniform distribution. Let the CDF of the complicated distribution be F. Assume that it is invertible, and let its inverse be given by F⁻¹. Let a draw from the uniform distribution be u. It can quite easily be proven that F⁻¹(u) has the same distribution as F itself.   F⁻¹ is called as the Transform Function, as it is used to transform the uniform distribution to the target distribution.   This has a very important implication in the generation of new data. Suppose that there is a probability distribution for a vector of size n² (an image of size nxn) called the “Human Distribution” H, which tells how likely it is that the image represents a human face. We can now simply generate a random vector from uniform distribution, pass it through H⁻¹ to generate a human face!   There are very obvious problems here:     The “human distribution” is a very complex distribution over a very large space   It may or may not exist       Generative Models   In reality, we cannot explicitly say what H is. Therefore, a generative model aims at estimating the value of the transform function (H⁻¹ in this case). Training such a model has two ways, direct and adversial. Both these methods have been explained below.       Direct training (Generative Matching Networks)   We have one neural network which aims to estimate the transform function by comparing the acheived distribution to the “true” distribution. However, we do not know the true distribution (otherwise we wouldn’t need to do all this!) We thus take samples of human faces from available data and generate human faces via the neural network. Then, the Maximum Mean Discrepency between the two estimated distributions is taken as the error for back-propogation.   The neural net would strive to reduce MMD, meaning that it would learn the transform function upon training for sufficient time. This way of training for generation is used in Generative Matching Networks.       Adversial training (GANs)   We have two neural networks here, the generator and the discriminator. The generator has the same role as the previous model, wherein it tries to estimate the transform function by generating images of a human face. The discriminator is handed images from both generator and the already available data. It is tasked with classifying the images into two groups, the ones from the generator and the ones from the true data set. The generator is hence tasked with fooling the discriminator to the best possible extent.   Both the networks are trained simultaneously, and it can be clearly seen that they both are competing with each other. The discriminator tries to increase the error between the generated and the true data set whereas the generator tries to decrease this value at the same time. This competition is why the architecture is referred to as an “Adversial” network. Both the nets improve in what they’re trying to acheive by competing against each other.  ","url": "http://localhost:4000/notes/dl/gan"
    },{
      "title": "Gryffin: Bayesian Optimization of Categorical Variables",
      "excerpt":"        We formulate a chemical reaction as an optimization algorithm. The parameters associated can be either continuous or categorical. There exist methods such as PHOENICS for the optimization of continuous variables, but its pretty tough for applying the same for categorical variables.   This is where GRYFFIN steps in, it tries to extrapolate the methods used for continuous variables to categorical variables.   Thus, to understand GRYFFIN we would have to first understand PHOENICS better.   PHOENICS - Continuous Variable Optimization   Let the true function be represented by $f$, and our estimate of the function (surrogate function) so far be $\\hat{f}$. A three layered bayesian neural network is used for this purpose.   This is done by sampling values, and the acquisition function $\\alpha (z)$ tells us which value to sample at.   This is an iterative procedure, the following steps are done:     $\\alpha (z)$ proposes the optimal value at which to sample, $x’$   Sample at $x’$, and rebuild $\\hat{f}$ using the bayesian network   We stop after a set number of iterations are done, or when convergence is reached     Extending to Categorical   For example, let there be three types of Ligands, A B and C. We can represent each of these ligands as a one-hot encoded vector, such as $(1,0,0)$ for Ligand A.   We now assume the following:   The Ligands A B and C are sufficiently independant and descriptive enough of the “Ligand Space”. {. :notice–success}   We then try to represent the rest of the ligands as a vector. That is, another ligand D might be represented by the vector $(0.2, 0.3, 0.7)$, using the Gumbel-SoftMax Distribution.      This is called as Soft One Hot Encoding    This method is not fool-proof though, it is plausible that our original assumption is not valid. That is, if A and B are similar in nature, there is a redundancy in the Ligand Space!   The issue of similarity is addressed by introducing a descriptor space.   Descriptor Space   Incorporate domain knowledge to measure similarity! That is, we can plot the ligands on a 2d plot where the axes correspond to the HOMO-LUMO densities, and use the “distance” of a ligand D from each of these ligands to get the soft-one hot vector.   We would like the set of descriptors to have:      High correlation with the objective function   Number of descriptors should be small   Low pair-wise correlation amongst each other   It is quite difficult to manually select a large number of descriptors which folow the above pair of rules.   What the paper did was just send the descriptor vector into a neural network with a single hidden layer and softsign activation to get a descriptor output vector, of smaller size.   ","url": "http://localhost:4000/notes/surp/gryff"
    },{
      "title": "Hello.",
      "excerpt":"                                                                                       Feel free to go through the content on my website through the tabs located at the top. You can also search for anything specific.                                                                                                 My academic and professional portfolio                                                    Sup.   Welcome to my website, hope you enjoy your stay.   I work on this site whenever inspiration strikes me, so you can expect occasional updates to keep it fresh and relevant.   I am proficient in multiple programming languages including C++ and Python.   As a passionate machine learning enthusiast, I am eager to explore the diverse range of applications this field has to offer across various industries and domains.   I have experience working with various frontend technologies including ReactJS, Angular, and Django, among others.       Hobbies and Interests.                              I am interested in Dungeons and Dragons, and I participate in sessions with friends when we all are free.                   Recently, I've been exploring the world of 3D printing and modeling as a way to create unique figurines and other captivating trinkets to enhance our gaming campaigns.                  There's a picture of us playing DnD together in my room!                     After a few months of self-guided learning, I've gained proficiency in both Fusion360 and Blender, which has allowed me to bring my imaginative designs to life.      I occasionally sketch, and am currently attempting to wrap my head around using my ipad for digital art.       I enjoy listening to music with a blend of synthwave, heavy metal and electronic. I listen to numerous artists, but Carpenter Brut's distinctive style has struck a particular chord with me and currently stands out as my favorite.      I cherish playing video games that have great storylines, unique gameplay mechanics, or are just really fun to play. I'm so into them that I even made a special tab at the top! If you're interested, feel free to check it out!                       That's all for now. Feel free to contact me if you wanna get in touch or just chat.          ","url": "http://localhost:4000/"
    },{
      "title": "Composite Integration",
      "excerpt":"        Newton Cotes methods are unreliable because they use equally spaced nodes, and this may not be efficient for higher degree polynomials which have huge variations. Instead, we can simple apply lower degree Newton-Cotes methods to consecutive sets of points and achieve better results.   Composite Trapezoidal Rule   \\[\\int_a^bf(x)dx \\approx \\frac{h}{2}\\left[ f(a) + 2\\sum_{j=1}^{n-1}f(x_j) + f(b) \\right] + (\\text{error term})\\]     Stability of Composite Rules   All composite rules are STABLE!   That is, the round-off error does not depend upon the number of computations performed. Recall that this was not the case for differentiation methods. The however have the same drawback as the Newton-Cotes methods wherein the nodes are equally spaced. This will be addressed in the next method.       Adaptive Quadrature Method   The step size is modified by predicting the function variation at a given $x$, to keep the error below a specified bound $\\epsilon$. The analysis for only the Simpson’s method was performed, although it can be extended to other methods as well. Note that the error in Simpson’s method is $\\mathcal{O}(h^5)$   The below condition ensures that $S(a,b)$ will have an error of $\\epsilon$. If the condition is not met, we simply divide the interval $(a,b)$ into two and try to achieve an error of $\\epsilon/2$ on each of the subintervals.   \\[\\vert S(a,b) - S(a,(a+b)/2) - S((a+b)/2,b)\\vert &lt; 15\\epsilon\\]  This algorithm terminates usually (not always).   Gaussian Quadrature   We try to choose points to be optimal mathematically to reduce the error, rather than choosing them to be equally spaced. That is, the parameters are chosen such that the method has the largest degree of precision.   \\[\\int_a^bf(x)dx = \\sum_{i=1}^n c_if(x_i)\\]  We have $2n$ parameters that need to be chosen, meaning that the largest polynomial that can be approximated is of degree $2n-1$. There are two main methods:   1. Special Cases   Work with the special cases of $2n-1$ degree polynomial $(1,x,\\ldots,x^{2n-1})$ and solve the coefficient equations.   2. Legendre Polynomials   Note that this method works when the domain of integration is $[-1,1]$. Legendre Polynomials of degree $n$ $(P_n)$ have two main characteristics:      They are monic   $\\int_{-1}^1 P_n(x) P(x) = 0$ when degree of $P &lt; n$      Importantly, the values of $x_1,\\ldots x_m$ are given exactly by the roots of $P_n$.   \\(c_i =\\int_{-1}^1\\prod_{j=1}^n\\frac{x-x_j}{x_i-x_j}dx\\)     Multiple Integrals   \\[\\int\\int f(x,y) dy dx = \\int \\left(\\int f(x,y) dy\\right) dx\\]  That is, simply apply the techniques learnt so far to the inner integral, and then apply it again for each term obtained for the outer integral computation!          Improper Integrals   \\[\\int_a^b \\frac{1}{(x-a)^p}\\text{ converges IFF }0&lt;p&lt;1\\]  Singularity at a single end point   Consider the following integral, and let the numerator $g(x)$ be $n+1$ times continuously differentiable.   \\[\\begin{align*} \\int_a^b\\frac{g(x)}{(x-a)^p}dx = \\int_a^b \\frac{g(x)-P_n(x)}{(x-a)^p}dx + \\int_a^b\\frac{P_n(x)}{(x-a)^p}dx \\end{align*}\\]  The second term in the RHS can be evaluated easily. The first term is evaluated defining the fraction to be zero when $x\\to a$, and simply using the techniques mentioned above (such as the composite Simpson’s rule).      Infinite Limits are handled by using the substitution $t=x^{-1}$ and using the techniques mentioned above.  ","url": "http://localhost:4000/notes/ma214/comp_int"
    },{
      "title": "Interpolation",
      "excerpt":"        We will be focusing on interpolation using polynomial functions.   \\[P(x) = a_nx^n+a_{n-1}x^{n-1}+\\ldots+a_0\\]  Given any continuous function $f:[a,b]\\rightarrow\\mathcal{R}$, there exists a polynomial that is as “close” to the given function as desired. This is called as the Weierstrass Approximation Theorem.   One natural choice would be to use the polynomial obtained from the Taylor’s theorem. However, note that these polynomials approximate the function only at a given point. It is not guaranteed that the Taylor’s polynomials at higher degree would offer higher a better approximation. (Works for $e^x$ but not for $1/x$)   Lagrange Interpolating Polynomials   Let $x_0, x_1, \\ldots x_n$ be distinct $(n+1)$ points and let $f$ be a function with $f(x_i)=y_i$. We would like to find a polynomial $P$ such that $P(x_i)=y_i$   Define $L_{n,i}(x_j) = \\delta_{i,j}$ where $\\delta_{i,j}$ is $1$ iff $i=j$ and $0$ otherwise. We can define $L_{n,i}(x)$ as follows:   \\[L_{n,i}(x) = \\frac{(x-x_0)(x-x_1)\\ldots(x-x_{i-1})(x-x_{i+1})\\ldots(x-x_n)}{(x_i-x_0)(x_i-x_1)\\ldots(x_i-x_{i-1})(x_i-x_{i+1})\\ldots(x_i-x_n)}\\]  Now that this term has been defined, we can easily write the interpolation function as:   \\[P(x) = y_0L_{n,0}(x)+y_1L_{n,1}(x)+\\ldots+y_nL_{n,n}(x)\\]  We can say that a unique interpolation polynomial of degree $\\leq n$ exists for a given set of $n+1$ points.   Error of interpolation   Let $f:[a,b]\\rightarrow \\mathcal{R}$ be $(n+1)$ differentiable, and $P(x)$ be the interpolating polynomial given $(n+1)$ distinct points $x_0, x_1\\ldots x_n$ where $x_i\\in[a,b]$.   For each $x\\in[a,b]$ there exists a $\\xi (x)\\in(a,b)$ such that   \\[f(x) = P(x) + \\frac{f^{(n+1)}(\\xi(x))}{(n+1)!}(x-x_0)(x-x_1)\\ldots(x-x_n)\\]  Neville’s Formula - Cumulative Computation   We would like to have a method in which the lower degree polynomials help in computing the higher degree interpolating polynomial. Consider the nodes ${x_0, \\ldots x_n}$ with $Q_i(x)$ being the interpolation of all points except $x_i$ and $Q_j(x)$ being the interpolation over all points except $x_j$. The interpolation over the entire set of nodes is given by:   \\[P(x) = \\frac{(x-x_j)Q_j(x) - (x-x_i)Q_i(x)}{x_i-x_j}\\]  This lets us build the interpolating polynomial from the ground up, one degree at a time. Let $P_{i,j,k,\\ldots}$ denote the interpolating polynomial obtained from the $i$‘th, $j$’th, $k$‘th and so on points, we can see that:      If we only care about the value of the interpolated polynomial, we don’t even need to calculate the polynomial itself; we can just replace $x$ in Neville’s formula with the required value. The value of $x$ here is $2.1$.      Divided Differences   This is another method of constructing the interpolated polynomial. Given $(n+1)$ nodes $x_0, x_1, \\ldots x_n$, we can define a new function $f[x_0, x_1, \\ldots x_n]$ which is the coefficient of $x^n$ in the interpolating polynomial. Note that the function is independent of the order of nodes.   We shall now try to construct a recursive relation over $f$. Let $P_{n-1}$ be the polynomial over ${x_0, x_1\\ldots x_{n-1}}$ and $Q_{n-1}$ be the polynomial over ${x_1, x_2\\ldots x_n}$. From Neville’s formula we can get the following:   \\[\\begin{align*} \tP_n(x) &amp;= \\frac{(x-x_n)P_{n-1} - (x-x_0)Q_{n-1}}{x_0-x_n} \\\\ \tf[x_0,\\ldots,x_n] &amp;= \\frac{f[x_0,\\ldots,x_{n-1}] - f[x_1,\\ldots,x_{n}]}{x_0 - x_n} \\end{align*}\\]  We know that for $x_i\\in{x_0,\\ldots x_{n-1}}, P_n(x_i)=P_{n-1}(x_i)$, by definition of the interpolating polynomial. Therefore, $P_n - P_{n-1}$ is equal to $0$ at the aforementioned $n$ points.   \\[\\begin{align*} P_n - P_{n-1} &amp;= \\alpha(x-x_0)(\\ldots)(x-x_{n-1}) \\\\ &amp;= f[x_0, \\ldots, x_{n}](x-x_0)(\\ldots)(x-x_{n-1}) \\\\ \\implies P_n &amp;= P_{n-1} + f[x_0, \\ldots, x_{n}](x-x_0)(\\ldots)(x-x_{n-1}) \\\\  \\end{align*}\\]  We can pre-compute the divided differences using the modified Neville’s formula and then use the above recursive formula to get the interpolating polynomial function recursively.      \\[\\begin{align*} P_n &amp;= f(x_0) + f[x_0,x_1](x-x_0) + f[x_0,x_1,x_2](x-x_0)(x-x_1)+\\ldots \\\\ &amp;= f(x_0) + (x-x_0)\\left[ f[x_0,x_1] + (x-x_1)[f[x_0,x_1,x_2] + (x-x_2)[\\ldots]]  \\right]\\\\ \\end{align*}\\]  Writing the recurrence relation in the nested form allows for efficient computation.   Newton’s Difference as a function      Theorem     If $f$ is $n$-times continuously differentiable on $[a,b]$ then   \\[f[x_0, \\ldots, x_n] = \\frac{f^{(n)}(\\xi)}{n!}\\]    for some $\\xi\\in[a,b]$    We would like to extend newton’s difference to be a function. From the modified Neville’s formula, we get that   \\[f[x_0, x] = \\frac{f(x_0) - f(x)}{x_0 - x} \\quad \\text{where }x\\neq x_0\\]  From the above theorem (extended MVT) and applying $\\lim_{x\\to x_0}$ we get the following base case for the function:   \\[f[x_0, x] =  \\begin{cases} \\frac{f(x_0) - x}{x_0 - x} &amp; x\\neq x_0  \\qquad \\text{ base case}\\\\ f'(x_0) &amp; x = x_0 \\end{cases}\\]  We can now extend this base case to get the general function as shown in the below equation. Note that we would have to take care when there is equality between the nodes. Divided differences function is continuous by induction.   \\[f[x_0, \\ldots, x_{n-1}, x] = f[x_0,\\ldots,x,x_{n-1} ] = \\frac{f[x_0,\\ldots,x_{n-2}, x] - f[x_1, \\ldots, x_{n-1}, x]}{x_0 - x_{n-1}}\\]      Hermite Polynomials   Given data points $x_0, x_1 \\ldots x_n$ we wish to find an interpolating polynomial ($H_{2n+1}$) such that both the function values and the derivatives of the interpolating polynomial equals the actual function.   Define $H_i(x)$ and $\\hat{H}_i(x)$ as follows, note that $L$ corresponds to the Lagrangian polynomial;   \\[\\begin{align*} \tH_i(x) &amp;= [1-2(x-x_i)L_i'(x_i)]L_i^2(x) \\\\ \t\\hat{H}_i(x) &amp;= (x-x_i)L_i^2(x) \\\\ \t\\implies H_{2n+1}(x) &amp;= \\sum_i f(x_i)H_i(x) + \\sum_i f'(x_i)\\hat{H}_i(x) \\\\ \\end{align*}\\]  The error for such an interpolation has a similar form to that of the normal interpolation, and is given below;   \\[f(x) = H_{2n+1}(x) + \\frac{(x-x_0)^2(x-x_1)^2\\ldots(x-x_n)^2}{(2n+2)!}f^{(2n+2)}(\\xi(x)) \\qquad \\forall x-in[a,b], \\xi(x)\\in(a,b)\\]  Note that $H_{2n+1}$ can be computed using the Divided differences function that was defined by us earlier. Define a new set $Z = { z_0, z_1\\ldots z_{2n+1} }$ where $z_{2t} = x_t$ and $z_{2t+1} = z_{2t}$. It can be thus computed using the following formula similar to the one used for computing $P$;   \\[H_{2n+1}(x) = f[z_0] + \\sum_{i=1}^{2n+1} f[z_0,\\ldots z_i](x-x_0)\\ldots(x-x_{i-1})\\]         Splines   Our methods for finding the interpolating polynomial fail because the order of the polynomial depends upon the number of datapoints, and a large degree polynomial usually is a bad idea as it is very sensitive to outliers.   Splines use a piecewise polynomial function instead to estimate the function $f(x)$, and tend to be more accurate and representative of the underlying function.   A cubic spline is the most simplest case because we have four equations that need to be satisfied, and a cubic equation has four variables that can be adjusted. The following constraints need to be satisfied for a given set of data points ${x_0, x_1, \\ldots x_n }$;      $S_j(x)$ is a polynomial defined on each $[x_j, x_{j+1}]$   $S_j(x_j) = f(x_j)$ and $S_j(x_{j+1}) = f(x_{j+1})$   $S_j’(x_{j+1}) = S_{j+1}’(x_{j+1})$ and $S_j’‘(x_{j+1}) = S_{j+1}’‘(x_{j+1})$   Moreover, there are two kinds of boundary conditions that may be imposed. Clamped boundary conditions are usually the better option, but natural boundaries are usually used because the derivative information is not available.      Natural or Free boundary:  $S’‘(x_0) = S’‘(x_n) = 0$   Clamped Boundary: $S’(x_0) = f’(x_0)$ and $S’(x_n) = f’(x_n)$   The cubic equation for the interval $[x_i, x_{i+1}]$ is usually written in the following manner;   \\[S_i(x) = a_i + b_i(x-x_i) + c_i(x-x_i)^2 + d_i(x-x_i)^3\\]  The error for clamped boundary condition is given by the following equation. The error for clamped is of the fourth order as well, but is more difficult to express because the clamped case is more prone to errors.   \\[\\vert f(x)-S(x) \\vert \\leq \\frac{5M}{384}\\max_j(x_{j+1}-x_j)^4 \\qquad M=\\max_{a\\leq x\\leq b}\\vert f^{(4)}(x)\\vert\\]  ","url": "http://localhost:4000/notes/ma214/interp"
    },{
      "title": "Automata Theory",
      "excerpt":"       My notes for the course CS310: Automata Theory are linked below. They are on a per-lecture basis.      Lecture 1   Lecture 2   Lecture 3        Lecture 4       Regular Languages   Context Free Languages  ","url": "http://localhost:4000/notes/cs310"
    },{
      "title": "Artificial Intelligence and Machine Learning",
      "excerpt":"      The notes of CS337 have been divided lecture-wise, and they can be found below; or accessed via the sidebar.      Lecture1   Lecture2   Lecture3   Lecture4   Lecture5   Lecture6   Lecture7   Lecture8   Lecture9   Lecture10   Lecture11   Lecture12   Lecture13   Lecture14   Lecture15   Lecture16   Lecture17   Lecture18  ","url": "http://localhost:4000/notes/cs337"
    },{
      "title": "Medical Image Computing",
      "excerpt":"       My notes for the course CS736: Medical Image Computing have been linked below. The notes are on a per-lecture basis.   Weeks 1 and 2     Lecture 1   Lecture 2   Lecture 3   Week 3     Lecture 4   Image Segmentation Part 2  ","url": "http://localhost:4000/notes/cs736"
    },{
      "title": "Implementation of Programming Languages",
      "excerpt":"       My notes for the course CS302: Implementation of Programming Languages are linked below. They are on a per-lecture topic basis.      Lecture 1   Lecture 2        Lecture 3       Lexical Analysis   Syntax Analysis  ","url": "http://localhost:4000/notes/cs302"
    },{
      "title": "Introduction to Game Theory",
      "excerpt":"       My notes for the course CS6001: Introduction to Game Theory have been linked below. The notes are divided based on topics.      Normal Form Games   Mixed Equilibria   Correlated Equilibrium  ","url": "http://localhost:4000/notes/cs6001"
    },{
      "title": "Introduction to Philosophy",
      "excerpt":"  The notes of HS301 have been divided topic-wise, and they can be found below; or accessed via the sidebar.      Branches of Philosophy   Classical Greek Philosophy   Socratic Period            Aristotle           “Modern” Philosophy   Indian Philosophy       Thank you Sudhansh for pointing out typos!  ","url": "http://localhost:4000/notes/hs301/"
    },{
      "title": "Numerical Analysis",
      "excerpt":"       My notes for the course MA214: Numerical Analysis have been linked below. The notes are on a topic basis.      Introduction to Numerical Analysis   Equations in One Variable   Interpolation   Numerical Differentiation and Integration   Post Midsem, Quiz2 Syllabus     Composite Integration   Ordinary Differential Equations   Linear Equations  ","url": "http://localhost:4000/notes/ma214"
    },{
      "title": "Medical Image Computing",
      "excerpt":"        ","url": "http://localhost:4000/notes/cs736/intro"
    },{
      "title": "The basics",
      "excerpt":"        Databases are ever-present in our lives. Old database systems had problems with:      data redundancy and inconsistency   difficulty in accessing data   data isolation   integrity problems, assigning “rules” to variables   crashes in between causing partial updates   concurrent access by multiple users needs to be addressed. Embedded databases (SQLITE) do not have concurrency issues as they are always accessed by a single user (music app storing playlist locally)   security problems         Data Models   the fuck is a relational model   Yeah I have no idea what he said here   Relational Model   All the data is stored in tables. Relational models essentially are tables with rows (tuples) and columns (attributes).   Logical Schema is the overall logical structure of the table.   Physical Schema is the overall physical structure of the table. That is, the data structures used to store the table itself.   Declarative Languages hide the physical schema and only show the logical schema to the programmer. doubtful   Physical Data Independence: the physical schema can be modified without changing the logical schema of the table.       Data Definition Language (DDL)   numeric(8,2) means 6 digits before decimal point, 2 after   Data Manipulation Language (DML, Query Language)   Language used for accessing and updating the data organized by the data model.   incomplete   SQL is non-procedural and not turing complete. This simplicity lets the compiler to compile the program efficiently. It is embedded in APIs to perform other complex queries.       Database Design   Logical Design is created once, and needs to be carefully thought out. Physical design is flexible, as indexes and the such can be added on the fly. Improper design causes poor data storage and inefficient querying.   missed something   Storage Manager   ughhh   ","url": "http://localhost:4000/notes/cs317/lec1"
    },{
      "title": "Random Fields",
      "excerpt":"        We shall extensively be using the Baye’s Rule, given below.   \\[P(H\\vert e) = \\frac{P(e\\vert H)P(H)}{P(e)}\\]                 Symbol       Meaning                       $H$       Hypothesis                 $e$       Evidence                 $P(H)$       Prior                 $P(e)$       Marginal                 $P(e\\vert H)$       Likelihood                 $P(H\\vert e)$       Posterior           Some of the applications of Baye’s Rule are as follows:      Image Restoration - $P(\\text{uncorrupted }\\vert\\text{ corrupted})$   Image Segmentation/Labeling - $P(\\text{Label Image }\\vert\\text{ corrupted})$   Classic Baye’s Example with gaussian prior on unknown mean   Building Prior Models on Images   Let the dimensionality of the space be $N$ (voxel count). The following prior beliefs are generally valid on uncorrupted images.      Image intensities/values are spatially (piecewise) smooth   Discontinuities possible only at object boundaries   Number of objects $«$ Number of pixels       Topological Space -   Random Field -   Neighbor   Clique $C$ - $C$ contains a single site, or every pair of sites in $C$ are neighbors of each other. $C_i$ denotes the set of cliques of size $i$.   Markov Random Field (MRF)   A random field with sites $S$ and neighborhood $N$ is an MRF when   \\[P(X_i\\vert X_{S-\\{i\\}}) := P(X_i\\vert X_{N_i})\\]  In words, the probability of a site can be computed using only its neighbors. This does not mean that $X_i$ and $X_j$ are independent if they are not neighbors.   MRF is said to be homogeneous if the functional form of $ P(X_i\\vert X_{N_i})$ is independent of the position of site $i$ in the topological space.   MRF allows us to model high dimensional $P(X)$ in terms of multiple low dimensional conditional probabilities. (9-dim when 8-neighbor system is used)       Gibbs Random Field (GRF)   A random field is a GRF when the joint distribution equals the Gibbs Distribution. \\(P(x) = \\frac{1}{Z}\\exp\\left(-\\frac{1}{T}U(x)\\right)\\)      $Z$ - Partition Function, normalization constant (depends on $T$ and $U$)   $T$ - Temperature, constant (low $T$, sharp curve ; high $T$, flat curve)   $U(x)$ - Energy function   \\[U(x) = \\sum_{c\\in C}V_c(x_c)\\]  $C$ is the set of all cliques, $x_c$ is the set of image values in clique $c$, and $V_c$ is the clique potential function defined for the clique $c$.   Homogenous GRF has $V_c$ independent of the location of clique $c$   Isotropic GRF has $V_c$ independent of spatial orientation of clique $c$      Simulated Annealing   $T$ has been used for a stochastic algorithm for optimization, called simulated annealing. (helps to get out of a local minima) Consider the problem of finding the global maximum for the blue curve above with an initial $T$ and initial solution $x$.      Keeping $T$ constant            Sample $y$ from an isotropic/symmetric PDF in the vicinity of $x$       Update $x$ to $y$ with probability $\\min \\left[ P(y)/P(x)\\right]$           If $T&lt;\\text{(small positive number less than 1)}$, stop   Else, reduce $T$ and repeat       $X$ is an MRF on sites $S$ wrt neighborhood system $N$ iff $X$ is a GRF on $S$ wrt neighborhood system $N$. (They are equivalent!)  ","url": "http://localhost:4000/notes/cs736/Lec1"
    },{
      "title": "Chomsky Hierarchy",
      "excerpt":"        The languages in increasing order of generality are:      Regular Languages - Finite State Automata   Context Free Grammar - Push Down Automata   Unrestricted Grammar - Turing Machine       Finite State Automaton Problems   Q1. Is a bitstring’s decimal representation divisible by 7?   A. Make an FSM tracking the remainder, start at 0.   HW1. What if the bitstring is fed in reverse?   HW2. Instead of a base2 input, what is the FSA if the input is decimal (base10)?   Context Free Grammar   FSA cannot be drawn for CFG. Example: $a^nb^n$ cannot be obtained using a finite state automaton.   It can be written in grammar notation however, as follows:   \\[\\begin{align}   S &amp;\\rightarrow \\epsilon \\\\   S &amp;\\rightarrow aSb \\\\ \\end{align}\\]  Another example of a language which cannot be represented by FSA is the “matching parentheses” problem. Both these can be represented using a PDA/grammar notation.   We would like un-ambiguous grammar, meaning that no two rules are the “same”. The following grammar is not ambiguous, the proof will be discussed later.   \\[\\begin{align}   S &amp;\\rightarrow \\epsilon \\\\   S &amp;\\rightarrow (S) \\\\   S &amp;\\rightarrow SS \\\\ \\end{align}\\]    The grammar in CFG has 4 components:   \\[G = (V, \\Sigma, R, S)\\]  V - Non-terminal variables (S in previous case) $\\Sigma$ - alphabet R - Rules ($S \\rightarrow SS$) S - Start symbol what dis   Unrestricted Grammar   CFG Grammar’s restrictions on R can be lifted to obtain unrestricted grammar.   \\[UG = (V, \\Sigma, P, S)\\]  P - The rules are of form $\\alpha \\rightarrow \\beta$ where $\\alpha\\beta\\in (V \\cup \\Sigma)^*$   Turing Machine and Unrestricted Grammar are equivalent. They can compute any mathematical function.   HW3. $L = { a^{n^2} \\vert n\\geq 0 }$  ","url": "http://localhost:4000/notes/cs310/lec1"
    },{
      "title": "Implementation of Programming Languages",
      "excerpt":"        Binding refers to finding certain attributes of certain objects. There are 6 steps involved here:      Conceptualization   Coding   Compiling   Linking - Obtain addresses of external functions, data   Loading - Actual addresses of code and data   Execution - Variable values, user inputs are obtained. There are no unbound objects at execution stage   We are concerned with the “binding time” taken between the coding and compiling stages.   Implementation Mechanisms   There are two methods;           Translator which translates the source program into a target program, after which the machine executes the target program. Two distinct steps are present here, translation and execution. Inputs are taken at execution and not needed at translation.            Interpreter is a program which takes both source program and input data to execute the code.       Note that the source program is never executed directly on the machine in either case. This is because there is a “gap” between the “levels” between the source program and execution. That is, the source code is high-level but the machine operates on low-level.                  Level       State       Operations                       High Level       Variable values       Expressions, control flow                 Low Level       Register values       Machine Instructions                   Translator reduces the level of specification to execution level.            Interpreter raises execution level to specification level. The program (interpreter) executes the code, not the system.                              Translation       Interpretation                               Analysis + Synthesis       Analysis + Execution                         Bindings before runtime       Bindings at runtime                 Turn around time       compilation + execution       analysis + execution           The turnaround time for interpreters is faster, but compilation needs to be done only once for multiple executions, saving time.                  Language       Method                       C, C++       Compilation (Backend)                 Java, C#, Python       Interpretation + JIT Compilation (Virtual Machine)           JIT (Just In Time) Compilation is where a repeated part of the byte code is compiled by the virtual machine for faster execution. Do note that this is done in runtime.   Frontend - Language Backend - The machine ISA   $m \\text{ frontends } + n\\text{ backends } = m\\times n \\text{ compilers}$       Compiler Structure                     Compilation overview            The first two steps in compilation are scanning and parsing. Parsing creates a “concrete Parse tree” with terminal and non-terminal nodes.                     Abstract Syntax Tree            An abstract syntax tree (AST) is constructed by the semantic analyzer to ensure that there are no type errors.   The abstract syntax tree is then used to construct the TAC list (Three Address Code). TAC stores the intermediary variables. This linearizes the control flow by flattening the nested control constructs.                     Compiled sample code            RTL List converts the instructions in the TAC list to the ISA. Addresses are still unknown at this stage. RTL List is converted to the Assembly Code wherin proper assembly language with register addresses and offsets using ISA.  ","url": "http://localhost:4000/notes/cs302/lec1"
    },{
      "title": "Image Priors",
      "excerpt":"        skipping over the proof for the equivalence of MRF and GRF   We have discussed earlier that images tend to be piecewise smooth. There are two general cases when the differences between the neighboring pixels is large:      Discontinuity - Likely edges in images   Outlier - a datum far from rest of neighborhood, eg “spotty” texture   We want the MRF/GRF prior which allows such properties, and which doesn’t assign a very low probability density to such configurations. Assigning a low probability will cause blurring during image denoising.   Lets start out with a simpler problem, lets model the prior on a single pixel given the neighborhood intensities of that pixel.   Let the pixel’s intensity be $x$, the neighborhood be $N$ and the intensities of neighbors be $y_i$. We shall model the intensities as:   \\[y_i = x + \\eta_i(i) \\qquad \\text{where }\\eta_i(x)\\text{ is deviation}\\]  Let the penalty function be $g(.)$, meaning that the penalty given the deviation would be $g(\\eta_i(x))$. The “energy” function for this distribution would be:   \\[E(x) = \\sum_i g(\\eta_i(x))\\]  Assumption - Even penalty function   We would want $g$ to be even, so that no sign of deviation is favored over the other. We can therefore re-write the energy equation in terms of a new function $H(x)$ where $g(\\vert x\\vert) = H(\\vert x\\vert^2)$.   The derivative using chain rule would be:   \\[\\frac{\\partial g(x)}{\\partial x} = \\frac{\\partial H(\\vert x\\vert^2)}{\\partial x} = 2xH'(\\vert x\\vert^2) = 2x\\cdot h(x)\\]  $h(x)$ is a newly defined function, and it is equal to $H’(\\vert x\\vert^2)$.   Computing the point where the derivative of $E(x)=0$, we get   \\[x = \\frac{\\sum_i h(\\eta_i)y_i}{\\sum_i h(\\eta_i)}\\]  And therefore, the gradient descent update on $x$ with a step-size factor $\\tau$ would be   \\[x^{n+1} = x^n + \\tau \\sum_i2\\eta_ih(\\eta_i)\\]  In both the cases, $h(\\eta_i)$ acts like a weighting function. This property is used for finding how good of a choice the original penalty function $g(x)$ was.   $h(x)$ acts as the weights in the first case, and $2xh(x)$ acts like a “force” in the second case.       Analyzing penalty functions   We want the penalty functions to have the following properties, in addition to being continuous(1) , real-valued(2) and non-negative(3):   Non-increasing, with the following conditions:      (4a) $\\lim_{u\\to\\infty} h(u) = 0$   (4b) $\\lim_{u\\to\\infty}g’(u)\\leq C$ where $C$ is a fixed positive constant           Quadratic Penalty - $g(x) = \\vert x\\vert^2$       The value of $h(x) = 1$ for this choice.       As deviation $(\\eta_i)\\to \\infty$, $h(\\eta_i)$ remains a constant. This is would mean that the large deviation would significantly increase the energy of the distribution, meaning that this penalty function would smooth out all the edges present making the image blurry.            Custom Function - $g(x) = \\gamma\\exp(-\\vert x\\vert^2/\\gamma)$       \\[\\text{Huber's Function - } g(x) =  \\begin{cases} \\frac{\\vert x\\vert^2}{2} &amp; x\\leq\\gamma \\\\ \\gamma\\vert x\\vert - \\frac{\\gamma^2}{2} &amp; x&gt;\\gamma  \\end{cases}\\] ","url": "http://localhost:4000/notes/cs736/Lec2"
    },{
      "title": "Relational Models",
      "excerpt":"        Domain - The set of allowed values for each attribute   Usually, attribute values are required to be atomic (indivisible). A special value null is part of every domain.   Example: parts of roll numbers in IITB can be used to interpret the branch of that particular student, meaning it isn’t atomic (and is bad practice)   Is null &lt; 5 True or False?   missed   Database Schema - logical structure of database; intructor(ID, name, dept_name, salary)   Database Instance - snapshot of data in database at a given instant       Keys   Let $R$ be the set of all attributes in a database. Let $K$ be a subset of $R$.   K is a Superkey of R if values of K are sufficient to identify a unique tuple of each possible relation $r(R)$.   A minimal superkey is called a Candidate key. One of the candidate keys is chosen to be the Primary Key.   Foreign Key Constraint is an integrity constraint which enforces that value in one relation in another. This avoids errors caused by improper data entry.   Example: dept_name in the instructor table can be linked to the dept_name in the department table. Here, instructor is called the Referencing Relation and department is the Referenced Relation.   Referenced Relation should be a primary key.   ","url": "http://localhost:4000/notes/cs317/lec2"
    },{
      "title": "Incremental Construction of Compilers",
      "excerpt":"        Retargetable Compiler dies not require manual changes, the backend is generated according to the specifications. the fuck   A compiler essentially maps Source features to Target features through phases of compilation.      problem with this? To solve this, the target features were partitioned but there was no improvement. The source features were partitioned as well ugh missed entire thing   Finally, we partition the different “increments” of language to modularly increase the reach of the compiler.   image  ","url": "http://localhost:4000/notes/cs302/lec2"
    },{
      "title": "Regular Expressions",
      "excerpt":"        We try to express a language using a base case, and an inductive case. There are four kinds of inductive rules:                  $E_1 + E_2$       Union       $L(E_1+E_2) = {E_1, E_2}$                       $E_1 E_2$       Concatenation       $L(E_1E_2) = {E_1E_2}$                 $E_1^*$       Repetition (?)       $L(E_1^*) = {\\epsilon, E_1, E_1E_1, \\ldots}$           Note that Finite State Automata and Regular Expressions are equivalent to each other, as discussed in the previous lecture.   Note that $(ab+ba)^* = { \\epsilon, ab, ba, abba, baab, \\ldots}$   HW1. Language using a,b as alphabets; accepts all words with even number of a’s   The FSA for this language is easy enough. The regular expression equivalent would be given by: \\(L = (b^*ab^*ab^*)^*\\)   HW2. Same alphabets as before; but even number of a’s AND odd number of b’s   Interesting read   Push Down Automata   We want the FSM to have access to another data structure for more expressive power (a stack). The bottom of the stack is denoted by $Z_o$ with the stack symbols being pushed on top of it. Do note that the input signals need not be the same as the stack symbols.   This lets the PDA to:      Move to a new state   Pop/Push from a stack   The transitions are labeled as $(a,X\\vert aX)$. Meaning that the transition corresponds to alphabet $a$ and we are pushing it onto the stack. Similarly $(b,aX\\vert X)$ meaning $a$ is popped from the stack.   A word is accepted on an EMPTY STACK; rejected when no transition or stack not empty   HW3. \\(L = \\{ \\text{Number of a's &lt; 2}\\times\\text{number of b's} \\}\\) Write the above language using CFG.   doubt1 - $a\\vert Wa$ meaning?   doubt2 - How is an empty string rejected?   Non-Determinism   Consider a language on $a,b$ which contains all palindromes. For example, $abbbba$ is a word in the language. This can be represented by a Non-Deterministic Automata, where we need to guess where we stop pushing and start popping.   A word in non-deterministic FSA is accepted if any single sequence of choices leads to a final state.  ","url": "http://localhost:4000/notes/cs310/lec2"
    },{
      "title": "Compilation Models",
      "excerpt":"        There are two broad compilation models:      Aho Ullman Model   Davidson Fraser Model      Register Transfer is not 3-coded(?), and this means that dependence on machine’s hardware is introduced very early on in the Davidson Fraser model.   GCC uses a modified Davidson Fraser Model, where (optimizer, target indep. IR) are present before the expander. This adds machine independent optimization.       Typical Front Ends   Scanner, parser and the semantic analyzer form a typical front end of a compiler. The flow of information generally has been shown in the following image.      Typical Back Ends in Aho Ullman Model   Machine independent optimization is performed upon the IR before generating the code and performing machine dependent optimiation.   Constant Propagation: consider the lines x=5;, y=10*x; and let these be the only places where x comes into play. The compiler can then simply replace the second line with y=10*5; completely eliminating x. This is a compile-time optimization.      Register allocator allocates global registers, while code generator allocates local registers.   Instruction scheduler tries to find the best possible order of instruction execution.   Peephole optimizer ???   GCC Framework      ","url": "http://localhost:4000/notes/cs302/lec3"
    },{
      "title": "Nothing to see here...",
      "excerpt":"        Prof told to read Stanford’s slides  ","url": "http://localhost:4000/notes/cs310/lec3"
    },{
      "title": "Relational Algebra",
      "excerpt":"        There are six basic operators:      Select   Project   Union   Set Difference   Cartesian Product      Select Operation \\(\\sigma_\\rho (r) \\qquad \\rho\\text{ - Selection Predicate}\\) Only the rows which satisfy the selection predicate are present in the returned relation.   Project Operation \\(\\Pi_{A_1, \\ldots, A_k}(r) \\qquad A_i\\text{ - Attribute}\\) The attributes that are not listed are removed from the relation $r$.   These operations can be composed as shown in the below expression. \\(\\Pi_{name}(\\sigma_{dept\\_name = Physics}(instructor))\\)   Cartesian Product \\(r_1 \\times r_2\\) Every row of $r_1$ is paired with every row of $r_2$. Cartesian product is usually used along with the select operation to compare rows between two different relations. This composition can be written in short as shown below: \\(\\sigma_\\theta(r\\times s) \\equiv r \\infty_\\theta s\\) Note that the predicate $\\theta$ is over the union of the attributes of $r,s$.   Union Operation \\(r\\cup s\\) The union operation is valid if:      $r,s$ have the same arity (same number of attributes)   The attribute domains must be comparable   Two relations with the above properties are said to be compatible.   ==doubtful==   Set Intersection Operation \\(r\\cap s\\) ==dammit man==   Set Difference Operation \\(r - s\\) ==eugh==   Assignment Operation \\(tmp \\leftarrow r\\) This operator acts similar to assignment in programming languages. This is used to break complex queries into smaller, more manageable queries.   Rename Operation \\(\\rho_x(E)\\qquad \\rho_{x(A_1, \\ldots A_n)}(E)\\) This solves the issue of multiple attributes with the same name. That is, $r\\times r$ would have every attribute repeated twice. The expression $E$ is renamed to $x$ in the first representation, and the attributes of the expression are renamed to $A_1, \\ldots A_n$ in the second expression.   Aggregate Functions                  Operation       Function                       avg       Average value                 min       Minimum value                 max       Maximum value                 sum       Sum of values                 count       Number of values           $Y_{aggregate function} (r)$ returns a single value. For example, $Y_{avg(salary)}(student)$ returns the average salary of all students.   However, ${}{dept_name} Y{avg(salary)}(student)$ groups the average salary of students in each department.   ","url": "http://localhost:4000/notes/cs317/lec3"
    },{
      "title": "Texture Modeling",
      "excerpt":"        We try to learn a bank of textures using which one would be able to model any texture as a linear combination of these stored textures. This idea is analogous to the wavelet basis methodology. Unlike wavelet models whose basis were mathematically defined, the texture bank is learnt from data. This method is called as Dictionary Learning.   Consider the simple case of linear regression   \\[Ax = b\\]   $b$ - observed data, $A$ - known system matrix   There are two problems here:           Which solution to pick if multiple solutions exist? (under constrained)            Overfitting (noisy data / over constrained)       Priors, regularization are the tools used to solve the above problems. Ridge regression is an example of a popular regularization technique where the 2-norm is minimized. The objective function in ridge regression is quadratic, meaning that finding the optimal value is easy.   \\[\\text{Ridge} = \\min \\{\\vert\\vert Ax-b\\vert\\vert^2 + \\vert\\vert \\Gamma x\\vert\\vert^2 \\} \\\\ \\hat{x} = (A^TA + \\Gamma^T\\Gamma)^{-1}A^Tb\\]  The Bayesian interpretation would be a gaussian prior with mean 0 and covariance $(\\Gamma^T\\Gamma)^{-1}$.   Lasso Regularization instead proposes using L1-norm instead of the L2-norm. Note that this objective function has no closed form solution, meaning that gradient descent must be used. The equivalent prior is the Laplacian distribution.       Dictionary Learning   One of the techniques earlier discussed was Dictionary Learning. This problem can now be formally defined as follows.      Data Sample - $X = [x_1, \\ldots x_K]$ where $X\\in \\mathcal{R}^d$   Dictionary      - $D\\in\\mathcal{R}^{d\\times n}:D=[d_1,\\ldots d_n]$ (each column is an “atom”)   Coefficient Vectors - $R = [r_1,\\ldots r_K]$ where $R\\in\\mathcal{R}^n$   \\[\\text{arg}\\min_{D\\in C, r_i\\in\\mathcal{R}^n} \\left[ \\sum_{i=1}^k\\vert\\vert x_i-Dr_i\\vert\\vert^2_2 + \\lambda\\vert\\vert r_i\\vert\\vert_0\\right] \\\\ C = \\{ \\mathcal{D}\\in\\mathcal{R}^{d\\times n} : \\vert\\vert d_i \\vert\\vert_2\\leq 1 \\forall i \\}\\]  The condition $\\mathcal{C}$ is required to avoid trivial solutions where $r_i\\to0$ and $D\\to\\infty$ which is not what we want. The regularization parameter can be changed on a case-by-case basis.  ","url": "http://localhost:4000/notes/cs736/Lec3"
    },{
      "title": "Non Deterministic Automata",
      "excerpt":"        Non determinism is of two types:      Don’t Care: The choice offered doesn’t matter eg: simplifying the LHS or RHS of an equation first   Don’t Know: We need to guess the right choices   NDA’s are of the second type. The transition function $\\delta$ is redefined to a transition relation which outputs a set containing all possible next states.   NDA is said to accept a word if any one of the possible end states is a final state.   Epsilon Transitions   NFA’s also allow $\\epsilon$-transitions between states.   H1) Draw the NFA for $L=  { abc } \\cup {\\text{all strings not ending in }cc}$ where $\\Sigma={a,b,c}$   It seems that NFAs have more expressive power than DFAs, as expressing languages in NFA seems simpler and easier. But is this actually the case?   NO.   NFA (with/without $\\epsilon$ transitions) has the same expressive power as DFA. They are both equivalent.       Decision Properties of Regular languages   A set of languages is called as a Language Class, and Language classes have two kinds of properties, closure and decision. We will be focusing on decision properties for now.   Decision Property for a class takes the formal description of a language to see if a property holds. (Formal description - using DFA)   Closure Properties are used to prove if a language is regular or not. For example, $L_1={0^n1^n}$ is not a regular language. However, proving $L_2={\\text{equal number of 0’s and 1’s}}$  to not be regular is tougher. Closure can be used as follows, if $L_2$ was regular, then RHS should be regular as well (contradiction!) \\(L_2 \\cap L(0^*1^*)= L_1\\) ==Pumping Lemma is used to show that $a^nb^n$ cannot be expressed by a DFA.==   Membership problem is checking if a word $w\\in L$. The problem is said to be decidable iff the algorithm used for solving it has the following two properties:      Soundness: The algorithm is correct   Terminate: The algorithm terminates in finite time   A list of problems that we will be dealing with in this course are given in the table below. The methods to solve the problems have been discussed further below the table.                  Problem       Question                       Emptiness       $L=\\phi$ ?                 Infiniteness       Is $L$ infinite?                 Containment       Given $L,M$ is $L \\subseteq M?$                 Minimality       Can a DFA with smaller states accept the same language? (use distinguishable states)                 Membership       Is $w\\in L$?                 Equivalence       Are $L_1$ and $L_2$ equivalent?                 Impossibility       Show $L$ cannot be accepted by any DFA (Pumping Lemma)               Infiniteness Property   If the DFA has $n$ states, and the language contains any string of length $n$ or more, then the language is infinite. Otherwise, the language is surely finite. (Proof using the pigeon-hole principle)   Pumping Lemma   For every regular language $L$ there exists an integer $n$ such that every string $w$ in $L$ of length greater than $n$, we can write $w = xyz$ such that      $\\vert xy \\vert &lt; n$   $\\vert y \\vert&gt;0$   $\\forall i\\geq0, xy^iz\\in L$   This is closely related to the infiniteness algorithm we stated earlier. $y$ is the pert of the DFA which has a loop, and can thus be infinitely “pumped” to keep generating new strings.   We can now prove that ${0^k1^k}$ is not a regular language. Assume it was, and let the integer be $n$. Consider the word $0^n1^n$ written as $xyz$. Because $\\vert xy \\vert&lt;n$, $y$ is made up fully of $0’s$. Therefore, $xy^iz$ does not belong in $L$ as the number of $1’s$ and $0’s$ is not equal.       Equivalence Algorithm   Given two DFA’s, take the cross-product between them. $(q,r)$ is a final state of this DFA iff exactly one of the two is a final state in its original DFA. That is, this new DFA accepts $L_1\\oplus L_2$. If they were equivalent, then this new DFA would not accept any states.   Containment Algorithm is exactly the same, except that we search for words accepted by $L$ and not by $M$.       Minimality Algorithm   Create a table containing pairs of all states in the DFA. Two states are said to be distinguishable if there exists a string which takes exactly one of the two to a final state. We can use induction using this property.      Mark all pairs where exactly one is the final state (basis)   Mark all pairs where a letter takes exactly one of the two to a previously marked state   Remove states unreachable from the start state   It can be proven using induction/contraction that the DFA obtained using this method is THE minimal DFA.       Closure Properties of Regular Languages   have to write notes  ","url": "http://localhost:4000/notes/cs310/lec4"
    },{
      "title": "Linear Equations",
      "excerpt":"        We will be looking at solving the system of linear equations given by $A\\cdot x = b$. We already know a unique solution exists if $A$ is invertible, and there are either infinitely many solutions or no solution exists if $A^{-1}$ doesn’t exist.      Cramer’s Rule   \\[x_j = \\frac{\\text{det }A_j}{\\text{det }A}\\]    Here, $A_j$ is the $j$‘th column of  $A$ being replaced by $b$.    We will be focusing on another method for finding the roots  for the given system of linear equations.       Gauss Elimination Method (GEM)   “Simplify” the given system of linear equations, and convert $A$ into an upper triangular matrix for easier computations. There are three transformations that can be performed.      Particularly, we perform the following sequence of operations iteratively for $i=2,\\ldots (n-1)$   \\[(E_j - (a_{ji}/a_{ii})E_i)\\to(E_j)\\]  If $a_{ii}$ is zero, then find $a_{ki}\\neq0,k&gt;i$ and interchange the $k$‘th and $i$’th rows. If no such $a_{ki}$ can be found, then a unique solution for the system of linear equations does not exist. Once the upper triangular matrix is obtained, we can simply solve the system of linear equations by back-substituting from $x_n$.   Operations Performed   We count addition/subtraction together and multiplication/divisions together. For performing the operation $(E_j - (a_{ji}/a_{ii})E_i)\\to(E_j)$, we would need to perform:      $(n-i)$ divisions + $(n-i)(n-i+1)$ multiplications   $(n-i)(n-i+1)$ subtractions   After the upper triangular matrix is obtained, the $i$th equation would be solved using:      $1$ division + $(n-i)$ multiplications   $1$ subtraction + $(n-i-1)$ additions   Adding both these results and summing it up over $i$, we get that the number of operations required is of order $\\mathcal{O(n^3/3)}$.   Scaled Partial Pivoting   $a_{ii}$ is called as the “Pivot” for the $i$th linear equation, as it divides the result on the RHS during back-substitution. If the pivot is very small, it can lead to errors caused due to finite digit arithmetic. Scaled Partial Pivoting is used to try and prevent such errors from taking place.   Scaling Factors $(s_p)$ are computed for the $p$th rows at the very start as follows:   \\[s_p = \\max_{1\\leq j\\leq n}\\vert a_{ij} \\vert\\]  Now suppose that we are at the $i$th step of GEM. We select the smallest integer $p\\geq i$ such that   \\[p = \\text{arg}\\max_{i\\leq k\\leq n} \\frac{\\vert a_{ki} \\vert}{s_k} \\implies (E_i)\\longleftrightarrow(E_p), \\text{ } s_i\\longleftrightarrow s_p\\]  and then we interchange the $i$th and the $p$th rows. Note that the scaling factors would be interchanged as well.   Operations Performed   Computing the scaling factors requires $n(n-1)$ comparisons at the start. For the $i$th row we would need to perform $(n-i+1)$ divisions, and $(n-i)$ comparisons. Notice that summation over $i$ would lead to both the terms being $\\mathcal{O}(n^2)$ meaning that the computation cost is not large for performing Scaled Partial Pivoting.       LU Decomposition   We’ve seen the steps used to solve a system of linear equations, and the steps used to factor the matrix $A$. It would be very useful if we can factor the matrix as $A = LU$ where $L$ and $U$ are lower and upper triangular matrices respectively.   Assume that no row exchanges are needed in solving a system of equations $A\\cdot x=b$ using GEM. The first step would be given by the replacement $(E_j-m_{j1}E_1)\\to(E_j)$ where $M_{j1} = a_{j1}/a_{11}$. We define a matrix called $M^{(1)}$ which does the exact same thing upon left multiplication.   \\[M^{(1)} = \\begin{pmatrix} 1 &amp; 0 &amp; 0 &amp; \\cdots &amp; 0 \\\\ -m_{21} &amp; 1 &amp; 0 &amp; \\cdots &amp; 0 \\\\ -m_{31} &amp; 0 &amp; 1 &amp; \\cdots &amp; 0 \\\\ \\vdots  &amp;   &amp;   &amp; \\ddots &amp; 0 \\\\ -m_{n1} &amp; 0 &amp; 0 &amp; \\cdots &amp; 1 \\end{pmatrix}\\]  Similarly, we define $M^{(i)}$ for the $i$th step of GEM. The method can thus be given by the following equation.   \\[\\begin{align*} M^{(n-1)}\\dots M^{(1)}Ax &amp;= M^{(n-1)}\\dots M^{(1)} b \\\\ A^{(n)}x &amp;= b^{(n)} \\end{align*}\\]  Note that $A^{(n)}$ is an upper triangular matrix, and is the $U$ in the factorization that we wish to achieve. Moreover, every $M^{(k)}$ is a lower triangular matrix and its inverse can be used to regain the original matrix $A$ (and can also be easily computed). Therefore, the factorization would be given by:   \\[\\begin{align*} A = LU &amp;= \\left( L^{(1)}L^{(2)}\\ldots L^{(n-1)} \\right)\\left( M^{(n-1)}M^{(n-2)}\\ldots M^{(1)} A\\right) \\\\ L^{(k)} &amp;= \\left( M^{(k)} \\right)^{-1} \\end{align*}\\]  Once the factorization is obtained we solve the system of equations in these two steps:      Let $y=Ux$. Solve for $y$ in $Ly=b$.   Once $y$ is obtained, solve for $x$ in $Ux=y$   Easing Row Interchange Constraint - PLU Decomposition   A permutation matrix $P$ is used to interchange the rows as needed. For an invertible matrix $A$, there exists a  permutation matrix $P$ such that $PAx=Pb$ does not require any permutations. Therefore, the decomposition would be given by   \\[A = P^{-1}LU = P'LU\\]  To get the permutation matrix $P$, simply perform row exchanges on the identity matrix. Similarly to get the inverse $P’$, perform column exchanges.   Special Matrices   We discuss a few special matrices which do NOT require permutations for performing GEM (and subsequently in the LU Decomposition).   Strictly Diagonally Dominant Matrices   An $n\\times n$ matrix $A$ is said to be diagonally dominant iff the following condition is satisfied:   \\[\\vert 2a_{ii} \\vert \\geq \\sum_{j=1}^n\\vert a_{ij} \\vert\\]  It is said to be Strictly Diagonally Dominant if the inequality is strict. Strictly Diagonally Dominant matrices are invertible, need no $P$ in GEM, and are stable with respect to the growth of round-off errors.   Positive Definite Matrices   A matrix $A$ is said to be PD if it is symmetric and for all $x\\neq 0$, $x^TAx &gt; 0$. Upon expanding this condition, we get the following.   \\[\\left( \\sum_{i=1}^n\\sum_{j=1}^n x_ia_{ij}x_j \\right) &gt; 0\\]  This condition cannot always be used to prove/disprove if $A$ is PD. Following are necessary conditions that are used to disprove $A$ being PD. (They are necessary, but not sufficient)      $A$ must be invertible   $a_{ii}&gt;0$ for all $i$   $a_{ii}a_{jj} &gt; (a_{ij})^2$   $\\displaystyle \\max_{1\\leq k,j\\leq n}\\vert a_{kj} \\vert \\leq \\max_{1\\leq i\\leq n}\\vert a_{ii} \\vert$      A matrix $A$ is PD if and only if every leading principal submatrix of $A$ has a positive determinant.    Cholesky Factorization   A matrix $A$ is PD if and only if $A=LL^T$ where $L$ is a lower triangular matrix. This is called a Cholesky Factorization.  ","url": "http://localhost:4000/notes/ma214/lin"
    },{
      "title": "CS218",
      "excerpt":"The notes shall discuss the various categories of algorithms, and different ways of coming up with them. They have been segregated based on the idea discussed.  - [Divide and Conquer]({{ site.baseurl }}/pdfs/CS218_DAQ.pdf) - [Dynamic Programming]({{ site.baseurl }}/pdfs/CS218_DP.pdf) - [Flow Networks]({{ site.baseurl }}/pdfs/CS218_FN.pdf) - [NP-Hardness]({{ site.baseurl }}/pdfs/CS218_NP.pdf)","url": "http://localhost:4000/notes/cs218/"
    },{
      "title": "CS224",
      "excerpt":"I'll be uploading the notes here, segregated by the layers in the 5 Layer model of Networking. - [Data Link Layer - Wifi and L2 switching]({{ site.baseurl }}/pdfs/CS224_DLL.pdf) - [Networking Layer]({{ site.baseurl }}/pdfs/CS224_NL.pdf)","url": "http://localhost:4000/notes/cs224/"
    },{
      "title": "HS101",
      "excerpt":"The notes for this course have been linked below.  [Macroeconomics](/notes/hs101/mac/)  ### Microeconomics - [GDP](/notes/hs101/mic1) - [Cost of Living](/notes/hs101/mic2) - [Production and Growth](/notes/hs101/mic3) - [Finances](/notes/hs101/mic4) - [Unemployment](/notes/hs101/mic5) - [Monetary System](/notes/hs101/mic6) - [Inflation](/notes/hs101/mic7) - [Aggregate Demand and Supply](/notes/hs101/mic8) - [Fiscal Policies on Aggregate Demand](/notes/hs101/mic9)","url": "http://localhost:4000/notes/hs101/"
    },{
      "title": "PH107",
      "excerpt":"I'll be adding the material I've made for PH107 in here. Do let me know if there's any errors via the feedback form.  ### Reading Material  - [A short take on the significance of Quantum Tunneling in Quantum Computing]({{ site.baseurl }}/pdfs/PH107_TunnelingEx.pdf)  ### Tutorial Solutions  - [Solutions for Tut9]({{ site.baseurl }}/pdfs/PH107_Tut9.pdf) - [Solutions for Tut10]({{ site.baseurl }}/pdfs/PH107_Tut10.pdf)  - [Mini Quiz Solutions]({{ site.baseurl }}/pdfs/Ph107_Mini.pdf)","url": "http://localhost:4000/notes/ph107/"
    },{
      "title": "SoS-2020",
      "excerpt":"The notes for Data Structures and Algorithms that I've written during Summer of Science, 2020. I've mainly reffered to the courses offered by _Coursera_, and some tutorials by _TutorialsPoint_.  - [Link to the repo](https://github.com/AkashCherukuri/Data-Structures-and-Algorithms) - [Link to the pdf]({{ site.baseurl }}/pdfs/SoS_Report.pdf)","url": "http://localhost:4000/notes/sos2020/"
    },{
      "title": "Notes for Chemical Catalysis",
      "excerpt":"# Table of Contents 1. [Random Forest Algorithm](#random-forest-algorithm) \t- [Working](#working) \t- [Hyperparameters](#important-hyperparameters) 2. [SMILES](#smiles-notation)  -1. [References](#references)  ***  # Random Forest Algorithm This is a very versatile algorithm, as it can act as both a classifier and an regressor. It is used in various fields, such as the stock market and banking for starters.  ### Working  This algorithm consists of building multiple decision trees to act as an ensemble. Each of the decision trees have access to a random set of features. Let the number of trees in the ensemble be `N`. Each individual tree produces a result when data is provided for classification, and the majority is given as the result of the entire classifier.  This is considered to be efficient as it is very likely that one tree might have trouble with a certain type of data, but the majority of the trees can classify the data just fine. However, for this to happen, it is necessary that all trees have next to no correlation with each other.  This is acheived in the following ways:  - **Bagging:**  \tLet one of the data set during training be \\[1,2,3,4\\]. This data is modified a little randomly so that all trees in the classifier get different variations of the same data. For example, one tree might get \\[1,1,2,4\\] and another can get \\[1,2,2,3\\] and so on.  - **Random Features:**  \tDiscussed already, each tree has access to a random subset of features. This further decreases the chances of two trees having similarities.  ### Important Hyperparameters  1. `n_estimators`: Number of trees in the ensemble. Large values imply better learning but slower training and classification times. 2. `max_features`: Maximum number of features that a node can consider before splitting. Large features would improve each individual tree but correlation is increased as well. 3. `max_depth`: Maximum depth of a tree in the ensemble. Large values can cause overfitting. 4. `min_samples_split`: The minimum samples that must be present in a node before split. 5. `min_samples_leaf`: The minimum samples that a leaf can have.    ***  # SMILES Notation  SMILES stands for **S**implified **M**olecular **I**nput **L**ine **E**ntry **S**ystem. This is somewhat akin to the IUPAC nomenclature, but is designed to be compact and use ASCII characters. Five basic syntax rules are to be followed, and they have been listed below.  1. **Atom and Bond Nomenclature**  \tAtoms are represented using their atomic symbols, and hydrogen is usually exempted in the string. That is, `C` refers to methane and `CC` refers to Ethane.  \tCapital letters denote normal atoms and small letters denote aromatic atoms. That is, `CCCCCC` is Cyclo-Hexane, wheras `cccccc` is Benzene. For atoms with multi character atomic symbols, it is usually better to represent them in \\[\\]. For example, Scandium is represented as `[Sc]` and not `Sc` as the latter refers to Sulphur being attached to an aromatic carbon.  \tThe symbols used for bonds are given below. Single bonds are usually not represented manually.  \t| Symbol |  Character  | \t|:------:|:-----------:| \t|   =    | Double Bond | \t|   #\t | Triple Bond | \t|   *    | Aromatic Bond | \t|   .    | Disconnected Structures |   2. Chains  \tAs explained earlier, hydrogens need not be written down for the structure to be generated. That is, `CC#C` refers to propyne. However, if hydrogens are represented anywhere in the string, it is assumed that ALL hydrogens have been mentioned explicitly. For example, `HC(H)=C(H)(H)` is ethene.  3. Branches  \tBranches in molecules are represented using parantheses. The bond by which the branch is attached to the \"main\" chain is given after the opening paranthesis. For example, `CC(=C)C` and `CC(C)=C` both represent 2-Methyl Prop-1-ene.  4. Rings  \tRings are represented using SMILES by marking the \"start\" and \"end\" carbons of a ring using a number. That is, Cyclohexane is represented as `C1CCCCC1`, and Benzene is `c1ccccc1`. If the start and end of a ring are connected via a double or triple bond, it is mentioned before the number of the \"start\" atom. That is, `C=1CCCCC1` is Cyclo-Hexene.  5. Charge on Atoms  \tThe charge on an atom is represented in braces as `+1` or `-1`. That is, Enolate ion of Prop-2-one is given by `CC(O{-1})=C`.  ***  # References  1. [Hyperparameter tuning for Random Forest algorithm](https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74) 2. [Understanding Random Forest Algorithm](https://towardsdatascience.com/understanding-random-forest-58381e0602d2)  3. [SMILES Tutorial by USEPA](https://archive.epa.gov/med/med_archive_03/web/html/smiles.html)","url": "http://localhost:4000/notes/chemcat/"
    },{
      "title": "Theory of Machine Learning",
      "excerpt":"These notes contain the theoretical aspect of Machine Learning; and have been made during the winter vacation of 2020. _Understanding Machine Learning: From Theory to Algorithms (c) 2014 by Shai Shalev-Shwartz and Shai Ben-David_ has been taken as the main reference while reading up on this topic.  The notes have been divided into two parts, the first pdf is handwritten and contains the nomenclature and some basic derivations. The second pdf is typed, and is where most of the time has been spent by me.  - [Link to first pdf]({{site.baseurl}}/pdfs/ML1.pdf) - [Link to second pdf]({{site.baseurl}}/pdfs/ML2.pdf)","url": "http://localhost:4000/notes/toml/"
    },{
      "title": "Notes",
      "excerpt":"The notes which I've written so far will be linked down below. These notes have been written by me for projects, as class notes, or out of pure interest in the topic. Some of the notes are handwritten, wheras others have been done in LaTeX.  I am aware that the notes might have many typos which might've been missed by me in proofreading; although I may or may not re-visit them again. If you find any typos or any errors in my notes, feel free to let me know [here](https://docs.google.com/forms/d/e/1FAIpQLSfg5K7Who3oHfU4l4fgulXwf8h9csXvU88QPf83HDsMjE65XA/viewform?usp=sf_link)!  ## Self-Notes  - [Deep Learning and Convolutional Networks](/notes/dl/intro) - [Theory of Machine Learning](/notes/toml/) - [Data Structures and Algorithms written for SoS-2020](/notes/sos2020/)  ## Class-Notes  - [CS6001 - Introduction to Game Theory](/notes/cs6001)  ---  - [CS736 - Medical Image Computing](/notes/cs736) - [CS310 - Automata Theory](/notes/cs310) - [CS302 - Implementation of Programming Languages](/notes/cs302) - [MA214 - Numerical Analysis](/notes/ma214)  ---  - [HS301 - Introduction to Philosophy](/notes/hs301) - [CS747 - Foundations of Intelligent and Learning Agents](/notes/cs747) - [CS337 - Artificial Intelligence and Machine Learning](/notes/cs337)  ---  - [PH566 - Advanced Simulation Methods in Physics]({{site.baseurl}}/pdfs/PH566.pdf) - [CS224 - Networks](/notes/cs224/) - [CS218 - Design and Analysis of Algorithms](/notes/cs218) - [HS101 - Humanities and Social Sciences](/notes/hs101)  --- - [CS251 - Notes for Python](/notes/cs251py/)  - [CS251 - Notes for Awk and Sed](/notes/cs251a_bash/) - [CS213 - Classnotes](/notes/cs213cn/) - [CS215 - Data Analysis and Interpretation]({{site.baseurl}}/pdfs/CS215.pdf)    The notes start from the Gaussian distribution. - [CS207 - Discrete Structures]({{site.baseurl}}/pdfs/CS207.pdf)    The notes start from the Chinese Remainder Theorem.  ## Teaching-Assistant-Notes  - [PH107 - Quantum Physics and its Applications](/notes/ph107/)","url": "http://localhost:4000/notes/"
    },{
      "title": "Ordinary Differential Equations",
      "excerpt":"        We say that an initial valued problem is **well-posed** if it has a unique solution, and the initial value problem obtained by small perturbations also has a unique solution. Initial value problems are of the following form.  $$ \\frac{dy}{dt} = f(t,y),\\quad a\\leq t\\leq b\\quad y(a)=\\alpha $$  There should exist a unique solution $y(t)$ to both this, and to the perturbed initial value problem $z(t)$.  ![image-20220323162001984](../../../assets/images/typora/image-20220323162001984.png)    ### Lipschitz Condition  $f(t,y)$ is said to satisfy the Lipschitz condition in $y$ on a set $D\\subset \\mathcal{R}^2$ if a Lipschitz constant $L>0$ exists such that $$ \\vert f(t,y_1)-f(t,y_2)\\vert \\leq L\\vert y_1-y_2 \\vert $$ whenever $(t,y_1), (t,y_2)\\in D$.  > Let $D=\\{ (t,y) a\\leq t\\leq b, -\\infty  #### Theorem > > Consider the following one-step difference method: > > $$ > w_{i+1} = w_i + h\\phi(t_i,w_i,h) > $$ > > Also let a $h_0>0$ exist such that $\\phi$ is continuous and satisfies the Lipschitz condition over the set $D = \\{ (t,w,h):a\\leq t\\leq b,-\\infty > If the above conditions are satisfied, then: > > 1. The method is stable > 2. The method is convergent **if and only if** the method is consistent > 3. $\\vert y(t_i) - w_i \\vert \\leq\\frac{\\tau(h)}{L}e^{L(t_i-a)}$","url": "http://localhost:4000/notes/ma214/ode"
    },{
      "title": "Projects",
      "excerpt":"         Getting things done      right.                Getting things done      quick.            Page under construction, check back later.  Look at the cool donut while you're here I guess, weee!     ","url": "http://localhost:4000/projects/"
    },{
      "title": "Regular Languages",
      "excerpt":"       Regular languages are the languages that can be defined using an automata or equivalently a regular expression. This section talks about regular expressions and then about the properties of regular languages.  &nbsp;    ## Regular Expressions  Inductively describe languages. If $E$ is a regular expression, $L(E)$ is the language that it describes.  |         Basis Definitions          |      Inductive Definitions       | | :--------------------------------: | :------------------------------: | | If $a$ is any symbol, $L(a)=\\{a\\}$ | $L(E_1+E_2) = L(E_1)\\cup L(E_2)$ | |    $L(\\epsilon) = \\{\\epsilon\\}$    |    $L(E_1E_2) = L(E_1)L(E_2)$    | |        $L(\\phi) = \\{\\phi\\}$        |       $L(E^*) = (L(E))^*$        |  Order of precedence: $* > \\text{concatenation} > +$  ==Regular Expressions are equivalent to Deterministic Finite Automatons (NFAs, $\\epsilon$-NFAs by extension as well)==  ![image-20220122122541431](../../../assets/images/typora/image-20220122122541431.png)  To prove the equivalence between RE and Finite State Automatons, we show the following:  - Show that for every RE, there is an automaton that accepts this RE’s language. We pick the  most accepting automaton, the $\\epsilon$-NFA.    It can be easily shown via induction that such an NFA can be constructed.    ![image-20220204141557201](../../../assets/images/typora/image-20220204141557201.png)     - For going backwards, use the the following method to find the corresponding RE. We use a DFA here as it is the most “restrictive” automaton.    [GFG Url](https://www.geeksforgeeks.org/generating-regular-expression-from-finite-automata/)  &nbsp;  ## Decision Properties of Regular languages  A set of languages is called as a Language Class, and Language classes have two kinds of properties, **closure** and **decision**. We will be focusing on decision properties for now.  **Decision Property** for a class takes the formal description of a language to see if a property holds. (Formal description - using DFA)  **Closure Properties** are used to prove if a language is regular or not. For example, $L_1=\\{0^n1^n\\}$ is not a regular language. However, proving $L_2=\\{\\text{equal number of 0's and 1's}\\}$  to not be regular is tougher. Closure can be used as follows, if $L_2$ was regular, then RHS should be regular as well (contradiction!)   $$ L_2 \\cap L(0^*1^*)= L_1 $$     {: .notice--info}  **Pumping Lemma** is used to show that $a^nb^n$ cannot be expressed by a DFA.  *Membership problem* is checking if a word $w\\in L$. The problem is said to be **decidable** iff the algorithm used for solving it has the following two properties:  1. **Soundness**: The algorithm is correct 2. **Terminate**: The algorithm terminates in finite time  A list of problems that we will be dealing with in this course are given in the table below. The methods to solve the problems have been discussed further below the table.  | Problem       | Question                                                     | | ------------- | ------------------------------------------------------------ | | Emptiness     | $L=\\phi$ ?                                                   | | Infiniteness  | Is $L$ infinite?                                             | | Containment   | Given $L,M$ is $L \\subseteq M?$                              | | Minimality    | Can a DFA with smaller states accept the same language? (use distinguishable states) | | Membership    | Is $w\\in L$?                                                 | | Equivalence   | Are $L_1$ and $L_2$ equivalent?                              | | Impossibility | Show $L$ cannot be accepted by any DFA (Pumping Lemma)       |  &nbsp;  ## Infiniteness Property  If the DFA has $n$ states, and the language contains any string of length $n$ or more, then the language is infinite. Otherwise, the language is surely finite. (Proof using the pigeon-hole principle)  ### Pumping Lemma  For every regular language $L$ there exists an integer $n$ such that every string $w$ in $L$ of length greater than $n$, we can write $w = xyz$ such that  1. $\\vert xy \\vert 0$ 3. $\\forall i\\geq0, xy^iz\\in L$  This is closely related to the infiniteness algorithm we stated earlier. $y$ is the pert of the DFA which has a loop, and can thus be infinitely “pumped” to keep generating new strings.  We can now prove that $\\{0^k1^k\\}$ is not a regular language. Assume it was, and let the integer be $n$. Consider the word $0^n1^n$ written as $xyz$. Because $\\vert xy \\vert<n$, $y$ is made up fully of $0's$. Therefore, $xy^iz$ does not belong in $L$ as the number of $1's$ and $0's$ would not be equal.   &nbsp;  ## Equivalence Algorithm  Given two DFA’s, take the cross-product between them. $(q,r)$ is a final state of this DFA iff **exactly one** of the two is a final state in its original DFA. That is, this new DFA accepts $L_1\\oplus L_2$. If they were equivalent, then this new DFA would not accept any states.   **Containment Algorithm** is exactly the same, except that we search for words accepted by $L$ and not by $M$.  &nbsp;  ## Minimality Algorithm  Create a table containing pairs of all states in the DFA. Two states are said to be **distinguishable** if there exists a string which takes exactly one of the two to a final state. We can use induction using this property.  1. Mark all pairs where exactly one is the final state (basis) 2. Mark all pairs where a letter takes exactly one of the two to a previously marked state 3. Remove states unreachable from the start state  It can be proven using induction/contraction that the DFA obtained using this method is **THE** minimal DFA.    &nbsp;  # Closure Properties of Regular Languages  Closure properties are used to know if a the result of an operation on a given set of languages also belongs to the same set.  The following operations are closed under the set of regular languages.  |    Operation    |    Notation    | | :-------------: | :------------: | |      Union      |   $L\\cup M$    | |  Concatenation  |      $LM$      | | Kleene Closure  |     $L^*$      | |  Intersection   |   $L\\cap M$    | |   Difference    |     $L-M$      | | Complementation | $\\Sigma^* - L$ |  Lets look at a new operation called **Reversal**, and discuss the closure under this operation.    ### Closure under Reversal  Given a language $L$, $L^R$ is the set of strings whose reversal is in $L$. That is, if $abc\\in L$, then $cba∈ L^R$. We shall prove this using the regular expression $E$ for the language $L$ to obtain $E^R$.  - **Basis** - If $E$ is a symbol, $\\epsilon$ or $\\phi$ then $E^R = E$ - **Induction**   - $(F+G)^R = F^R+G^R$   - $(FG)^R = G^RF^R$   - $(E^*)^R = (E^R)^*$  This method can be used to solve that problem where we wanted to find the DFA of all binary strings which when reversed were divisible by 8.    ### Closure under Homomorphism  Homomorphism refers to the application of a function $h$ which essentially replaces alphabets in the language.  ![image-20220204144442966](../../../assets/images/typora/image-20220204144442966.png)    Similarly, inverse homomorphisms are closed under the domain of regular languages as well.  ![image-20220204144524009](../../../assets/images/typora/image-20220204144524009.png)  ![image-20220204144622872](../../../assets/images/typora/image-20220204144622872.png)    ![image-20220204144630894](../../../assets/images/typora/image-20220204144630894.png)  ![image-20220204144634880](../../../assets/images/typora/image-20220204144634880.png)  ","url": "http://localhost:4000/notes/cs310/reg"
    },{
      "title": "Resources Used",
      "excerpt":"---   - [Neural Networks and Deep Learning by Miachel Nielson](http://neuralnetworksanddeeplearning.com/index.html) - A Machine Learning course [here](https://www.coursera.org/learn/machine-learning)   - Notes on Machine Learning [here](http://cs229.stanford.edu/summer2019/cs229-notes1.pdf) - [Understanding Generative Adversarial Networks](https://towardsdatascience.com/understanding-generative-adversarial-networks-gans-cd6e4651a29)","url": "http://localhost:4000/notes/dl/resources"
    },{
      "title": "Katana Zero Review",
      "excerpt":"     - **Rating** - 4/5 - **Studio** - Askiisoft - **Publisher** - Devolver Digital - **Genre** - 2D Action-Platformer  (Spoiler-free)  &nbsp;   Katana Zero is a retro-style action-platformer developed by Askiisoft and published by Devolver Digital. The visuals and the soundtrack are reminiscent of Hotline Miami, with the core gameplay loop feeling more polished.   &nbsp;   ## Gameplay You play as a nameless samurai, working for a dystopian government as a hitman. The game follows a one-hit-kill rule, meaning that a single hit is enough to eliminate both you and your enemies. A well-timed attack can parry bullets, and you can slow down time to make parrying and dodging easier. During a conversation, you can choose dialogue with an additional option of interrupting the speaker, which might open up other dialogue paths.   The game is highly replayable and even has a customizable speedrun mode with associated medals, which adds incentive to get faster. New game plus also includes an insanely difficult Hard-mode, which will keep players engaged for quite a while after finishing the game. The Hard-mode adds in new enemies, with the number of regular enemies increased significantly.   &nbsp;       ## Presentation Katana Zero's presentation is a cut above the rest. The visuals combine a neo-noir aesthetic and pixelized graphics to yield scenes that, at times, look stunning. The visuals are complemented by a fantastic synthwave soundtrack composed by LudoWic and Bill Kiley.  Evident care has been put into the presentation, making the game feel the best it possibly can. The most notable effects are screen-shake and hit pause, which makes landing an attack highly satisfying. It also offers a few basic options for the player to tweak some of these effects as they wish.  Here's one of the more melancholic tracks from the game;       Tunç Çakır - Blue Room (feat. LudoWic)        &nbsp;   ## A candle that burns twice as bright... Katana Zero is a relatively short game. It would take roughly 4 hours to complete for the first time. My personal best for speedrunning the game is approximately 26 minutes. Including NG+, it would take around 8 hours to see everything that the game has to offer.  It makes up for this by being effortless to play. I prefer a four-hour game worth replaying many times to a padded-out game barely worth playing once, so this isn't a negative for me (but it might be one for you). The story is where the game falls short as a stand-alone title. It starts off very strong, but not much is resolved at the end. However, a sequel is in the making, which might answer the many questions posed. (Keyword - Might; which is why the rating is subject to change)  &nbsp;    ## In Conclusion Katana Zero is one of the most exciting Indie games that I have played to date. The gameplay is exhilarating, the campaign effortless, and the story intriguing. I highly recommend this game, and if you're not willing to buy this game for full-price, you should at the very least be on the lookout for an offer; because even if you believe this game is not worth your money, I can essentially guarantee it is worth your time.","url": "http://localhost:4000/reviews/KatanaZero"
    },{
      "title": "About the Reviews",
      "excerpt":"## Ranking System I will be rating games on  a scale of 0 through 5. Each number's meaning has been explained below. I plan to be reasonably thorough with these reviews.  - 0 -> Unplayable - 1 -> Playable, but would not reccommend - 2 -> Enjoyable, but with serious flaws - 3 -> Reccommended, but with minor flaws - 4 -> Highly Reccommended, respectable - 5 -> Masterpiece   ## Interests I tend to enjoy faster, reflex based games more than strategic, slow-paced ones. Do keep in this in mind if you do intend to read the review, as I tend to be sterner towards such genres. My rating is my opinion alone, and is (obviously) not absolute.   ## Why here? Putting one's ideas into words effectively is an important skill to possess, be it an Engineer or otherwise. Similarly, being able to look at game design and analyzing how effectively it acheives (or fails to acheive) its goals requires critical thinking, which I wish to train upon. Games are just a medium I enjoy, and thus, makes the process of me honing my skills enjoyable.  I am putting it up on my website for everyone to view, and provide constructive feedback on; as it would ultimately help me in my journey no matter how small.","url": "http://localhost:4000/reviews/about/"
    },{
      "title": "Video Game Reviews",
      "excerpt":"              Katana Zero      ","url": "http://localhost:4000/reviews/"
    },{
      "title": "Syntax Analysis",
      "excerpt":"       Syntax analysis aims to discover the larger structures in a program. The tokens identified through scanning are analyzed to infer the relationships between them. These relationships are represented in the form of a **Parse Tree of (concrete) Syntax Tree**. An Abstract Syntax Tree is derived from this parse tree by the IR.  ![image-20220202112805316](../../../assets/images/typora/image-20220202112805316.png)  Parser ensures that the program is well formed by performing **syntax checking**. There are three places where a parser is placed in a compiler organization.  1. **Parse Driven Syntax Tree Generation** : The parser is works as an autonomous body, not concerned with other processes of a compiler 2.  **Parser Driven Frontend** : The frontend (scanner, semantic analysis) is interleaved with the parser 3.  **Parser Driven Compilation** : Entirety of compilation (scanner, semantic analysis, and code generation) is interleaved with the parser  Although parsers were originally written manually, we now have tools which can automatically generate parsers.    ## Syntax Specification  The syntax of a program should be specified such that it is unambiguous, correct and complete, and convenient for both the designer and implementer. **Context Free Grammars** meet these requirements, and are thus used for syntax specification.    $$ G = (N,T,S,P) $$    $N$ is a set of non-terminals, $T$ is a set of terminals, $S$ is a special nonterminal ($S\\in T$) called the start, and $P$ is the set of production rules. A statement is obtained from the given set of rules via a **derivation**.  Its easy to generate a statement from a start state. However, our aim now is to ascertain whether a given statement belongs to a CFG. This is tough to do efficiently as we can’t simply just apply every rule without logic.    $$ \\text{type idlist }\\implies \\text{ integer idlist, id;} $$    Here, we say that `type idlist` derives `integer idlist, id;`.  | Symbol                     | Meaning                                 | | -------------------------- | --------------------------------------- | | $a\\implies b$              | $a$ derives $b$ in a single step        | | $a\\overset{*}{\\implies} b$ | $a$ derives $b$ in a zero or more steps | | $a\\overset{+}{\\implies} b$ | $a$ derives $b$ in a one or more steps  |    ### Sentential Forms  A string $\\alpha\\in (N\\cup T)^{\\*}$ that is derivable from the start state itself is called a **sentential form** of the CFG $G$. That  is, $S\\overset{\\*}{\\implies} \\alpha$ . Note that every sentence is a sentential form but not the other way around.   > A sentential form is “on its way” towards becoming a sentence    ### Leftmost and Rightmost derivations  Denoted by $a\\overset{lm}{\\implies} b$ and $a\\overset{rm}{\\implies} b$ respectively. Leftmost derivation is when the leftmost non terminal string is expanded upon. Similarly for rightmost.  ![image-20220202122900907](../../../assets/images/typora/image-20220202122900907.png)    A *parse tree* is a pictorial form of representing a derivation. The root of the tree is labeled with the start state ($S$), and each of the leaf nodes is labeled with a terminal or $\\epsilon$. The derivation of a parse tree is given by reading its leaves left-to-right.    ### Ambiguous Grammars  A grammar is said to be ambiguous when more than one parse tree exists for a derivation. (implying that more than one leftmost, rightmost derivations exist) Note that this is a property of the grammar and not the language itself.  Ambiguities are eradicated via:  - Precedence - Associativity  &nbsp;    # Parsing Strategies  “Parsing” refers to the technique of building the parse tree. This can be done bottom-up or in a top-down manner. We shall be looking at bottom-up parsing in this course. Note that we usually work from the left to the right, constructing the parse tree from the bottom up. This implies that the resulting decoded derivation would be *Rightmost* in nature.  **Handle** - A right-sentential form $\\gamma$ such that there exists a production rule $A\\rightarrow\\beta$ and $\\beta$ is a substring of $\\gamma$.  We shall assume that we know how to detect handles in a string, and proceed with parsing algorithms. Note that only terminals can appear after $\\beta$ in $\\gamma$ as it is the right sentential form.    ## SLR(0) Parsing  There are 4 basic actions of the shift-reduce parser;  1. **Shift** - We move a single lexeme from the input buffer onto the stack until the appearance of a handle 2. **Reduce** - Upon noticing a handle, it is popped and replaced with the LHS of the corresponding production rule 3. **Accept** - The parser accepts the string when the input buffer is empty and the start symbol $S$ is the only symbol present on the stack 4. **Error** - An error is thrown in all other cases, with the string being rejected  This method is also called as LR(0) parsing. Note that $ is used to represent the bottom of the stack and the end of the input buffer string.  Two types of conflicts are possible here:  1. **Shift-Reduce Conflict** - Consider the input $β a\\gamma$ with the rules being $A\\to\\beta$ and $B\\to β a \\gamma$. 2 options present; either accept the first rule and reduce, or go for the second rule and shift. 2. **Reduce-Reduce Conflict** - Let the stack be $β\\gamma$ with rules $A\\to\\beta\\gamma$ and $B\\to\\gamma$. Again, 2 options present; either reduce using the first rule or using the second rule.  ![image-20220223215627976](../../../assets/images/typora/image-20220223215627976.png)    ## SLR(1) Parsing  The only difference between SLR parsing and LR parsing is the parsing table. Follow the given parse table blindly for now. An example of the table is given below.  ![image-20220223215901825](../../../assets/images/typora/image-20220223215901825.png)  Now, we shall look at ways to construct this table to be able to “formalize” shift-reduce parsing. Firstly, make sure to take note of precedence and associativity rules inherent in the grammar. (That is, $a+b+c$ would be expanded as $(a+b)+c$, and $a+b\\*c$ would be expanded as $a+(b\\*c)$)  *Viable Prefix* - A prefix of the right sentential form that doesn’t extend beyond the current handle. It is either a string with no handle, or a string that ends with a handle.  The set of viable prefixes forms a regular language, and can be recognized as by DFA. Coupled with the stack recognizing handles that shift reduce parsing keeps track of, it is possible to define a PDA for this.  Consider the following set of rules, with the aforementioned rules of precedence and associativity.   $$ \\begin{align*} E&\\to E+E\\\\ E&→ E*E\\\\ E&→ \\text{id}\\\\ \\end{align*} $$   A “tree” is constructed to generate all viable prefixes, from the most basic case ($\\epsilon$). Remember that a handle doesn’t necessarily need to be reduced once it is noticed, it is possible that the next token prioritizes shifting. Example, $E+E*E$   ![image-20220223222400036](../../../assets/images/typora/image-20220223222400036.png)    #### Valid items for viable prefixes  An item is a production with a $\\cdot$ somewhere in the RHS. The dot $\\cdot$ separates what has been seen from what may be seen in the input. We guess these from the rules that define the language.  A **complete item** has $\\cdot$ at  the end, and suggests a reduction. The valid items for a few expressions is given below.  For $\\epsilon$,  $$ \\begin{align*} \tE&\\to\\cdot E+E\\\\ \tE&\\to\\cdot E*E\\\\ \tE&\\to\\cdot \\text{id}\\\\ \\end{align*} $$ For $E$, $$ \\begin{align*} \tE&\\to E\\cdot+E\\\\ \tE&\\to E\\cdot*E \\end{align*} $$   For $E+$; $\\cdot E$ is called a **Kernel Item** and the other computed results are called **Closure Items** $$ \\begin{align*} \tE&\\to E+·E  \\\\ \t\\\\ \t⇒ E&\\to E+·E+E \\\\ \tE&\\to E+·E*E \\\\ \tE&\\to E+·\\text{id} \\end{align*} $$    #### Computing LR(0) item sets for grammar  1. **Add a new start state** $E'$ which points to the original start state. The start state of the DFA is constructed by putting a dot before the original start state, and take its closure. ($E'→· E$) 2. For all the states remaining which have a rule of form $· E$ where $E$ is a non terminal, take its closure and make a new state. Link back if such a state has been previously created. Repeat this step until all states’ rules have been dealt with.  ![image-20220223225055942](../../../assets/images/typora/image-20220223225055942.png)   Certain transitions are removed from this to enforce associativity and precedence. (For example, + from $I_5$)    #### First and Follow Sets  $\\text{FIRST}(\\beta)$ contains the **terminals** that may begin a string derivable from $\\beta$. If $\\beta$ derives $\\epsilon$, then $\\epsilon\\in \\text{FIRST}(\\beta)$.  Consider the rule $A\\to X_1\\ldots X_k$. Obviously, $\\text{FIRST}(X_1)\\subseteq \\text{FIRST}(A)$. If $A$ could derive $\\epsilon$ (that is, if $\\epsilon\\in\\text{FIRST}(X_1)$), then we could also say that $\\text{FIRST}(X_2)\\subseteq\\text{FIRST}(A)$. (and if $X_2$ derived $\\epsilon$ we say the same thing about $X_3$ and so on)    $\\text{FOLLOW}(A)$ contains the **terminals** that follow $A$ in some right sentential form. If $A$ is the start symbol, then $\\$\\in\\text{FOLLOW}(A)$.  Consider the rule $A\\to\\alpha B\\beta$. It can be said that $\\text{FIRST}(\\beta)-\\{\\epsilon\\}\\subseteq \\text{FOLLOW}(B)$ (obviously!). Moreover, if $\\beta$ can derive $\\epsilon$ then $\\text{FOLLOW}(A)\\subseteq\\text{FOLLOW}(B)$ (also obviously because $B$ is the rightmost variable in this derivation).     #### Creating the Parsing Table  1. Create the $LR(0)$ item sets’ DFA as explained earlier  2. For all the non-terminals in the (new) grammar, compute the $\\text{FOLLOW}$ set  3. Create a table with the rows labeled as state numbers from the DFA and columns labeled as all the tokens encounterable (including \\$)  4. For every state, do the following;     1. transition using terminal to state $n$ - $sn$     2. transition using non-terminal to state $n$ - $cn$     3. $\\cdot$ is present at the end of item (a thing has been derived) - $rn$ for all the things in the $\\text{FOLLOW}$ set, $n$ is the rule to be applied for reducing        For example, $E'\\to E\\cdot$ would have $acc$ in \\$ because $\\text{FOLLOW}(E')=  \\{\\$\\}$  ![image-20220224002728138](../../../assets/images/typora/image-20220224002728138.png)  ($I_6$ doesn’t have the + edge, its a typo)    ## CLR(1) / LR(1) Parsing  Although SLR(0) worked well, it fails at times because a symbol in a $\\text{FOLLOW}$ set need not follow the variable in **every** right sentential form. LR(1) sets are constructed to consider the lookahead symbol as well.   The “1” in LR(1) stands for the first level of look-ahead that the sets use. A subset of the LR(0) is constructed.  Rules are of form $A\\to \\alpha\\cdot\\beta,a$ meaning that the rule is valid when $A$ is followed by $a$. The closure computation for a rule $A\\to\\alpha\\cdot B\\beta,a$ would be given by items of form $B\\to\\cdot\\gamma,\\text{FIRST}(\\beta a)$. The lookahead is not changed on transitions, however. That is, transitioning on $B$ in the previous rule would yield $A\\to\\alpha B\\cdot\\beta, a$.  ![image-20220224141338392](../../../assets/images/typora/image-20220224141338392.png)    Generating the table is exactly the same as the SLR(1), except at the reduce step. Instead of looking at the $\\text{FOLLOW}$ sets, we look at the look-ahead for the corresponding reduction in the DFA and reduce only for those terminals.     ## LALR(1) Parsing  Stands for “Look Ahead LR(1)” parsing. Idea is to merge item sets with identical cores which may have different lookaheads. That is, $A\\to\\alpha, a$ and $A\\to\\alpha,b$ are merged to be $A\\to\\alpha,a/b$. ==This can introduce reduce-reduce conflicts but never introduce shift-reduce conflicts.==  Note that we do not construct LR(1) sets and merge for LALR(1). Instead, we can just construct LR(0) sets and use a lookahead propagation algorithm.    The expressive power of parsing is $$ \\text{SLR(0)} <\\text{SLR(1)} <\\text{LALR(1)} <\\text{LR(1)} $$ ","url": "http://localhost:4000/notes/cs302/synt"
    },]